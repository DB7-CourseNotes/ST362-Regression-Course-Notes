---
title: "L01: Distributions"
institute: "Jam: **Gamma Ray** by Beck"
---

::: {.content-visible when-format="pdf"}

## The Syllabus Part

### Basic Info

:::

## Introduction

### Today's Learning Outcomes

- Important facts about distributions.\lspace
- CIs and t-tests.\lspace

## Distributions

### Normal

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(\frac{(x-\mu)^2}{-2\sigma^2}\right);\;-\infty<x<\infty
$$

Shiny: [https://shiny.abdn.ac.uk/Stats/apps/app_normal/#section-theory](https://shiny.abdn.ac.uk/Stats/apps/app_normal/#section-theory)

\pspace

- Completely determined by $\mu$ and $\sigma$.\lspace
- If $X \sim N(\mu,\sigma^2)$, then $Z=\frac{X-\mu}{\sigma} \sim N(0,1)$.
    - Often called "standardizing"\lspace
- You won't *need* to memorize the pdf, but it's useful!

:::notes
```{r}
#| eval: false
#| echo: true
shiny::runGitHub(repo = "DBecker7/DB7_TeachingApps", 
    subdir = "Tools/pnorm")
```

The normal distribution is the foundation for pretty much everything that we're going to do in this class. In general, things aren't normally distributed, but the assumption is very robust and works out in a lot of cases.

In this lecture we're going to build up an important result that we'll use frequently. In particular, we want some background into why the F and $t$ distributions show up so often!



:::

### But first, Gamma!

:::: {.columns}
::: {.column width="50%"}
\vspace{1cm}

$t$ is based on the Gamma ($\Gamma$) function:

$$
\Gamma(q) = \int_0^\infty e^{-t}t^{q-1}dt
$$

- Interesting property: $\Gamma(k+1) = k!$ for integer $k$.
    - In general, $\Gamma(q) = (q-1)\Gamma(q-1)$\lspace
- Also, $\Gamma(1/2) = \pi^{1/2}$
    - $\Gamma(3/2) = (3/2-1)\gamma(1/2) = \sqrt{\pi}/2$


:::
::: {.column width="50%"}
\includegraphics[width=\textwidth]{figs/gamma.png}
:::
::::

### The $t$ Distribution

$$
f(x; \nu) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu\pi}\Gamma(\nu/2)}\left(1 + \frac{x^2}{\nu}\right)^{-(\nu + 1)/2}
$$

- The notation $f(x; \nu)$ means "function of $x$, given a value of $\nu$."
- Completely determined by $\nu$\lspace
- As $\nu\rightarrow\infty$, this becomes $N(0,1)$.
    - $\nu>30$ is pretty much normal already.
    - For $\nu<\infty$, $t$ has wider tails than normal.

### The $\chi^2$ distribution - variances

If $Z_1, Z_2, ..., Z_k$ are iid $N(0,1)$, then $\chi^2_k = \sum_{i=1}^kZ_i^2$ has a chi-square distribution on $k$ degrees of freedom.

$$
f(x; k) = \frac{1}{2^{k/2}\Gamma(k/2)}x^{k/2-1}e^{-x/2}
$$

- If $X_i\sim N(\mu_i\sigma_i)$, then we can just standardize each first.\lspace
- As $k\rightarrow\infty$, $(\chi^2_k-k)/\sqrt{2k}\stackrel{d}{\rightarrow} N(0,1)$.
    - That is, for large $k$ this can be approximated by a normal distribution.\lspace
- Very related to the *variance*
    - $(n-1)s^2/\sigma^2$ follows a $\chi^2_n$ distribution.

### The $F$ distribution - ratio of variances

If $S_1$ and $S_2$ are independent $\chi^2$ distributions with degrees of freedom $\nu_1$ and $\nu_2$, then $\frac{S_1/\nu_1}{S_2/\nu_2}$ follows an $F_{\nu_1,\nu_2}$ distribution.

$$
f(x; \nu_1, \nu_2) = \frac{\Gamma\left(\frac{\nu_1+\nu_2}{2}\right)\left(\frac{\nu_1}{\nu_2}\right)^{\nu_1/2}}{\Gamma(\nu_1/2)\Gamma(\nu_2/2)}\frac{x^{\nu_1/2-1}}{(1 + x\nu_1/\nu_2)^{(\nu_1+\nu_2)/2}}
$$

- If $s_1^2$ and $s_2^2$ are the sample-based estimates of the true values $\sigma_1^2$ and $\sigma_2^2$, then $\frac{s_1^2/\sigma_1^2}{s_2^2/\sigma_2^2}$ follows an $F_{\nu_1, \nu_2}$ distribution.
    - Under $H_0$, $\sigma_1=\sigma_2$, so we don't need the true values.
    - Note: $s_1^2$ is calculated from $S_1^2/\nu_1$.

## Confidence Intervals

### General Idea: Terminology

Consider the **statistic** $t = \frac{\hat\theta - \theta_0}{se(\hat\theta)}$ (a statistic is any number that can be calculated from data alone).

\pspace

- $\theta$ is the """**true**""" value of the parameter.
    - Unknown, unknowable, not used in formula - hypothesize $\theta = \theta_0$ instead.\lspace
- $\hat\theta$ is our **estimator** of $\theta$.
    - **Estimator** is a function, like $\hat\mu = \bar X =(1/n)\sum_{i=1}^nX_i$.
        - $X$ is a random variable.
    - **Estimate** is a number, like the calculated mean of a sample.\lspace
- $se(\hat\theta)$ is the **standard error** of $\hat\theta$.
    - If we had a different sample, the value of $\hat\theta$ would be different. It has **variance**.
    - **Standard error**: the standard deviation of the **sampling distribution**.

### General Idea: Distributional Assumptions

Consider the quantity (*not* statistic) $t = \frac{\hat\theta - \theta}{se(\hat\theta)}$.

\pspace

If we assume $X\sim N(\mu, \sigma^2)$ and $\hat\theta = \bar X$ is an unbiased estimate of $\theta$ on $\nu=n-1$ degrees of freedom, then $t \sim t_\nu$

\pspace

From this, we can find the lower and upper $\alpha/2$\% of the $t$$ curve.

- $t_\nu(\alpha/2)$ is the lower $\alpha/2$\%.
    - i.e., if $\alpha = 0.11$, then it's $t(\nu, 0.055)$, the lower 5.5\% area.
- $t_\nu(1 - \alpha/2)$ is the upper $\alpha/2$\%.
- Since $t$ is symmetric, $t_\nu(1 - \alpha/2) = -t_\nu(\alpha/2)$.


:::notes
```{r}
#| eval: false
#| echo: true
shiny::runGitHub(repo = "DBecker7/DB7_TeachingApps", 
    subdir = "Tools/pvalues")
```
:::

### General Idea: Distribution to Quantiles

Under the null hypothesis, $\theta = \theta_0$, so
$$
t = \frac{\hat\theta - \theta}{se(\hat\theta)} = \frac{\hat\theta - \theta_0}{se(\hat\theta)} \sim t_\nu
$$

We know that $\bar X$ is a random variable, and we want everything in between its 5.5\% and 94.5\% **quantiles**. \vspace{-3mm}

- This is a confidence interval!

### General Idea: Distribution to CI

We can do this easily for the $t_\nu$ distribution: we want all values $t_0$ such that

\begin{align*}
t_\nu(5.5) &\le t_0 \le t_\nu(94.5)\\
t_\nu(5.5) &\le \frac{\hat\theta - \theta_0}{se(\hat\theta)} \le t_\nu(94.5)\\
se(\hat\theta)t_\nu(5.5) &\le \hat\theta - \theta_0 \le se(\hat\theta)t_\nu(94.5)\\
\hat\theta + se(\hat\theta)t_\nu(5.5) &\le \theta_0 \le \hat\theta + se(\hat\theta)t_\nu(94.5)
\end{align*}

The CI is all values $\theta_0$ that would *not* be rejected by the null hypothesis $\theta = \theta_0$ at the $\alpha$\% level.

Since $t_\nu(1 - \alpha/2) = -t_\nu(\alpha/2)$, our 89\% CI is $\hat\theta \pm se(\hat\theta)t_\nu(5.5)$.


### What is the Standard Error?

If we're estimating the mean, $\hat\theta = (1/n)\sum_{i=1}^nX_i$, where we assume $X_i \stackrel{iid}{\sim} N(\mu, \sigma)$.
$$
E(\hat\theta) = E\left(\frac{1}{n}\sum_{i=1}^nX_i\right) = \frac{1}{n}\sum_{i=1}^nE(X_i) = \frac{n\mu}{n} = \mu
$$
From this, we see that the mean is an unbiased estimator! (This is nice, but not required.)

$$
V(\hat\theta) = V\left(\frac{1}{n}\sum_{i=1}^nX_i\right) = \frac{1}{n^2}V\left(\sum_{i=1}^nX_i\right) \stackrel{indep}{=} \frac{1}{n^2}\sum_{i=1}^nV(X_i) = \sigma^2/n \stackrel{plug-in}{=} s^2/n
$$
where $s^2$ is the estimate variance since we cannot know the true mean. Note that $s^2$ is a biased estimator for $\sigma$.

### CI for Variance

From before: $\frac{(n-1)s^2}{\sigma^2} \sim \chi^2_n$. 

Let $\chi^2_n(0.055)$ be the lower 5.5\% quantile, $\chi^2_n(0.945)$ be the upper.

For homework, find the CI from:
$$
\chi^2_n(0.055) \le \frac{(n-1)s^2}{\sigma^2} \le \chi^2_n(0.945)
$$
Note that $\chi^2_n(0.055)\ne-\chi^2_n(0.945)$.


### Summary

- Distributions exist and are important
    - Most things will be normal, which leads to $t$, $\chi^2$, and $F$.\lspace
- CIs are all values that would not be rejected by a hypothesis test.
    - The null hypothesis determines the distribution of the test statistic, which allows us to find the CI.

\pspace

For Next Class:\vspace{-3mm}

- Read through Ch01, especially procedure for least squares estimation.

<---

## Participation Questions

### Q1

Which of the following is a Normal distribution?

\pspace

1. $f(x; \nu_1, \nu_2) = \frac{\Gamma\left(\frac{\nu_1+\nu_2}{2}\right)\left(\frac{\nu_1}{\nu_2}\right)^{\nu_1/2}}{\Gamma(\nu_1/2)\Gamma(\nu_2/2)}\frac{x^{\nu_1/2-1}}{(1 + x\nu_1/\nu_2)^{(\nu_1+\nu_2)/2}}$
1. $f(x) = \frac{1}{\sqrt{2\pi}}\exp\left(-x^2/2\right)$
2. $f(x; \nu_1, \nu_2) = \frac{\Gamma\left(\frac{\nu_1+\nu_2}{2}\right)\left(\frac{\nu_1}{\nu_2}\right)^{\nu_1/2}}{\Gamma(\nu_1/2)\Gamma(\nu_2/2)}\frac{x^{\nu_1/2-1}}{(1 + x\nu_1/\nu_2)^{(\nu_1+\nu_2)/2}}$
3. $f(x; k) = \frac{1}{2^{k/2}\Gamma(k/2)}x^{k/2-1}e^{-x/2}$

### Q2

If you know $\mu$ and $\sigma$, then you know the exact shape of the normal distribution.

\pspace

1. True
2. False

### Q3

A confidence interval for $\theta$ contains all values $\theta_0$ that would not be rejected by a hypothesis test (assume that both are at the same significance level).

1. True
2. False

### Q4

Which of the following is the correct value for $E(\hat\theta)$, where $\hat\theta = \sum_{i=1}^n\left(a + bX_i\right)$ and $E(X_i)=\mu$ for all $i$?

\pspace

1. 
2. $a + b\mu$
3. $b\mu$
4. $na + nb\mu$

### Q5

Which of the following is the definition of an estimator?

1. A value calculated from data.
2. A function that returns the estimate for a parameter.
3. Any function of the data.
4. A person who estimates.

### Q6

The general approach to finding confidence intervals is to find a function of the statistic and the parameter it's estimating that follows a known distribution and then solve for the unknown parameter.

\pspace

1.
2. False
3. True
4. 

--->
