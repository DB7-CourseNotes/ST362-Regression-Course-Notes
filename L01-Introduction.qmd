---
title: "Stats Review"
subtitle: "Simulating Linear Models, Understanding Distributions"
institute: "Jam: **Gamma Ray** by Beck"
date: "last-modified"
---

::: {.content-visible unless-profile="book"}

## Preamble

### Basic Info

\centering
\includegraphics[width=0.75\textwidth]{figs/phdSyllabus.png}

\footnotesize{Source: Ph.D. Comics by Jorge Cham}
\raggedright

:::

## Introduction

### Today's Learning Outcomes

- Important facts about distributions.\lspace
- CIs and t-tests.\lspace

## Simulating Distributions from Linear Models

### Linear Models 101

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$
where

- $x_i$ is assumed to be fixed
- $\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)$
    - "iid" = "independent and identically distributed"

### Simulating from a linear model

- Choose values for $\beta_0$ and $\beta_1$
- Fix $x$
- Simulate $\epsilon_i \sim N(0, \sigma^2)$
- Estimate $\hat\beta_0$, $\hat\beta_1$, **and** $\hat\sigma^2$.

### Sampling Distributions of Estimates

::: {.content-visible when-profile="book"}
The following code will simulate from a linear model 1000 times. This is *not* the most efficient way to do this, but it's more clear what's going on and we're working with some pretty small numbers.

```{r}
#| label: sim-means
#| code-fold: true
#| code-summary: "Show the code"
set.seed(2112)
n <- 40
beta0 <- 10
beta1 <- -5
sigma <- 3
x_vals <- runif(n, 0, 10)

# Sets up an empty vector
# (double means double precision, so it allows non-integers;
# it is not doubling the value.)
beta0_ests <- double(n)
beta1_ests <- double(n)
sigma_ests <- double(n)
for (i in 1:1000) {
    y <- beta0 + beta1 * x_vals + rnorm(n, 0, sigma)   
    mylm <- lm(y ~ x_vals)
    beta0_ests[i] <- coef(mylm)[1]
    beta1_ests[i] <- coef(mylm)[2]
    sigma_ests[i] <- summary(mylm)$sigma
}

par(mfrow = c(1, 3))
hist(beta0_ests, breaks = 30)
abline(v = beta0, col = "red", lwd = 3)

hist(beta1_ests, breaks = 30)
abline(v = beta1, col = "red", lwd = 3)

hist(sigma_ests^2, breaks = 30)
abline(v = sigma^2, col = "red", lwd = 3)
```

Every time we generate new data, we get a different model! On average we're getting the right model, but it's never exactly equal. In addition, there's a nice bell-shaped curve centered at the true value. In this class, we're interested in both of these things - whether each model is approximating the true value, as well as how much it varies around that value.

It's really nice to know that the average value of the estimates is equal to the true value. This is great! But we're statisticians - at the very least we want variance, if not a full distribution!
:::

::: {.content-visible unless-profile="book"}

```{r}
#| label: sim-means-pdf
par(mfrow = c(1, 3))
hist(beta0_ests, breaks = 30)
abline(v = beta0, col = "red", lwd = 3)

hist(beta1_ests, breaks = 30)
abline(v = beta1, col = "red", lwd = 3)

hist(sigma_ests^2, breaks = 30)
abline(v = sigma^2, col = "red", lwd = 3)
```
:::


### But what's the distribution?


```{r}
#| label: sim-dists
#| code-fold: true
#| code-summary: "Show the code"
par(mfrow = c(1, 3))
hist(beta0_ests, breaks = 30, freq = FALSE)
abline(v = beta0, col = "red", lwd = 3)
Sxx <- sum((x_vals - mean(x_vals))^2)
curve(dnorm(x, 10, sigma * sqrt(sum(x_vals^2)/ (n * Sxx))), from = 5, to = 15, add = TRUE, col = 2, lwd = 3)

hist(beta1_ests, breaks = 30, freq = FALSE)
abline(v = beta1, col = "red", lwd = 3)
curve(dnorm(x, -5, sigma/sqrt(Sxx)), from = -10, to = 0, add = TRUE, col = 2, lwd = 3, n = 500)

hist(sigma_ests^2, breaks = 30, freq = FALSE)
abline(v = sigma^2, col = "red", lwd = 3)
curve(dchisq(x * (n - 1) / sigma^2, df = n - 1) * (n - 1) / sigma^2, from = 1, to = 20, add = TRUE, col = 2, lwd = 3, n = 500)
```

::: {.content-visible when-profile="book"}
I was able to draw a curve that perfectly matches the distribution of the simulated data *based on theory*. You won't yet know what each of the distributions are, but you can see that all of the `curve()` function calls rely only on the true values.

The $x$ values are also used in the calculation, which is not as important as it sounds. In regression, we assume that the $x$ values are fixed (or could have been known ahead of time). You may have noticed that I simulated $x$ values, but this was done once and it was outside the `for` loop. 
:::

### Lessons 

- All of the parameters **have sampling distributions**!\lspace
- The **variance** of the sampling distributions is important\lspace
- The **distributions** are well known to statisticians
    - Soon you'll know them too!\lspace
- **Simulations** let us "touch" theory!

::: {.content-visible when-profile="book"}
In this course, we'll generally follow the pattern of learning theory, verifying via simulation, then applying it to real-world data. The theory is the point of the course, with the hopes that you'll either learn advanced theory or have the tools you need to apply it. Simulation is an intermediate step that helps you fully understand the assumptions that you're making!
:::

## Linear Model Example

### `mtcars` data

::: {.content-visible when-profile="book"}
We'll make great use of the `mtcars` data set in this course! We'll just focus on what's immediately relevant, and get to know it better a little later on.

For now, here's the important bits:
:::

::: {.columns}
::: {.column}
- `mpg`: Miles per Gallon (fuel efficiency)
- `hp`: The horsepower of the car
    - Measures how many horses the car would be able to take in a fight
:::
::: {.column}

```{r}
#| label: mtcars_residplots
#| echo: false
plot(mpg ~ hp, data = mtcars)
abline(lm(mpg ~ hp, data = mtcars))
```
:::
:::

::: {.content-visible when-profile="book"}
From the plot, it looks like a linear model might be appropriate (although it might not be perfect).

In the simulations, we had a perfect situation: the data were simulated from a perfectly linear relationship and the residuals were perfectly iid normal. In practice, we have to check this before we can do any sort of analysis! In other words, we need our real-world data to come to us as if they were simulated from a perfect model.
:::

### Assumptions and Residual Diagnostics

::: {.columns}
::: {.column}
\vspace{0.5cm}

Assumptions:

\pspace

1. There's actually a linear relationship
    - Check "Residuals versus Fitted"\lspace
2. The residuals are normally distributed
    - Check "QQ Norm"\lspace
3. The variance is constant
    - Check "Scale-Location"\lspace
4. The residuals are independent
    - Check the study setup!

:::
::: {.column}
```{r}
#| fig-height: 5
#| fig-width: 5
par(mfrow = c(2, 2))
plot(lm(mpg ~ hp, data = mtcars))
```
:::
:::

::: {.content-visible when-profile="book"}

:::

## Distributions

### Normal

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(\frac{(x-\mu)^2}{-2\sigma^2}\right);\;-\infty<x<\infty
$$

Shiny: [https://shiny.abdn.ac.uk/Stats/apps/app_normal/#section-theory](https://shiny.abdn.ac.uk/Stats/apps/app_normal/#section-theory)

\pspace

- Completely determined by $\mu$ and $\sigma$.\lspace
- If $X \sim N(\mu,\sigma^2)$, then $Z=\frac{X-\mu}{\sigma} \sim N(0,1)$.
    - Often called "standardizing"\lspace
- You won't *need* to memorize the pdf, but it's useful!

::: {.content-visible when-profile="book"}
```{r}
#| eval: false
#| echo: true
shiny::runGitHub(repo = "DBecker7/DB7_TeachingApps", 
    subdir = "Tools/pnorm")
```

The normal distribution is the foundation for pretty much everything that we're going to do in this class. In general, things aren't normally distributed, but the assumption is very robust and works out in a lot of cases.

In this lecture we're going to build up an important result that we'll use frequently. In particular, we want some background into why the F and $t$ distributions show up so often!

:::

### The $t$ Distribution

$$
f(x; \nu) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu\pi}\Gamma(\nu/2)}\left(1 + \frac{x^2}{\nu}\right)^{-(\nu + 1)/2}
$$

- The notation $f(x; \nu)$ means "function of $x$, given a value of $\nu$."
- Completely determined by $\nu$\lspace
- As $\nu\rightarrow\infty$, this becomes $N(0,1)$.
    - $\nu>30$ is pretty much normal already.
    - For $\nu<\infty$, $t$ has wider tails than normal.

### The $\chi^2$ distribution - variances

If $Z_1, Z_2, ..., Z_k$ are iid $N(0,1)$, then $\chi^2_k = \sum_{i=1}^kZ_i^2$ has a chi-square distribution on $k$ degrees of freedom.

$$
f(x; k) = \frac{1}{2^{k/2}\Gamma(k/2)}x^{k/2-1}e^{-x/2}
$$

- If $X_i\sim N(\mu_i\sigma_i)$, then we can just standardize each first.\lspace
- As $k\rightarrow\infty$, $(\chi^2_k-k)/\sqrt{2k}\stackrel{d}{\rightarrow} N(0,1)$.
    - That is, for large $k$ this can be approximated by a normal distribution.\lspace
- Very related to the *variance*
    - $(n-1)s^2/\sigma^2$ follows a $\chi^2_n$ distribution.

### The $F$ distribution - ratio of variances

If $S_1$ and $S_2$ are independent $\chi^2$ distributions with degrees of freedom $\nu_1$ and $\nu_2$, then $\frac{S_1/\nu_1}{S_2/\nu_2}$ follows an $F_{\nu_1,\nu_2}$ distribution.

$$
f(x; \nu_1, \nu_2) = \frac{\Gamma\left(\frac{\nu_1+\nu_2}{2}\right)\left(\frac{\nu_1}{\nu_2}\right)^{\nu_1/2}}{\Gamma(\nu_1/2)\Gamma(\nu_2/2)}\frac{x^{\nu_1/2-1}}{(1 + x\nu_1/\nu_2)^{(\nu_1+\nu_2)/2}}
$$

- If $s_1^2$ and $s_2^2$ are the sample-based estimates of the true values $\sigma_1^2$ and $\sigma_2^2$, then $\frac{s_1^2/\sigma_1^2}{s_2^2/\sigma_2^2}$ follows an $F_{\nu_1, \nu_2}$ distribution.
    - Under $H_0$, $\sigma_1=\sigma_2$, so we don't need the true values.
    - Note: $s_1^2$ is calculated from $s_1^2/\nu_1$.

## Confidence Intervals

### General Idea: Terminology

Consider the **statistic** $t = \frac{\hat\theta - \theta_0}{se(\hat\theta)}$ (a statistic is any number that can be calculated from data alone).

\pspace

- $\theta$ is the """**true**""" value of the parameter.
    - Unknown, unknowable, not used in formula - hypothesize $\theta = \theta_0$ instead.\lspace
- $\hat\theta$ is our **estimator** of $\theta$.
    - **Estimator** is a function, like $\hat\mu = \bar X =(1/n)\sum_{i=1}^nX_i$.
        - $X$ is a random variable.
    - **Estimate** is a number, like the calculated mean of a sample.\lspace
- $se(\hat\theta)$ is the **standard error** of $\hat\theta$.
    - If we had a different sample, the value of $\hat\theta$ would be different. It has **variance**.
    - **Standard error**: the standard deviation of the **sampling distribution**.

### General Idea: Distributional Assumptions

Consider the quantity (*not* statistic) $t = \frac{\hat\theta - \theta}{se(\hat\theta)}$.

\pspace

If we assume $X\sim N(\mu, \sigma^2)$ and $\hat\theta = \bar X$ is an unbiased estimate of $\theta$ on $\nu=n-1$ degrees of freedom, then $t \sim t_\nu$

\pspace

From this, we can find the lower and upper $\alpha/2$\% of the $t$$ curve.

- $t_\nu(\alpha/2)$ is the lower $\alpha/2$\%.
    - i.e., if $\alpha = 0.11$, then it's $t(\nu, 0.055)$, the lower 5.5\% area.
- $t_\nu(1 - \alpha/2)$ is the upper $\alpha/2$\%.
- Since $t$ is symmetric, $t_\nu(1 - \alpha/2) = -t_\nu(\alpha/2)$.


::: {.content-visible when-profile="book"}
```{r}
#| eval: false
#| echo: true
shiny::runGitHub(repo = "DBecker7/DB7_TeachingApps", 
    subdir = "Tools/pvalues")
```
:::

### General Idea: Distribution to Quantiles

Under the null hypothesis, $\theta = \theta_0$, so
$$
t = \frac{\hat\theta - \theta}{se(\hat\theta)} = \frac{\hat\theta - \theta_0}{se(\hat\theta)} \sim t_\nu
$$

We know that $\bar X$ is a random variable, and we want everything in between its 5.5\% and 94.5\% **quantiles**. \vspace{-3mm}

- This is a confidence interval!

### General Idea: Distribution to CI

We can do this easily for the $t_\nu$ distribution: we want all values $t_0$ such that

\begin{align*}
t_\nu(5.5) &\le t_0 \le t_\nu(94.5)\\
t_\nu(5.5) &\le \frac{\hat\theta - \theta_0}{se(\hat\theta)} \le t_\nu(94.5)\\
se(\hat\theta)t_\nu(5.5) &\le \hat\theta - \theta_0 \le se(\hat\theta)t_\nu(94.5)\\
\hat\theta + se(\hat\theta)t_\nu(5.5) &\le \theta_0 \le \hat\theta + se(\hat\theta)t_\nu(94.5)
\end{align*}

The CI is all values $\theta_0$ that would *not* be rejected by the null hypothesis $\theta = \theta_0$ at the $\alpha$\% level.

Since $t_\nu(1 - \alpha/2) = -t_\nu(\alpha/2)$, our 89\% CI is $\hat\theta \pm se(\hat\theta)t_\nu(5.5)$.


### What is the Standard Error?

If we're estimating the mean, $\hat\theta = (1/n)\sum_{i=1}^nX_i$, where we assume $X_i \stackrel{iid}{\sim} N(\mu, \sigma)$.
$$
E(\hat\theta) = E\left(\frac{1}{n}\sum_{i=1}^nX_i\right) = \frac{1}{n}\sum_{i=1}^nE(X_i) = \frac{n\mu}{n} = \mu
$$
From this, we see that the mean is an unbiased estimator! (This is nice, but not required.)

$$
V(\hat\theta) = V\left(\frac{1}{n}\sum_{i=1}^nX_i\right) = \frac{1}{n^2}V\left(\sum_{i=1}^nX_i\right) \stackrel{indep}{=} \frac{1}{n^2}\sum_{i=1}^nV(X_i) = \sigma^2/n \stackrel{plug-in}{=} s^2/n
$$
where $s^2$ is the estimate variance since we cannot know the true mean. Note that $s^2$ is a biased estimator for $\sigma$.

### CI for Variance

From before: $\frac{(n-1)s^2}{\sigma^2} \sim \chi^2_n$. 

Let $\chi^2_n(0.055)$ be the lower 5.5\% quantile, $\chi^2_n(0.945)$ be the upper.

For homework, find the CI from:
$$
\chi^2_n(0.055) \le \frac{(n-1)s^2}{\sigma^2} \le \chi^2_n(0.945)
$$
Note that $\chi^2_n(0.055)\ne-\chi^2_n(0.945)$.


### Summary

- Distributions exist and are important
    - Most things will be normal, which leads to $t$, $\chi^2$, and $F$.\lspace
- CIs are all values that would not be rejected by a hypothesis test.
    - The null hypothesis determines the distribution of the test statistic, which allows us to find the CI.


::: {.content-visible unless-profile="book"}

## Participation Questions

### Q1

Which of the following is a Normal distribution?

\pspace

1. $f(x; \nu_1, \nu_2) = \frac{\Gamma\left(\frac{\nu_1+\nu_2}{2}\right)\left(\frac{\nu_1}{\nu_2}\right)^{\nu_1/2}}{\Gamma(\nu_1/2)\Gamma(\nu_2/2)}\frac{x^{\nu_1/2-1}}{(1 + x\nu_1/\nu_2)^{(\nu_1+\nu_2)/2}}$
1. $f(x) = \frac{1}{\sqrt{2\pi}}\exp\left(-x^2/2\right)$
2. $f(x; \nu_1, \nu_2) = \frac{\Gamma\left(\frac{\nu_1+\nu_2}{2}\right)\left(\frac{\nu_1}{\nu_2}\right)^{\nu_1/2}}{\Gamma(\nu_1/2)\Gamma(\nu_2/2)}\frac{x^{\nu_1/2-1}}{(1 + x\nu_1/\nu_2)^{(\nu_1+\nu_2)/2}}$
3. $f(x; k) = \frac{1}{2^{k/2}\Gamma(k/2)}x^{k/2-1}e^{-x/2}$

### Q2

If you know $\mu$ and $\sigma$, then you know the exact shape of the normal distribution.

\pspace

1. True
2. False

### Q3

A confidence interval for $\theta$ contains all values $\theta_0$ that would not be rejected by a hypothesis test (assume that both are at the same significance level).

1. True
2. False

### Q4

Which of the following is the correct value for $E(\hat\theta)$, where $\hat\theta = \sum_{i=1}^n\left(a + bX_i\right)$ and $E(X_i)=\mu$ for all $i$?

\pspace

1. 
2. $a + b\mu$
3. $b\mu$
4. $na + nb\mu$

### Q5

Which of the following is the definition of an estimator?

1. A value calculated from data.
2. A function that returns the estimate for a parameter.
3. Any function of the data.
4. A person who estimates.

### Q6

The general approach to finding confidence intervals is to find a function of the statistic and the parameter it's estimating that follows a known distribution and then solve for the unknown parameter.

\pspace

1.
2. False
3. True
4. 

:::
