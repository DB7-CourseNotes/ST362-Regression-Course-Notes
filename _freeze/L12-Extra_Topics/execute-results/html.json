{
  "hash": "d8ee24c144e40a0156cb368edd4b017e",
  "result": {
    "markdown": "---\ntitle: \"Extra Topics\"\ninstitute: \"Jam: **Hypotheticals** by Lake Street Drive\"\n---\n\n\n\n\n\n## Standardizing $X$\n\n### Mean-Centering\n\nConsider $y_i = \\beta_0 + \\beta_1 x'_i$, where $x'_i$ are the \"centered\" versions of $x_i$:\n$$\nx'_i = x_i - \\bar x\n$$\n\n\\pspace\n\nThen $\\bar{x'} = 0$ and the coefficient estimates become:\n\\begin{align*}\n\\hat\\beta_0 &= \\bar y - \\hat\\beta_1\\bar{x'} = \\bar y\\\\\n&\\text{and}\\\\\n\\hat\\beta_1 &= \\frac{S_{XX}}{S_{YY}} = \\frac{\\sum_{i=1}^n(x_i' - \\bar{x'})^2}{\\sum_{i=1}^n(x_i' - \\bar{x'})(y_i - \\bar{y})} = \\frac{\\sum_{i=1}^nx_i'^2}{\\sum_{i=1}^nx_i'(y_i - \\bar{y})}\n\\end{align*}\n\n### Mean-Centering and Covariance\n\n:::: {.columns}\n::: {.column width=\"50%\"}\nFor un-centered data:\n\\begin{align*}\nV(\\hat{\\underline\\beta}) &= (X^TX)^{-1}\\sigma^2,\\\\\n\\text{where }(X^TX)^{-1} &= \\frac{1}{nS_{XX}}\\begin{bmatrix}\\sum x_i^2 & -n\\bar x\\\\-n \\bar x & n\\end{bmatrix}\n\\end{align*}\nNote also that $S_{x'x'} = \\sum_{i=1}^n{x'}_i^2$, so\n\\begin{align*}\nV(\\hat{\\underline\\beta}^c) &= \\frac{\\sigma^2}{nS_{X'X'}}\\begin{bmatrix}\\sum {x'}_i^2 & 0\\\\0 & n\\end{bmatrix}\\\\\n& = \\sigma^2\\begin{bmatrix}n^{-1} & 0 \\\\ 0 & (\\sum_{i=1}^n{x'}_i^2)^{-1}\\end{bmatrix}\n\\end{align*}\n\n$\\implies$ *no covariance*!!!\n\n:::\n::: {.column width=\"50%\"}\nSimulations with same data, but one uses centered data (code in L02 Rmd).\n\n\\pspace\n\n\n::: {.cell hash='L12-Extra_Topics_cache/html/unnamed-chunk-2_be27138ba51913a765d81a00b7845e60'}\n::: {.cell-output-display}\n![](L12-Extra_Topics_files/figure-html/unnamed-chunk-2-1.png){width=480}\n:::\n:::\n\n:::\n::::\n\n\n### Comments on $V(\\hat{\\underline\\beta})$\n\n$$\nV(\\hat{\\underline\\beta}^c) = \\sigma^2\\begin{bmatrix}n^{-1} & 0 \\\\ 0 & (\\sum_{i=1}^n{x'}_i^2)^{-1}\\end{bmatrix}\n$$\n\n- $V(\\hat\\beta_0) = \\sigma^2/n \\implies sd(\\hat\\beta_0) = \\sigma/\\sqrt{n}$.\n    - The t-test for significance of $\\beta_0$ is just a hypothesis test for $\\bar y = 0$. \\lspace\n- Note that $\\underline y$ hasn't changed, so $\\hat{\\underline\\epsilon}$ and $\\sigma^2$ are unchanged.\n- $V(\\hat\\beta_0) = \\sigma^2/\\sum_{i=1}^n{x'}_i^2$ isn't all that interesting...\n\n### Standardizing $\\underline x$\n\nIn addition to mean-centering, divide by the sd of $\\underline x$:\n$$\nz_i = \\frac{x_i - \\bar x}{\\sqrt{S_{XX}/(n-1)}}\n$$\n\nThen $\\bar z = 0$ and $sd(z) = 0 \\implies S_{ZZ} = n-1$.\n\n\n\\pspace\\pause\n\nIt can be shown that:\n$$\nV(\\hat{\\underline\\beta}^s) = \\sigma^2\\begin{bmatrix}n^{-1} & 0 \\\\ 0 & (n-1)^{-1}\\end{bmatrix}\n$$\n\n- $\\underline x$ doesn't matter!!!\n\n### Standardizing in Multiple Linear Regression\n\nSuppose we standardize each column of $X$ (except the first).\n\nSeveral things happen:\n\n- All predictors are now in units of standard deviations!!!\n    - Coefficients are directly comparable!\n- Covariances disappear!!!\n\n\\pspace\n\nStandardizing doesn't hurt and can often help $\\implies$ it's almost always worth it!\n\n## General Linear Hypotheses\n\n### Diet vs. Exercise\n\nWhich is more important for weight loss?\n\n\\pspace\n\nWe can set this up in a linear regression framework:\n$$\n\\text{Loss}_i = \\beta_0 + \\beta_1\\text{CaloriesConsumed}_i + \\beta_2\\text{ExercisesMinutes}_i\n$$\nwhere we assume CaloriesConsumed and ExerciseMinutes are standardized.\n\n\\pspace\n\nOur question about the importance of diet versus exercise becomes a hypothesis test:\n$$\nH_0:\\; \\beta_1 = \\beta_2\\text{ vs. }H_a:\\; \\beta_1 \\ne \\beta_2\n$$\nAlternatively, the null can be written as $\\beta_1 - \\beta_2 = 0$.\n\n### Linearly Independent Hypotheses\n\nIn some cases, we might have a collection of hypotheses. For ANOVA:\n$$\nH_0:\\; \\beta_2 - \\beta_1 = 0,\\; \\beta_3 - \\beta_2 = 0,\\; \\beta_4 - \\beta_3 = 0,\\dots,\\; and\\; \\beta_{p-1} - \\beta_{p-2} = 0\n$$\nThese hypotheses are **linearly indepenent**. \\pause To see why, we can write them in matrix form:\n$$\n\\begin{bmatrix}\n0 & -1 & 1 & 0 & 0 & ...\\\\\n0 & 0 & -1 & 1 & 0 & ...\\\\\n0 & 0 & 0 & -1 & 1 & ...\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & ...\\\\\n\\end{bmatrix}\\hat{\\underline\\beta} = \\underline 0\n$$\nwhere none of the rows are linear combinations of the others.\n\nWe'll use the notation $C\\underline\\beta = \\underline 0$.\n\n### Linearly Independent Hypotheses\n\nThe $C$ matrix can be row-reduced to the hypotheses $\\beta_i=0\\;\\forall i>0$. In this case, our hypothesized model is:\n$$\nY_i = \\beta_0 + \\underline\\epsilon\n$$\n\n\\pspace\n\nWe have reduced $Y = X\\underline\\beta + \\underline\\epsilon$ to $Y = Z\\underline\\alpha + \\underline\\epsilon$, where $\\underline\\alpha = (\\beta_0)$ and $Z$ is a column of ones.\n\n### Linearly Dependent Hypotheses\n\nConsider the model $Y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{11}x_1^2 + \\underline\\epsilon$ and the hypotheses:\n$$\nH_0:\\; \\beta_{11} = 0,\\ \\beta_1 - \\beta_2 = 0,\\; \\beta_1 - \\beta_2 + \\beta_3 = 0,\\; 2\\beta_1 - 2\\beta_2 + 3\\beta_3 = 0\n$$\nWe can write this as:\n$$\n\\begin{bmatrix}\n0 & 0 & 0  & 1\\\\\n0 & 1 & -1 & 0\\\\\n0 & 1 & -1 & 1\\\\\n0 & 2 & -2 & 3\n\\end{bmatrix}\\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\\\\\beta_{11}\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\\\0\\\\0\\end{bmatrix}\n$$\nWith a little work, we can show that this reduces to the model:\n$$\nY = \\beta_0 + \\beta(x_1 + x_2) + \\underline\\epsilon \\Leftrightarrow Y = Z\\underline\\alpha + \\underline\\epsilon\n$$\n\n\n### Testing General Linear Hypotheses\n\nConsider an arbitrary matrix for $C$ (not linearly dependent), such that we can row-reduce $C$ to $q$ linearly independent hypotheses.\n\n\\pspace\n\n- **Full Model**\n    - $SS_E = Y^TY - \\hat{\\underline\\beta}^TX^TY$ on $n-p$ df.\\lspace\n- **Hypothesized Model**\n    - $SS_W = Y^TY - \\hat{\\underline\\alpha}^TZ^TY$ on $n-p-q$ df.\n\n\\pspace\n\nFrom these, we get:\n$$\n\\left(\\frac{SSW-SSE}{q}\\right)/\\left(\\frac{SSE}{n-p}\\right) \\sim F_{q, n-p}\n$$\nIn other words, we test whether the restrictions significantly change the $SS_E$!\n\n## Generalized Least Squares\n\n### Main Idea\n\nWhat if the variance of $\\epsilon_i$ isn't the same for all $i$?\n\n\\pspace\n\nIn other words, $V(\\underline\\epsilon) = V\\sigma^2$ for some matrix $V$.\n\n\\pspace\n\n- The structure of $V$ changes how we approach this.\n    - Weighted least squares: $V$ is diagonal.\n    - Generalized: $V$ is symmetric and positive-definite, but otherwise arbitrary.\n\n### Transforming the Instability Away\n\nIn the model $Y = X\\underline\\beta + \\underline\\epsilon$, we want $V(Y) = I\\sigma^2$, but we have $V(Y) = V\\sigma^2$\n\n\\pspace\n\nSince $V$ is symmetric and positive-definite, we can find a matrix $P$ such that:\n$$\nP^TP = PP = P^2 = V\n$$\n\nWe can pre-multiply the model by $P^{-1}$ so that $V(P^{-1}Y) = V^{-1}V\\sigma^2 = I\\sigma^2$:\n$$\nP^{-1}Y = P^{-1}X\\underline\\beta + P^{-1}\\underline\\epsilon \\Leftrightarrow Z = Q\\underline\\beta + \\underline f\n$$\n\n### Generalized Least Squares Results\n\n\\begin{align*}\n\\underline f^T\\underline f &= \\underline\\epsilon^TV^{-1}\\underline\\epsilon = (Y - X\\underline\\beta)^TV^{-1}(Y - X\\underline\\beta)\\\\\n\\hat{\\underline\\beta} &= (X^TV^{-1}X)^{-1}X^TV^{-1}Y\\\\\nSS_T &= \\hat{\\underline\\beta}^TQ^TZ = Y^TV^{-1}X(X^TV^{-1}X)^{-1}X^TV^{-1}Y\\\\\nSST &= Z^TZ = Y^TV^{-1}Y\\\\\n\\hat Y &= X\\hat{\\underline\\beta}\\\\\n\\hat{\\underline f} &= P^{-1}(Y-\\hat Y) = P^{-1}(I-X(X^TV^{-1}X)^{-1}X^TV^{-1})Y\n\\end{align*}\n\nMost things are just switching $Y$ with $P^{-1}Y$, etc., except one...\n\n### OLS when you should have used GLS\n\nSuppose the true model has $V(\\underline\\epsilon) = V\\sigma^2$.\n\n\\pspace\n\nLet $\\hat{\\underline\\beta}_O$ be the estimate of $\\underline\\beta$ if we were to fit with Ordinary Least Squares. Then:\n\n- $E(\\hat{\\underline\\beta}_O) = \\underline\\beta$\n- $V(\\hat{\\underline\\beta}_O) = (X^TX)^{-1}X^TVX(X^TX)^{-1}\\sigma^2$\n\nThe OLS estimate is **unbiased**, but has a much higher **variance**!\n\n### Choosing $V$\n\n- For serially correlated data, $V_{ii} = 1$ and $V_{ij} = \\rho^{|i-j|}$\n    - This is choosing $V$ based on *model assumptions*!\n    - $\\rho$ must be estimated ahead of time.\\lspace\n- If we have repeteated $x$-values, we can use the estimated variance from there.\n    - Choosing $V$ based on the data\\lspace\n- In a controlled experiment, where we have known weights for different $x$-values\n    - E.g., more skilled surgeons, machine age.\n\n## Participation Questions\n\n### Q1\n\nWhich of the following will result in no correlation between $\\beta_0$ and $\\beta_1$?\n\n\\pspace\n\n1. Centering\n2. Standardizing\n3. Both centering and standardizing\n4. None of the above\n\n\n### Q2\n\nWhat's the primary reason for standardizing the predictors?\n\n\\pspace\n\n1. Remove correlation between the $\\hat\\beta$s\n2. Make it so that the variance is not a function of the $X$-values.\n3. Ensure that the values of $\\hat\\beta$ are comparable.\n4. Make it so that general linear hypotheses are possible.\n\n### Q3\n\nIn a general linear hypothesis, $q$ is the rank of the $C$ matrix in $C\\underline\\beta = \\underline 0$. \n\n1. True\n2. False\n\n### Q4\n\nGeneralized Least Squares requires strong assumptions about the matrix $V$.\n\n\\pspace\n\n1. True\n2. False\n\n### Q5\n\nIgnoring correlation/unequal variance in $\\underline\\epsilon$ will lead to a biased estimate of $\\underline\\beta$\n\n\\pspace\n\n1. True\n2. False\n\n### Q6\n\nWhat do you expect the hat matrix to be for GLS?\n\n\\pspace\n\n1. $X(X^TX)^{-1}X^TY$\n2. $X(X^TV^{-1}X)^{-1}X^TV^{-1}Y$\n3. $XV^{-1}(X^TV^{-1}X)^{-1}X^TV^{-1}Y$\n4. $XV^{-T}(X^TV^{-1}X)^{-1}X^TV^{-1}Y$\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}