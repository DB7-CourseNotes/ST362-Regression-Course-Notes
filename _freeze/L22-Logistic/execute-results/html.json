{
  "hash": "9de6b70b42c9e465575bd37cb8bf8f03",
  "result": {
    "markdown": "---\ntitle: \"Classification\"\ninstitute: \"Jam: **Love's a Logistical Thing** by PJ Parker\"\n---\n\n\n\n\n## Logistic Regression\n\n### Goal: Predict a 1\n\n- Response: 0 or 1\n    - Predictions: probability of a 1?\\lspace\n\n\n### The Logistic Function - A Sigmoidal Function\n\n\nIf $t\\in\\mathbb{R}$, then\n$$\n\\sigma(t) = \\dfrac{\\exp(t)}{1 + \\exp(t)}\\in[0,1]\n$$\nwhere $\\sigma(\\cdot)$ is the **logistic** function.\n\n\n::: {.cell hash='L22-Logistic_cache/html/unnamed-chunk-2_309e99a67702dcb14716cbeb0d40b54a'}\n::: {.cell-output-display}\n![](L22-Logistic_files/figure-html/unnamed-chunk-2-1.png){width=480}\n:::\n:::\n\n\n\n### Logistic Function - Now with Parameters\n\n\n::: {.cell layout-ncol=\"3\" hash='L22-Logistic_cache/html/unnamed-chunk-3_e8680e9cfea845fe7e8daf48911b2058'}\n::: {.cell-output-display}\n![](L22-Logistic_files/figure-html/unnamed-chunk-3-1.png){width=384}\n:::\n\n::: {.cell-output-display}\n![](L22-Logistic_files/figure-html/unnamed-chunk-3-2.png){width=384}\n:::\n\n::: {.cell-output-display}\n![](L22-Logistic_files/figure-html/unnamed-chunk-3-3.png){width=384}\n:::\n:::\n\n\n### Logistic Function - Now with Parameters *Estimated from DATA*\n\n\\vspace{1cm}\n\\begin{align*}\n\\eta(x_i) &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + ...\\\\\np(x_i) &= \\sigma(\\eta(x_i)) = \\dfrac{\\exp(\\eta(x_i))}{1 + \\exp(\\eta(x_i))}\\\\\n\\implies \\log\\left(\\frac{p_i(x_i)}{1-p_i(x_i)}\\right) &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + ...\n\\end{align*}\n\n\n\n::: {.cell hash='L22-Logistic_cache/html/unnamed-chunk-4_032ecb047b0a01058b09f69e27122e91'}\n\n```{.r .cell-code}\nlibrary(ISLR2)\nlibrary(ggplot2)\nlibrary(dplyr)\n#| echo: false\nDefault %>% \n    mutate(default = as.numeric(factor(default)) - 1) %>%\n    ggplot() + theme_minimal() +\n        aes(x = balance, y = default) + \n        geom_jitter(width = 0, height = 0.05) +\n        geom_smooth(method = \"glm\", se = FALSE,\n            method.args = list(family = \"binomial\")) +\n        labs(x = \"Credit Card Balance\",\n            y = \"Default?\")\n```\n\n::: {.cell-output-display}\n![](L22-Logistic_files/figure-html/unnamed-chunk-4-1.png){width=384}\n:::\n:::\n\n\n\\centering\n$\\eta(x_i) = -10.65 + 0.0054\\cdot\\text{balance}_i$\n\n\n### Logistic Regression\n\n- The response is 0 or 1 (no or yes, dont' default or default, etc.)\\lspace\n- The probability of a 1 increases according to the sigmoid function.\n    - The **linear predictor** is $\\eta(x_i) = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\cdots$\n    - The probability of class 1 is $P(\\text{class }1 | \\text{predictors}) = \\sigma(\\eta(x_i))$\\lspace\n- Instead of normality assumptions, we use a binomial distribution.\n\nIt's just one step away from a linear model!\n\n### Interpreting Parameters\n\n- General structure: \"For each one unit increase in $x_i$, some function of $y_i$ changes by some function of $\\beta$\"\".\\newline\\pause\n- For logistic regression:\n    - For each one unit increase in $x_i$, $\\log\\left(\\frac{p(x_i)}{1-p(x_i)}\\right)$ increases by $\\beta$.\\pause\\newline\n- The **odds** are $\\frac{p(x_i)}{1-p(x_i)}$.\n    - \"1 in 5 people with offs of 1/4 will default on their loan.\"\\newline\n- $\\beta$ represents the **change in log odds** for a one unit increase.\n    - \"**log odds _ratio_**\".\n\n### Estimating Parameters: Maximum Likelihood\n\nFor all observations:\n\n- If $y_i = 0$, we want $p(x_i)$ to be as *low* as possible.\n    - Maximize $1 - P(Y_{i'} = 1|\\beta_0,\\beta_1,X)$\\lspace\n- If $y_i = 1$, we want $p(x_i)$ to be as *high* as possible.\n    - Maximize $P(Y_{i'} = 1|\\beta_0,\\beta_1,X)$\n\n\\quad\n\nThese can be combined as:\n$$\n\\ell(\\beta_0,\\beta_1) = \\prod_{i':y_{i'} = 0}(1 - P(Y_{i'} = 1|\\beta_0,\\beta_1,X))\\prod_{i:y_i=1}P(Y_i = 1|\\beta_0,\\beta_1,X)\n$$\nWhich is NOT just the sum of squared errors!\n\nUnlike linear regression, there's no closed form for $\\hat\\beta_0$ and $\\hat\\beta_1$ $\\Rightarrow$ need numerical methods.\n\n### Examples: Two different predictors in the `Default` data\n\n:::: {.columns}\n::: {.column width=\"45%\"}\n\n::: {.cell hash='L22-Logistic_cache/html/unnamed-chunk-5_913f38a88387d8d02e8999c517340293'}\n::: {.cell-output-display}\n![](L22-Logistic_files/figure-html/unnamed-chunk-5-1.png){width=384}\n:::\n:::\n\n\n$$\n\\eta(x_i) = -3.5 + 0.5\\cdot\\text{student}\n$$\n\nThe odds of a student defaulting are $\\exp(0.5)\\approx1.65$ times as high as a non-student.\\pause\n:::\n::: {.column width=\"45%\"}\n\n::: {.cell hash='L22-Logistic_cache/html/unnamed-chunk-6_6ed6a2375cb7c2cc8f5cf513fdf65064'}\n::: {.cell-output-display}\n![](L22-Logistic_files/figure-html/unnamed-chunk-6-1.png){width=384}\n:::\n:::\n\n\n$$\n\\eta(x_i) = -10.65 + 0.005\\cdot\\text{balance}\n$$\n\nEach extra dollar of credit card balance increases the odds of defaulting by a factor of 1.005.\n:::\n::::\n\n\\pause\n*The scale of the predictors matters.*\n\n### Odds versus Probabilities\n\n\"The odds of a student defaulting are $\\exp(0.5)\\approx1.65$ times as high as a non-student.\"\n\n$$\n\\frac{P(\\text{defaulting} | \\text{student} = 1)}{1 - P(\\text{defaulting} | \\text{student} = 1)} \\biggm/ \\frac{P(\\text{defaulting} | \\text{student} = 0)}{1 - P(\\text{defaulting} | \\text{student} = 0)} = 1.65\n$$\nThis **cannot** be solved for $P(\\text{defaulting} | \\text{student} = 1)$!\n\n\\quad\\pause\n\n$$\nP(\\text{defaulting} | \\text{student} = 1) = \\dfrac{\\exp(\\eta(x_i))}{1 + \\exp(\\eta(x_i))} = \\dfrac{\\exp(-3.5 + 0.5\\cdot 1)}{1 + \\exp(-3.5 + 0.5\\cdot 1)} \\approx 0.047\n$$\n\n\n### Multiple ~~Linear~~ Logistic Regression\n\n- Predictors can be **multicollinear**, **confounded**, and have **interactions**.\n    - Logistic is just Linear on a transformed scale!\\lspace\n- We do *not* look for transformations of the response.\n    - It's already a transformation of the response $p_i(x_i)$!\\lspace\n- We *do* look for transformations of the predictors!\n    - Sigmoid + Polynomial is where the real fun is.\\lspace\n\n### Errors in Logistic Regression: Deviance\n\n- All \"errors\" are either $p(x_i)$ or $1 - p(x_i)$.\n    - Distance from either 0 or 1.\n\n\\pspace\n\nInstead, we use the *deviance*.\n\n- If $p(x_i)$ were the true probability in a binomial distribution, what's the probability of the observed value (0 or 1)? \n    - This is used more broadly in **Generalized Linear Models** (GLMs). Logistic Regression is one of many GLMs.\n\n\n\n\n### Logistic Decision Boundaries\n\n$$\nP(\\text{defaulting} | \\eta(x_i)) > p \\implies a + bx_1 + cx_2 + dx_3 > e\n$$\n\nFor some (linear) hyperplane $a + bx_1 + cx_2 + dx_3$ and some value $e$.\n\n\\quad\n\n- Choosing $p=0.5$ is logical, but other thresholds can be chosen.\n    - Cancer example: want to be more admissive of false positives\n        - Would rather operate and be wrong than falsely tell the patient that they're healthy!\n\n\n\n### \n\n\n\n::: {.cell hash='L22-Logistic_cache/html/decision_boundary_109245f194bb6291d42bc501dc177299'}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(ISLR2)\n\nDefault$default <- as.numeric(factor(Default$default)) - 1\nDefault$student <- as.numeric(factor(Default$student)) - 1\n\ndecision_grid <- expand.grid(\n    student = c(0,1),\n    balance = seq(0, 2655, length.out = 250),\n    income= seq(770, 73555, length.out = 250)\n)\n\nmy_glm <- glm(default ~ student + balance + income,\n    data = Default, family = binomial)\ndecision_grid$pred <- predict(my_glm, newdata = decision_grid)\n\nstudent_labels <- c(\"Not Student\", \"Student\")\nnames(student_labels) <- c(0, 1)\n\nggplot() + theme_minimal() +\n    geom_tile(data = decision_grid,\n        mapping = aes(x = balance, y = income, fill = factor(pred > 0.5))) +\n    scale_fill_manual(values = c(\"firebrick\", \"green\", \"firebrick\", \"green\")) +\n    geom_point(data = Default, \n        mapping = aes(x = balance, y = income, \n            fill = factor(default == 1)),\n        shape = 21) +\n    facet_wrap(~ student, \n        labeller = labeller(student = student_labels)) +\n    labs(x = \"Credit Card Balance\",\n        y = \"Income\",\n        fill = \"Default?\")\n```\n\n::: {.cell-output-display}\n![](L22-Logistic_files/figure-html/decision_boundary-1.png){width=864}\n:::\n:::\n\n\n\n### Predictions - Just Plug it In!\n\n\n\n\n\n| | Intercept | Student | Balance | Income |\n| --- | --- | --- | --- | --- |\n| $\\beta$ | -10.09 | -0.65 | 0.0057 | 0.000003 |\n\nWe can make a prediction for a student with $2,000 balance and $20,000 income:\n\\begin{align*}\n\\eta(x) &= \\beta_0 + \\beta_1\\cdot 1 + \\beta_2\\cdot 2000 + \\beta_3\\cdot 20000 \\approx 0.0178\\\\\n&\\\\\nP(\\text{defaulting} | x) &= \\dfrac{\\exp(\\eta(x))}{1 + \\exp(\\eta(x))} \\approx \\dfrac{\\exp(0.0178)}{1 + \\exp(0.0178)} \\approx 0.504\\\\\n&\\\\\n&P(\\text{defaulting} | x) > 0.5 \\implies \\text{Predict Default}\n\\end{align*}\n\n\n## Classification Basics\n\n### Goal: Predict a Category\n\n- **Binary:** Yes/no, success/failure, etc.\\newline\n- **Categorical:** 2 or more categories.\n    - A.k.a. qualitative, but that's a social science word.\n\n\\quad\n\nIn both: predict whether an observation is in category $j$ given its predictors.\n$$\nP(Y_i = j| x = x_i) \\stackrel{def}{=} p_j(x_i)\n$$\n\n\n### Classification Confusion\n\n**Confusion Matrix:** A tabular summary of classification errors.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n| | True Pay ($\\cdot 0$) | True Def ($\\cdot 1$)|\n|---|---|---|\n| Pred Pay ($0 \\cdot$) | Good (00) | Bad (01) |\n| Pred Def ($1 \\cdot$) | Bad (10) | Good (11) |\n\n\n:::\n::: {.column width=\"50%\"}\n\\vspace{0.25cm}\n\n- Two ways to be wrong\\newline\n- Two ways to be right\\newline\n- Different applications have different needs\n:::\n::::\n\\quad\\pause \n\n\\quad\\centering\n\n**Accuracy:** $\\dfrac{\\text{Correct Predictions}}{\\text{Number of Predictions}} =\\frac{00 + 11}{00 + 01 + 10 + 11}$\n\n\n### Is \"Accuracy\" Good?\n\nTask: Predict whether a person has cancer \n\n(In this made up example, 0.02\\% of people have cancer).\n\n\\quad\n\n| | True Healthy | True Cancer |\n|---|---|---|\n| Pred. Healthy | Save a Life | Lose a Life |\n| Pred. Cancer | Expensive/Invasive | All good |\n\n\\quad\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n- **Easy:**  99.8\\% accuracy.\n    - Always guess \"Not Cancer\"\n:::\n::: {.column width=\"50%\"}\n- **Very Hard:** 99.82\\% accuracy.\n:::\n::::\n\n\n\n### The Confusion Matrix for Default Data\n\n\n\n\n\n| | True Payment | True Default |\n| --- | --- | --- |\n| Pred Payment | 9627 | 228 |\n| Pred Default | 40 | 105 |\n\n- This model: 97.32% accuracy.\n    - Naive model: always predict \"Pay\" - 96.67% accuracy!\n\nOther important measures (not on exam): \n\n- Sensitivity: $\\dfrac{\\text{True Positives}}{\\text{All Positives in Data}} = \\dfrac{9627}{9627 + 40} = 99.58%$ (Naive: 100%)\\lspace\n- Specificity: $\\dfrac{\\text{True Negatives}}{\\text{All Negatives in Data}} = \\dfrac{105}{105 + 228} = 31.53$ (Naive: 0%)\n\n### Logistic Regression in R\n\nSee Course Notes\n\n:::notes\nModel building works very similarly, but it's *very* difficult to interpret the residual plots.\n\n\n::: {.cell hash='L22-Logistic_cache/html/unnamed-chunk-10_6e19880e949524d7b267a8fd1fbdd002'}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\npeng <- penguins[complete.cases(penguins), ]\n\nlog_cont <- glm(sex ~ bill_length_mm + \n        bill_depth_mm + flipper_length_mm,\n    data = peng, family = \"binomial\")\n\nanova(log_cont, test = \"Chisq\") # Sequential\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: sex\n\nTerms added sequentially (first to last)\n\n                  Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \nNULL                                332     461.61              \nbill_length_mm     1   41.185       331     420.42 1.385e-10 ***\nbill_depth_mm      1   96.786       330     323.64 < 2.2e-16 ***\nflipper_length_mm  1   72.666       329     250.97 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nlog_spec <- update(log_cont, ~ . + species * flipper_length_mm)\n\nanova(log_cont, log_spec, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm\nModel 2: sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + \n    flipper_length_mm:species\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n1       329     250.97                          \n2       325     174.71  4   76.265 1.076e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nfull_spec <- glm(sex ~ species*(bill_length_mm + \n        bill_depth_mm + flipper_length_mm),\n    data = peng, family = \"binomial\")\n\nanova(log_spec, full_spec, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + \n    flipper_length_mm:species\nModel 2: sex ~ species * (bill_length_mm + bill_depth_mm + flipper_length_mm)\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)\n1       325     174.71                     \n2       321     170.32  4   4.3894   0.3559\n```\n:::\n\n```{.r .cell-code}\nanova(log_spec, update(log_spec, ~ . - species:flipper_length_mm), test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + \n    flipper_length_mm:species\nModel 2: sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)\n1       325     174.71                     \n2       327     178.78 -2  -4.0697   0.1307\n```\n:::\n\n```{.r .cell-code}\nsummary(log_spec)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + \n    species + flipper_length_mm:species, family = \"binomial\", \n    data = peng)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.2365  -0.3125   0.0033   0.3252   2.5852  \n\nCoefficients:\n                                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                        -69.63032   10.83543  -6.426 1.31e-10 ***\nbill_length_mm                       0.65617    0.10521   6.237 4.47e-10 ***\nbill_depth_mm                        1.96180    0.29827   6.577 4.79e-11 ***\nflipper_length_mm                    0.04329    0.04375   0.990   0.3224    \nspeciesChinstrap                   -38.57778   22.93391  -1.682   0.0925 .  \nspeciesGentoo                      -34.58899   20.48998  -1.688   0.0914 .  \nflipper_length_mm:speciesChinstrap   0.15957    0.11696   1.364   0.1725    \nflipper_length_mm:speciesGentoo      0.15988    0.09666   1.654   0.0981 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 461.61  on 332  degrees of freedom\nResidual deviance: 174.71  on 325  degrees of freedom\nAIC: 190.71\n\nNumber of Fisher Scoring iterations: 7\n```\n:::\n\n```{.r .cell-code}\ncoef(log_spec)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                       (Intercept)                     bill_length_mm \n                      -69.63031836                         0.65616618 \n                     bill_depth_mm                  flipper_length_mm \n                        1.96180392                         0.04329349 \n                  speciesChinstrap                      speciesGentoo \n                      -38.57777987                       -34.58899421 \nflipper_length_mm:speciesChinstrap    flipper_length_mm:speciesGentoo \n                        0.15956974                         0.15987824 \n```\n:::\n:::\n\n\nThe residual plots are the same as before:\n\n\n::: {.cell hash='L22-Logistic_cache/html/unnamed-chunk-11_9e50c3d742d251650fcc7451eaffb86a'}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(log_spec)\n```\n\n::: {.cell-output-display}\n![](L22-Logistic_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThe predictions can either be on the logit scale (`type = \"link\"`, the default) or on the response scale (probabilities).\n\n\n::: {.cell hash='L22-Logistic_cache/html/unnamed-chunk-12_56b33efc2dce282d84480efb3f042589'}\n\n```{.r .cell-code}\npredict(log_spec, type = \"response\") |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        1         2         3         4         5         6 \n0.6335863 0.1789059 0.6382736 0.6613775 0.9918045 0.2059974 \n```\n:::\n:::\n\n\nRegularization is often used with logistic regression (in python's scikit-learn package, Ridge regularization is used by default without warning the user).\n\n\n::: {.cell hash='L22-Logistic_cache/html/unnamed-chunk-13_ffac1882fb16317566de3b3fba070a09'}\n\n```{.r .cell-code}\nlibrary(glmnet)\n\nX <- model.matrix(sex ~ species*(bill_length_mm + \n        bill_depth_mm + flipper_length_mm),\n    data = peng)\ny <- as.factor(peng$sex)\n\nmycv <- cv.glmnet(X, y, family = binomial)\n\nmylasso <- glmnet(X, y,\n    data = peng, family = \"binomial\", alpha = 1,\n    lambda = mycv$lambda.1se)\ncoef(mylasso)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n                                             s0\n(Intercept)                        -46.90824021\n(Intercept)                          .         \nspeciesChinstrap                    -3.49235983\nspeciesGentoo                        .         \nbill_length_mm                       0.32778935\nbill_depth_mm                        1.26452716\nflipper_length_mm                    0.05765817\nspeciesChinstrap:bill_length_mm      .         \nspeciesGentoo:bill_length_mm         .         \nspeciesChinstrap:bill_depth_mm       .         \nspeciesGentoo:bill_depth_mm          .         \nspeciesChinstrap:flipper_length_mm   .         \nspeciesGentoo:flipper_length_mm      .         \n```\n:::\n:::\n\n:::\n\n## Multinomial Regression\n\n### Multinomial Logistic Regression: K Classes\n\nWe have a total probability of 1 to distribute across the classes,\\pause\n\n- **Stick breaking**\n    1. Fit a logistic regression of `class 1` versus `not class 1`.\n        - Remove obs. with `class 1`\n    2. Fit a logistic regression of `class 2` versus `not class 1`.\n        - Remove obs. with `class 2`\n    3. ...\n    4. Class $K$ gets whatever probability is left over.\\pause\\newline\n- **Softmaxing**\n    1. For all classes, fit a logistic regression of `class k` versus `not class k`.\n    2. In the end, divide by the total probability to make sure they sum to 1.\n    - Very often used in machine learning!\n\nThese two give the same results!\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}