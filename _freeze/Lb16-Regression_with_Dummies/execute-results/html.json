{
  "hash": "8ac72ac2366ed477aaccae3abff5c0bc",
  "result": {
    "markdown": "---\ntitle: \"Regression with Categorical Predictors\"\noutput: html_notebook\n---\n\n\n## Regression as a t-test\n\nWhen we do regression against a dummy variable, we are actually just doing a t-test.\n\n-   A regression models the *mean* of $y$ for each value of $x$.\n-   A one-unit increase in $X$ is the difference between 0 and 1.\n    -   The slope is the difference in *means*.\n-   We assume constant variance\n    -   Same variance at $x=0$ and $x=1$.\n-   A t-test has a df of $n-1$, so does a regression with just one predictor.\n\n\n::: {.cell hash='Lb16-Regression_with_Dummies_cache/html/unnamed-chunk-1_33fe025836388884c6d693591fc19f87'}\n\n```{.r .cell-code}\n# Note that I need the var.equal=TRUE to match the assumptions of regression.\nt.test(mpg ~ am, data = mtcars, var.equal = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tTwo Sample t-test\n\ndata:  mpg by am\nt = -4.1061, df = 30, p-value = 0.000285\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -10.84837  -3.64151\nsample estimates:\nmean in group 0 mean in group 1 \n       17.14737        24.39231 \n```\n:::\n:::\n\n::: {.cell hash='Lb16-Regression_with_Dummies_cache/html/unnamed-chunk-2_13733288b525d05652622412ec0a8a09'}\n\n```{.r .cell-code}\nlm(mpg ~ am, data = mtcars) |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3923 -3.0923 -0.2974  3.2439  9.5077 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   17.147      1.125  15.247 1.13e-15 ***\nam             7.245      1.764   4.106 0.000285 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.902 on 30 degrees of freedom\nMultiple R-squared:  0.3598,\tAdjusted R-squared:  0.3385 \nF-statistic: 16.86 on 1 and 30 DF,  p-value: 0.000285\n```\n:::\n:::\n\n\nThe two outputs have the exact same test statistic and p-value!\n\n## Categorical Predictors as ANOVA\n\nIn order for R to recognize a variable as a dummy variable, we need to tell it that the values should be interpreted as a `factor`. This is a very particular data type:\n\n-   Every observation is one `level` of the categorical variable.\n\n-   Every observation can only be *one* level of that categorical variable\n\n    -   E.g., a car can't have 4 cylinders *and* 6 cylinders\n\n-   There are a finite number of possible categories.\n\n    -   Unless we specify, R assumes that the unique values that it sees constitutes all the possible categories. In other words, it won't let us ask about cars with 5 cylinders because we're making a `factor` variable with 4, 6, and 8 as the observed values.\n\n\n::: {.cell hash='Lb16-Regression_with_Dummies_cache/html/unnamed-chunk-3_a8f75bdc65296b23efb585760f730a45'}\n\n```{.r .cell-code}\nanova(aov(mpg ~ factor(cyl), data = mtcars))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: mpg\n            Df Sum Sq Mean Sq F value    Pr(>F)    \nfactor(cyl)  2 824.78  412.39  39.697 4.979e-09 ***\nResiduals   29 301.26   10.39                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell hash='Lb16-Regression_with_Dummies_cache/html/unnamed-chunk-4_503dd5b76deeab1953592fc163cb5c24'}\n\n```{.r .cell-code}\nsummary(lm(mpg ~ factor(cyl), data = mtcars))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ factor(cyl), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2636 -1.8357  0.0286  1.3893  7.2364 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   26.6636     0.9718  27.437  < 2e-16 ***\nfactor(cyl)6  -6.9208     1.5583  -4.441 0.000119 ***\nfactor(cyl)8 -11.5636     1.2986  -8.905 8.57e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.223 on 29 degrees of freedom\nMultiple R-squared:  0.7325,\tAdjusted R-squared:  0.714 \nF-statistic:  39.7 on 2 and 29 DF,  p-value: 4.979e-09\n```\n:::\n:::\n\n\nThe overall test of significance has an F-value of 39.7 and a p-value of 4.979x10$^{-9}$\n\nBut what is up with the `summary.lm()` output? Why do we have a row labelled `factor(cyl)6`??\n\n## Coding Dummy Variables (what `factor` is actually doing)\n\nWhen we have multiple variables, we set one aside as the \"**reference**\" category. For a factor variable with $k$ categories, we set up $k-1$ new variables to denote category membership. For example:\n\n-   `4` is the reference category for `cyl`. If all the new dummy variables that we create are all 0, then the car must be 4 cylinders.\n\n-   We set up a dummy variable for `6` cylinders, which is a column with a 1 if the car has 6 cylinders and a 0 otherwise. This is what's labelled as `factor(cyl)6` in the output.\n\n    -   This can be denoted $I(cyl == 6)$.\n\n-   Similarly, we have `factor(cyl)8` = $I(cyl == 8)$.\n\nConsider the model $y = \\beta_0 + \\beta_1I(cyl == 6) + \\beta_2I(cyl == 8)$.\n\nWith this setup, the intercept represents the estimate for 4 cylinder cars, $\\beta_1$ represents the difference in 4 and 6 cylinder cars, while $\\beta_2$ represents the difference in 4 and 8 cylinder cars (you can find the difference between 6 and 8 by doing the right math).\n\nFrom `model.matrix()`, we can see this in action:\n\n\n::: {.cell hash='Lb16-Regression_with_Dummies_cache/html/unnamed-chunk-5_4deec62843651705ee4442eb2807647f'}\n\n```{.r .cell-code}\ncbind(model.matrix(mpg ~ factor(cyl), data = mtcars), cyl = cbind(mtcars$cyl)) |> head(12)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  (Intercept) factor(cyl)6 factor(cyl)8  \nMazda RX4                   1            1            0 6\nMazda RX4 Wag               1            1            0 6\nDatsun 710                  1            0            0 4\nHornet 4 Drive              1            1            0 6\nHornet Sportabout           1            0            1 8\nValiant                     1            1            0 6\nDuster 360                  1            0            1 8\nMerc 240D                   1            0            0 4\nMerc 230                    1            0            0 4\nMerc 280                    1            1            0 6\nMerc 280C                   1            1            0 6\nMerc 450SE                  1            0            1 8\n```\n:::\n:::\n\n\nThe model can be written as:\n\n$$\ny_i = \\begin{cases}\\beta_0 & \\text{if }\\; cyl == 4\\\\ \\beta_0 + \\beta_1 & \\text{if }\\; cyl == 6\\\\\\beta_0  + \\beta_2 & \\text{if }\\; cyl == 8\\end{cases}\n$$\n\nAnd indeed we can show that this is equivalent to fitting three separate models:\n\n\n::: {.cell hash='Lb16-Regression_with_Dummies_cache/html/unnamed-chunk-6_ca9140e3134cba536e4e88d0693988da'}\n\n```{.r .cell-code}\ncyl4 <- coef(lm(mpg ~ 1, data = subset(mtcars, cyl == 4)))\ncyl6 <- coef(lm(mpg ~ 1, data = subset(mtcars, cyl == 6)))\ncyl8 <- coef(lm(mpg ~ 1, data = subset(mtcars, cyl == 8)))\nallcyl <- coef(lm(mpg ~ factor(cyl), data = mtcars))\nprint(c(cyl4 = unname(cyl4), beta0=unname(allcyl[1])))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    cyl4    beta0 \n26.66364 26.66364 \n```\n:::\n\n```{.r .cell-code}\nprint(c(cyl6 = unname(cyl6), beta0_plus_beta1 = unname(allcyl[1] + allcyl[2])))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            cyl6 beta0_plus_beta1 \n        19.74286         19.74286 \n```\n:::\n\n```{.r .cell-code}\nprint(c(cyl8 = unname(cyl8), beta0_plus_beta2 = unname(allcyl[1] + allcyl[3])))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            cyl8 beta0_plus_beta2 \n            15.1             15.1 \n```\n:::\n:::\n\n\nThe advantage of having all three in one is that we can test for significance easily!\n\nOne way (a bad way) to visualize this is to treat $I(cyl ==6)$ and $I(cyl==8)$ as separate variables:\n\n\n::: {.cell hash='Lb16-Regression_with_Dummies_cache/html/unnamed-chunk-7_cc6eef5b602047f9b63260271605fa76'}\n\n```{.r .cell-code}\npar(mfrow = c(1,2))\nplot(mpg ~ I(as.numeric(factor(cyl)) - 1), data = subset(mtcars, cyl %in% c(4,6)),\n    xlab = \"Cyl (0 = 4, 1 = 6)\", main = \"I(cyl == 6)\")\nabline(lm(mpg ~ factor(cyl), data = subset(mtcars, cyl %in% c(4,6))))\nplot(mpg ~ I(as.numeric(factor(cyl)) - 1), data = subset(mtcars, cyl %in% c(4,8)),\n    xlab = \"Cyl (0 = 4, 1 = 8)\", main = \"I(cyl == 8)\")\nabline(lm(mpg ~ factor(cyl), data = subset(mtcars, cyl %in% c(4,8))))\n```\n\n::: {.cell-output-display}\n![](Lb16-Regression_with_Dummies_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nFrom this, we can see that the slope of the line is indeed looking at the difference in means.\n\n### Categorical and Continuous Variables\n\nIf we have `cyl` and `disp` in the model, we get the following:\n\n$$\ny = \\beta_0 + \\beta_1I(cyl == 6) + \\beta_2I(cyl == 8) + \\beta_3 disp\n$$ which is equivalent to: $$\ny_i = \\begin{cases}\\beta_0  + \\beta_3 disp& \\text{if }\\; cyl == 4\\\\ \\beta_0 + \\beta_1  + \\beta_3 disp& \\text{if }\\; cyl == 6\\\\\\beta_0  + \\beta_2  + \\beta_3 disp& \\text{if }\\; cyl == 8\\end{cases}\n$$ This is three different models of mpg versus disp, but with a different intercept depending on the value of `cyl`.\n\n\n::: {.cell hash='Lb16-Regression_with_Dummies_cache/html/unnamed-chunk-8_850472dabbe0ffaa97dd5bfcaf06ab54'}\n\n```{.r .cell-code}\ncyldisp <- coef(lm(mpg ~ factor(cyl) + disp, data = mtcars))\ncyldisp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n (Intercept) factor(cyl)6 factor(cyl)8         disp \n 29.53476781  -4.78584624  -4.79208587  -0.02730864 \n```\n:::\n\n```{.r .cell-code}\nplot(mpg ~ disp, col = factor(cyl), data = mtcars)\nabline(a = cyldisp[1], b = cyldisp[4], col = 1, lty = 1)\nabline(a = cyldisp[1] + cyldisp[2], b = cyldisp[4], col = 2, lty = 1)\nabline(a = cyldisp[1] + cyldisp[3], b = cyldisp[4], col = 3, lty = 2)\n```\n\n::: {.cell-output-display}\n![](Lb16-Regression_with_Dummies_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThe plot looks like it only has 2 lines, but that's because $\\beta_1=\\beta_2$, so one line is plotted on top of the other! I changed the linetypes so you can see this.\n\nIt looks like the red and green lines are fitting the red and green data, but the black line doesn't look quite right.\n\n### Three Different Models\n\nIf we have an **interaction** between `cyl` and `disp`, then we essentially get 3 models. $$\ny = \\beta_0 + \\beta_1I(6) + \\beta_2I(8) + \\beta_3 disp + \\beta_4I(6)disp + \\beta_5I(8)disp\n$$ where $I(6)$ is just shorthand for $I(cyl == 6)$.\n\nThis is the same as: $$\ny_i = \\begin{cases}\\beta_0  + \\beta_3 disp& \\text{if }\\; cyl == 4\\\\ (\\beta_0 + \\beta_1)  + (\\beta_3 + \\beta_4) disp& \\text{if }\\; cyl == 6\\\\(\\beta_0  + \\beta_2)  + (\\beta_3 + \\beta_5) disp& \\text{if }\\; cyl == 8\\end{cases}\n$$\n\nIn R, we cans use the fanciness of the formula notation. R interprets `*` as interaction *as well as lower order terms.*\n\n\n::: {.cell hash='Lb16-Regression_with_Dummies_cache/html/unnamed-chunk-9_509a5a17e09d63c722ecdef3df3abf56'}\n\n```{.r .cell-code}\ncoef(lm(mpg ~ disp * factor(cyl), data = mtcars))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      (Intercept)              disp      factor(cyl)6      factor(cyl)8 \n       40.8719553        -0.1351418       -21.7899679       -18.8391564 \ndisp:factor(cyl)6 disp:factor(cyl)8 \n        0.1387469         0.1155077 \n```\n:::\n\n```{.r .cell-code}\ncoef(lm(mpg ~ disp, data = subset(mtcars, cyl == 4))) # Others will be similar\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)        disp \n 40.8719553  -0.1351418 \n```\n:::\n:::\n\n\n`ggplot2` makes it super easy to plot this.\n\n\n::: {.cell hash='Lb16-Regression_with_Dummies_cache/html/unnamed-chunk-10_44d1d3e43e8b7b78a5299dcb63549c89'}\n\n```{.r .cell-code}\nlibrary(ggplot2); theme_set(theme_bw())\nggplot(mtcars) +\n    aes(x = disp, y = mpg, colour = factor(cyl)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = \"y ~ x\")\n```\n\n::: {.cell-output-display}\n![](Lb16-Regression_with_Dummies_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nRecall that the model without interaction terms (different intercepts, same slopes) had practically the same intercept for `6` and `8`. The slopes look different here, but there isn't a lot of data in the `6` category and I suspect that we could force it to have the same intercept and slope as the `8` category.\n\n## Comparison to Other Models\n\nIn previous lectures, we've looked at this same relationship using polynomial model for displacement and a transformation for mpg. Let's see how these stack up!\n\nHow do we compare such models? There isn't a statistical test for which one fits best, but, as always, we want to know about the residuals!\n\n\n::: {.cell hash='Lb16-Regression_with_Dummies_cache/html/unnamed-chunk-11_66ae84728ad4438fc340e2d2d10b0c3a'}\n\n```{.r .cell-code}\npoly_lm <- lm(mpg ~ poly(disp, 2), data = mtcars)\nlog_lm <- lm(log(mpg) ~ disp, data = mtcars)\ninteract_lm <- lm(mpg ~ factor(cyl)*disp, data = mtcars)\n```\n:::\n\n::: {.cell hash='Lb16-Regression_with_Dummies_cache/html/unnamed-chunk-12_89f497b5509d87de084ac0e8ecaeb7ab'}\n\n```{.r .cell-code}\npar(mfrow = c(1,3))\n\nresid_plot <- 1 # Re-run with different values to check other plots\n# Use similar colours to the ggplot.\nmycolours <- c(\"red\", \"green\", \"blue\")[as.numeric(factor(mtcars$cyl))]\n\nplot(poly_lm, which = resid_plot, main = \"Polynomial\", col = mycolours)\nplot(log_lm, which = resid_plot, main = \"Log Transform\", col = mycolours)\nplot(interact_lm, which = resid_plot, main = \"Interaction\", col = mycolours)\n```\n\n::: {.cell-output-display}\n![](Lb16-Regression_with_Dummies_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n-   Residuals versus fitted looks quite a bit better for the interaction model.\n\n-   Normal Q-Q also looks better for interaction model.\n\n-   Scale-Location is slightly better for interaction model, although still not perfect.\n\n-   Residuals versus leverage indicates the Hornet 4 Drive car has somewhat high leverage. I'm guessing this is the largest green dot in the plot up above - the green line would be very different without it.\n\n    -   This highlights the importance of carefully interpreting Cook's Distance. In this case, the interaction model is a combination of three different models.\n\nThe plots are quite similar, but for the most part the interaction model seems to work best.\n\nANOVA can be used to compare the residual variance across non-nested models, but is not appropriate for one of the three models we just saw. See if you can guess which !\n\n\n::: {.cell hash='Lb16-Regression_with_Dummies_cache/html/unnamed-chunk-13_0d68b07ae3c22b1d01fc6e3cb7324219'}\n\n```{.r .cell-code}\nanova(poly_lm, log_lm, interact_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: mpg ~ poly(disp, 2)\nModel 2: mpg ~ factor(cyl) * disp\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \n1     29 233.39                                \n2     26 146.23  3    87.159 5.1655 0.006199 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe models have a significantly *different* fit, so the one with the lowest residual variance probably fits better.\n\n\n::: {.cell hash='Lb16-Regression_with_Dummies_cache/html/unnamed-chunk-14_ac600d636185d56091e65727e0aa8c16'}\n\n```{.r .cell-code}\nc(summary(poly_lm)$sigma, summary(interact_lm)$sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.836907 2.371581\n```\n:::\n:::\n\n\nThere are two important points here:\n\n1.  The interaction model fits the context of the problem. It absolutely makes sense that an engine with 4 cylinders is different from an engine with 6 cylinders, and part of that difference is the relationship between mpg and displacement. There are no two engines which are equivalent except someone added two cylinders to it; the cylinder values represent fundamentally different groups.\n    -   In other words, the theory supports an interaction model. There's no theory that I know of that states there should be a quadratic relationship between mpg and displacement. If we want to say something about cars in general (inference), it's best to go with the context of the problem.\n2.  The interaction model *should* fit better since it contains more information - it has the displacement *and* the number of cylinders, the other models only had displacement.\n    -   Also note that the interaction model takes up more degrees of freedom. This can be a negative, especially with small samples.\n\n### This is an ANCOVA Model\n\n\n::: {.cell hash='Lb16-Regression_with_Dummies_cache/html/unnamed-chunk-15_1a55e393e025d9372e70a6d633a74ee7'}\n\n```{.r .cell-code}\nanova(interact_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: mpg\n                 Df Sum Sq Mean Sq F value   Pr(>F)    \nfactor(cyl)       2 824.78  412.39 73.3221 2.05e-11 ***\ndisp              1  57.64   57.64 10.2487 0.003591 ** \nfactor(cyl):disp  2  97.39   48.69  8.6574 0.001313 ** \nResiduals        26 146.23    5.62                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe p-value for the ANCOVA test is 0.001313, indicating that there is a significantly different covariance between mpg and displacement depending on the number of cylinders.\n\n-   The lower-order terms `factor(cyl)` and `disp` *must* be present in order for this test to make sense.\n\n    -   It is technically possible to fit a model with just the interaction term, but it's slightly better to have extra predictors with a 0 coefficient than be missing predictors with a non-0 coefficient.\n\n        -   For the curious, the syntax for this model would be `lm(mpg ~ factor(cyl):disp, data = mtcars)`, where the `:` indicates multiplication. This isn't used often, since it's almost always incorrect to include interaction terms without the individual effects.\n\n        -   It's not clearly better in all cases! There may be some contextual reason why it makes sense to only have an interaction.\n\n-   Recall that the `anova()` function reports the **sequential sum-of-squares**. In this situation, we do not care about the p-values for `factor(cyl)` and `disp`, so we do not care which order they enter the model in. We only care about the p-value for inclusion of the interaction term `factor(cyl):disp`.\n\n    -   R's formula notation is clunky, but leads to a lot of great situations like this. By using `factor(cyl)*disp)`, it added the lower order terms first and then the interaction, thus making the sequential sum-of-squares useful!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}