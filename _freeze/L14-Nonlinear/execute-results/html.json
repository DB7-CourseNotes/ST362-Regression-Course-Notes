{
  "hash": "e52f3d842a1152b3f5571e0a023732a9",
  "result": {
    "markdown": "---\ntitle: \"Non-Linear Relationships with Linear Models\"\ninstitute: \"Jam: **Higher Order** by Empire Me\"\n---\n\n\n\n\n\n## Non-Linear Relationships\n\n### Arbitrarily Shaped Functions\n\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\\vspace{1cm}\n\nThe plot on the right is the function:\n$$\ny = 2 + \\frac{1}{5}x^2 - 8\\log(x) - 0.005x^3 + 20\\sin\\left(\\frac{x}{2}\\right) + \\epsilon\n$$\n\n:::\n::: {.column width=\"50%\"}\n\n\n::: {.cell hash='L14-Nonlinear_cache/html/unnamed-chunk-2_dd2aa054346f70eb1301f38c4c6b3f60'}\n::: {.cell-output-display}\n![](L14-Nonlinear_files/figure-html/unnamed-chunk-2-1.png){width=432}\n:::\n:::\n\n:::\n::::\n\n\\pause\\pspace\n\nThe twist: The fitted line is just a polynomial model: $y = \\beta_0 + \\sum_{j=1}^{12}\\beta_jx^j$\n\n### Fitting a Polynomial\n\nTo fit a polynomial of order $k$:\n$$\ny = \\beta_0 + \\sum_{j=1}^{k}\\beta_jx^j + \\epsilon\n$$\nwe can simply fit a linear model to **transformed** predictors, i.e.:\n$$\nx_1 = x;\\; x_2 = x^2;\\; x_3 = x^3;\\;...\n$$\nand we can just fit a linear model as usual!\n\n\\pspace\n\n... that seems too easy?\n\n### Choosing Polynomial Order\n\nThere are two common options:\n\n1. Domain knowledge\n    - Is there a theoretical reason to use a cubic?\\lspace\n2. Reduce prediction error\n    - Cross-validation or ANOVA, depending on problem.\n\n### Domain Knowledge: Stopping Distance\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\\vspace{1cm}\n\nThe stopping distance is theoretically proportional to the square of the speed.\n\n\\pspace\n\n- A line might fit\n    - Fits poorly at 0 (negative stopping distances for positive speed?)\n- A quadratic fits better?\n- A cubic does something funky at 0.\n\n:::\n::: {.column width=\"50%\"}\n\n\n::: {.cell hash='L14-Nonlinear_cache/html/unnamed-chunk-3_57bc3b3b475597aef15b1f13a2bb67e0'}\n::: {.cell-output-display}\n![](L14-Nonlinear_files/figure-html/unnamed-chunk-3-1.png){width=432}\n:::\n:::\n\n:::\n::::\n\n### Choosing Order with ANOVA\n\n\\footnotesize\n\n::: {.cell hash='L14-Nonlinear_cache/html/unnamed-chunk-4_6914e9a81456c418d96b4cff4be73afb'}\n\n```{.r .cell-code}\nX <- data.frame(dist = cars$dist, x1 = cars$speed, x2 = cars$speed^2, \n    x3 = cars$speed^3, x4 = cars$speed^4)\nlm(dist ~ ., data = X) |> anova()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: dist\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \nx1         1 21185.5 21185.5 92.5775 1.716e-12 ***\nx2         1   528.8   528.8  2.3108    0.1355    \nx3         1   190.4   190.4  0.8318    0.3666    \nx4         1   336.5   336.5  1.4707    0.2316    \nResiduals 45 10297.8   228.8                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\\normalsize\nIn this situation, Sequential Sum-of-Squares makes sense! (Disagrees with theory, though. Go with theory.)\n\n### Stopping Distance $\\propto$ Speed$^2$\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\\vspace{1cm}\n\nA second order polynomial is:\n$$\ny = \\beta_0 + \\beta_1x + \\beta_2x^2\n$$\n\n\\pspace\n\nThe implied model is:\n$$\ny = \\beta_2x^2\n$$\n\n:::\n::: {.column width=\"50%\"}\n\n\n::: {.cell hash='L14-Nonlinear_cache/html/unnamed-chunk-5_ea44a9fdff76ea6b3bc06fc364761b90'}\n::: {.cell-output-display}\n![](L14-Nonlinear_files/figure-html/unnamed-chunk-5-1.png){width=432}\n:::\n:::\n\n:::\n::::\n\n### Unconventional ESS\n\n\\footnotesize\n\n::: {.cell hash='L14-Nonlinear_cache/html/unnamed-chunk-6_6414dd3d46c7e07c7ba523397f2c2e1f'}\n\n```{.r .cell-code}\nX <- data.frame(dist = cars$dist, x1 = cars$speed, x2 = cars$speed^2, \n    x3 = cars$speed^3, x4 = cars$speed^4)\nm1 <- lm(dist ~ x1 + x2, data = X)\nm2 <- lm(dist ~ -1 + x2, data = X)\nanova(m2, m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: dist ~ -1 + x2\nModel 2: dist ~ x1 + x2\n  Res.Df   RSS Df Sum of Sq      F Pr(>F)\n1     49 11936                           \n2     47 10825  2    1111.2 2.4123 0.1006\n```\n:::\n:::\n\n\n\\normalsize Not a significant difference in models, so go with simpler one: $y = \\beta_2x^2$\n\n- This is *highly* specific to this situation - see cautions later.\n\n\n### Polynomial Models *will* Overfit!\n\nTrue model: $f(x) = 2 + 25x + 5x^2 - x^3$ (cubic), Var($\\epsilon$) = 40\n\n\n::: {.cell hash='L14-Nonlinear_cache/html/unnamed-chunk-7_2b1f8cf1f2b197a54787e52013960c70'}\n::: {.cell-output-display}\n![](L14-Nonlinear_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n### Multiple Regression Polynomial Models\n\nA full polynomial model of order 2 with two predictors is:\n$$\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2 + \\beta_{12}x_1x_2\n$$\nIn R this can be specified as:\n\n\n::: {.cell hash='L14-Nonlinear_cache/html/unnamed-chunk-8_104876e959605f60eac13aa377f09ac9'}\n\n```{.r .cell-code}\nlm(y ~ (x1 + x2)^2)\n```\n:::\n\n\n- This is why you can't use `y ~ x + x^2` to get a polynomial model - R tries to interpret this as a model specification rather than a transformation. \\lspace\n- We'll learn more about interactions and transformations in the next few lectures.\n\n\n## Cautions about Polynomials\n\n### Lower Order Terms\n\nUnless there's a strong physical reason,\n\n\\begin{center}\n\\emph{always include lower order terms!}\n\\end{center}\n\n\\pspace\n\n- $(ax - b)^2$ is the model, not $\\beta_0 + \\beta_1x + \\beta_{11}x^2$\n\n### Orders higher than 3 are rarely jutified\n\nRecall the interpretation of a slope: \n\n- A one unit increase in $x$ is associated with a $\\beta_1$ unit increase in $y$.\n    - This interpretation fails in quadratrics, and fails spectacularly in higher orders.\n\n\\pspace\n\nSee **splines** for more flexibility\n\n### Extrapolation is Fraught with Peril\n\nUnless you have the true order (you don't), polynomials diverge almost immediately.\n\n\n::: {.cell hash='L14-Nonlinear_cache/html/unnamed-chunk-9_8e8f75ae3caa2edfe7579ee288d67be3'}\n\n```{.r .cell-code}\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/polyFit\")\n```\n:::\n\n\n### $x$ and $x^2$ are correlated\n\nThis strongly affects parameter estimates.\n\n\\pspace\n\n... unless...\n\n###  `poly()` uses **orthogonal** polynomials\n\n\n::: {.cell hash='L14-Nonlinear_cache/html/unnamed-chunk-10_736ec2c5285c30eacbcfb5a1f5418743'}\n\n```{.r .cell-code}\nbetas <- replicate(1000, \n    coef(lm(y ~ poly(x, 2, raw = TRUE),\n         data = data.frame(x = runif(30, 0, 10), \n            y = (x - 2)^2 + rnorm(30, 0, 10)))))\nplot(t(betas))\n```\n:::\n\n\nAfter squaring, cubing, etc., each column of X is transformed to be orthogonal to the previous.\n\n\\pspace\n\n- Takes care of transformations for you when using `predict()`.\\lspace\n- The `coef()` function is useless.\\lspace\n- Mean-centering also helps!\n\n### Orthogonal Polynomials\n\n\n::: {.cell hash='L14-Nonlinear_cache/html/unnamed-chunk-11_eeebbda6bd0ea3968c79eb8d5db34235'}\n\n```{.r .cell-code}\nx <- sort(runif(60, 0, 10))\npar(mfrow = c(2,3))\nfor(i in 1:3) {\n    plot(x, poly(x, 3, raw = TRUE)[,i])\n}\nfor(i in 1:3) {\n    plot(x, poly(x, 3, raw = FALSE)[,i])\n}\n```\n:::\n\n\n## Should I Use a Polynomial?\n\n### Example: mtcars\n\n[code]\n\n### Summary\n\n\n\n\n## Participation Questions\n\n### Q1\n\nGiven a correctly structured model matrix ($X$), polynomial models can be fit with the same routines as linear models (without modification).\n\n1. True\n2. False\n\n### Q2\n\nAny smooth function can be approximated by a polynomial.\n\n1. True\n2. False\n\n### Q3\n\nOrthogonal polynomials are used because:\n\n1. They remove covariance between $x$, $x^2$, $x^3$, etc.\n2. They remove the covariance between $\\beta_1$, $\\beta_{11}$, $\\beta_{111}$, etc.\n3. They result in p-values that make sense.\n4. All of the above.\n\n### Q4\n\nWhich statement is *true*?\n\n1. We should start with a high order polynomial and use Sequential Sum-of-Squares to choose the order.\n2. We should try second order polynomials if we think there's a curve to our model, but should generally avoid polynomials unless there's a strong contextual reason.\n3. We saw last week that estimates are still unbiased in the presence of extraneous predictors, so it's fine to include a higher order polynomial in our model.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}