{
  "hash": "df45df2cd3c149d11cadbc3f3bc6348c",
  "result": {
    "markdown": "---\ntitle: \"Stats Review\"\nsubtitle: \"This is a test\"\ninstitute: \"Jam: **Gamma Ray** by Beck\"\ndate: \"last-modified\"\n---\n\n\n::: {.content-visible when-format=\"pdf\"}\n\n## Preamble\n\n### Basic Info\n\n\\centering\n\\includegraphics[width=0.75\\textwidth]{figs/phdSyllabus.png}\n\n\\footnotesize{Source: Ph.D. Comics by Jorge Cham}\n\\raggedright\n\n:::\n\n## Introduction\n\n### Today's Learning Outcomes\n\n- Important facts about distributions.\\lspace\n- CIs and t-tests.\\lspace\n\n## Distributions\n\n### Normal\n\n$$\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{(x-\\mu)^2}{-2\\sigma^2}\\right);\\;-\\infty<x<\\infty\n$$\n\nShiny: [https://shiny.abdn.ac.uk/Stats/apps/app_normal/#section-theory](https://shiny.abdn.ac.uk/Stats/apps/app_normal/#section-theory)\n\n\\pspace\n\n- Completely determined by $\\mu$ and $\\sigma$.\\lspace\n- If $X \\sim N(\\mu,\\sigma^2)$, then $Z=\\frac{X-\\mu}{\\sigma} \\sim N(0,1)$.\n    - Often called \"standardizing\"\\lspace\n- You won't *need* to memorize the pdf, but it's useful!\n\n:::notes\n\n::: {.cell hash='L01-Introduction_cache/html/unnamed-chunk-1_eabb97a8edba9334df1a56bd24ada461'}\n\n```{.r .cell-code}\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pnorm\")\n```\n:::\n\n\nThe normal distribution is the foundation for pretty much everything that we're going to do in this class. In general, things aren't normally distributed, but the assumption is very robust and works out in a lot of cases.\n\nIn this lecture we're going to build up an important result that we'll use frequently. In particular, we want some background into why the F and $t$ distributions show up so often!\n\n\n\n:::\n\n### But first, Gamma!\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\\vspace{1cm}\n\n$t$ is based on the Gamma ($\\Gamma$) function:\n\n$$\n\\Gamma(q) = \\int_0^\\infty e^{-t}t^{q-1}dt\n$$\n\n- Interesting property: $\\Gamma(k+1) = k!$ for integer $k$.\n    - In general, $\\Gamma(q) = (q-1)\\Gamma(q-1)$\\lspace\n- Also, $\\Gamma(1/2) = \\pi^{1/2}$\n    - $\\Gamma(3/2) = (3/2-1)\\gamma(1/2) = \\sqrt{\\pi}/2$\n\n\n:::\n::: {.column width=\"50%\"}\n\\includegraphics[width=\\textwidth]{figs/gamma.png}\n:::\n::::\n\n### The $t$ Distribution\n\n$$\nf(x; \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu + 1}{2}\\right)}{\\sqrt{\\nu\\pi}\\Gamma(\\nu/2)}\\left(1 + \\frac{x^2}{\\nu}\\right)^{-(\\nu + 1)/2}\n$$\n\n- The notation $f(x; \\nu)$ means \"function of $x$, given a value of $\\nu$.\"\n- Completely determined by $\\nu$\\lspace\n- As $\\nu\\rightarrow\\infty$, this becomes $N(0,1)$.\n    - $\\nu>30$ is pretty much normal already.\n    - For $\\nu<\\infty$, $t$ has wider tails than normal.\n\n### The $\\chi^2$ distribution - variances\n\nIf $Z_1, Z_2, ..., Z_k$ are iid $N(0,1)$, then $\\chi^2_k = \\sum_{i=1}^kZ_i^2$ has a chi-square distribution on $k$ degrees of freedom.\n\n$$\nf(x; k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2}\n$$\n\n- If $X_i\\sim N(\\mu_i\\sigma_i)$, then we can just standardize each first.\\lspace\n- As $k\\rightarrow\\infty$, $(\\chi^2_k-k)/\\sqrt{2k}\\stackrel{d}{\\rightarrow} N(0,1)$.\n    - That is, for large $k$ this can be approximated by a normal distribution.\\lspace\n- Very related to the *variance*\n    - $(n-1)s^2/\\sigma^2$ follows a $\\chi^2_n$ distribution.\n\n### The $F$ distribution - ratio of variances\n\nIf $S_1$ and $S_2$ are independent $\\chi^2$ distributions with degrees of freedom $\\nu_1$ and $\\nu_2$, then $\\frac{S_1/\\nu_1}{S_2/\\nu_2}$ follows an $F_{\\nu_1,\\nu_2}$ distribution.\n\n$$\nf(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}\n$$\n\n- If $s_1^2$ and $s_2^2$ are the sample-based estimates of the true values $\\sigma_1^2$ and $\\sigma_2^2$, then $\\frac{s_1^2/\\sigma_1^2}{s_2^2/\\sigma_2^2}$ follows an $F_{\\nu_1, \\nu_2}$ distribution.\n    - Under $H_0$, $\\sigma_1=\\sigma_2$, so we don't need the true values.\n    - Note: $s_1^2$ is calculated from $S_1^2/\\nu_1$.\n\n## Confidence Intervals\n\n### General Idea: Terminology\n\nConsider the **statistic** $t = \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)}$ (a statistic is any number that can be calculated from data alone).\n\n\\pspace\n\n- $\\theta$ is the \"\"\"**true**\"\"\" value of the parameter.\n    - Unknown, unknowable, not used in formula - hypothesize $\\theta = \\theta_0$ instead.\\lspace\n- $\\hat\\theta$ is our **estimator** of $\\theta$.\n    - **Estimator** is a function, like $\\hat\\mu = \\bar X =(1/n)\\sum_{i=1}^nX_i$.\n        - $X$ is a random variable.\n    - **Estimate** is a number, like the calculated mean of a sample.\\lspace\n- $se(\\hat\\theta)$ is the **standard error** of $\\hat\\theta$.\n    - If we had a different sample, the value of $\\hat\\theta$ would be different. It has **variance**.\n    - **Standard error**: the standard deviation of the **sampling distribution**.\n\n### General Idea: Distributional Assumptions\n\nConsider the quantity (*not* statistic) $t = \\frac{\\hat\\theta - \\theta}{se(\\hat\\theta)}$.\n\n\\pspace\n\nIf we assume $X\\sim N(\\mu, \\sigma^2)$ and $\\hat\\theta = \\bar X$ is an unbiased estimate of $\\theta$ on $\\nu=n-1$ degrees of freedom, then $t \\sim t_\\nu$\n\n\\pspace\n\nFrom this, we can find the lower and upper $\\alpha/2$\\% of the $t$$ curve.\n\n- $t_\\nu(\\alpha/2)$ is the lower $\\alpha/2$\\%.\n    - i.e., if $\\alpha = 0.11$, then it's $t(\\nu, 0.055)$, the lower 5.5\\% area.\n- $t_\\nu(1 - \\alpha/2)$ is the upper $\\alpha/2$\\%.\n- Since $t$ is symmetric, $t_\\nu(1 - \\alpha/2) = -t_\\nu(\\alpha/2)$.\n\n\n:::notes\n\n::: {.cell hash='L01-Introduction_cache/html/unnamed-chunk-2_efc7ac60f18597b56a6933922a196e8b'}\n\n```{.r .cell-code}\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pvalues\")\n```\n:::\n\n:::\n\n### General Idea: Distribution to Quantiles\n\nUnder the null hypothesis, $\\theta = \\theta_0$, so\n$$\nt = \\frac{\\hat\\theta - \\theta}{se(\\hat\\theta)} = \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)} \\sim t_\\nu\n$$\n\nWe know that $\\bar X$ is a random variable, and we want everything in between its 5.5\\% and 94.5\\% **quantiles**. \\vspace{-3mm}\n\n- This is a confidence interval!\n\n### General Idea: Distribution to CI\n\nWe can do this easily for the $t_\\nu$ distribution: we want all values $t_0$ such that\n\n\\begin{align*}\nt_\\nu(5.5) &\\le t_0 \\le t_\\nu(94.5)\\\\\nt_\\nu(5.5) &\\le \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)} \\le t_\\nu(94.5)\\\\\nse(\\hat\\theta)t_\\nu(5.5) &\\le \\hat\\theta - \\theta_0 \\le se(\\hat\\theta)t_\\nu(94.5)\\\\\n\\hat\\theta + se(\\hat\\theta)t_\\nu(5.5) &\\le \\theta_0 \\le \\hat\\theta + se(\\hat\\theta)t_\\nu(94.5)\n\\end{align*}\n\nThe CI is all values $\\theta_0$ that would *not* be rejected by the null hypothesis $\\theta = \\theta_0$ at the $\\alpha$\\% level.\n\nSince $t_\\nu(1 - \\alpha/2) = -t_\\nu(\\alpha/2)$, our 89\\% CI is $\\hat\\theta \\pm se(\\hat\\theta)t_\\nu(5.5)$.\n\n\n### What is the Standard Error?\n\nIf we're estimating the mean, $\\hat\\theta = (1/n)\\sum_{i=1}^nX_i$, where we assume $X_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma)$.\n$$\nE(\\hat\\theta) = E\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n}\\sum_{i=1}^nE(X_i) = \\frac{n\\mu}{n} = \\mu\n$$\nFrom this, we see that the mean is an unbiased estimator! (This is nice, but not required.)\n\n$$\nV(\\hat\\theta) = V\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n^2}V\\left(\\sum_{i=1}^nX_i\\right) \\stackrel{indep}{=} \\frac{1}{n^2}\\sum_{i=1}^nV(X_i) = \\sigma^2/n \\stackrel{plug-in}{=} s^2/n\n$$\nwhere $s^2$ is the estimate variance since we cannot know the true mean. Note that $s^2$ is a biased estimator for $\\sigma$.\n\n### CI for Variance\n\nFrom before: $\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_n$. \n\nLet $\\chi^2_n(0.055)$ be the lower 5.5\\% quantile, $\\chi^2_n(0.945)$ be the upper.\n\nFor homework, find the CI from:\n$$\n\\chi^2_n(0.055) \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi^2_n(0.945)\n$$\nNote that $\\chi^2_n(0.055)\\ne-\\chi^2_n(0.945)$.\n\n\n### Summary\n\n- Distributions exist and are important\n    - Most things will be normal, which leads to $t$, $\\chi^2$, and $F$.\\lspace\n- CIs are all values that would not be rejected by a hypothesis test.\n    - The null hypothesis determines the distribution of the test statistic, which allows us to find the CI.\n\n\n::: {.content-visible when-format=\"pdf\"}\n\n## Participation Questions\n\n### Q1\n\nWhich of the following is a Normal distribution?\n\n\\pspace\n\n1. $f(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}$\n1. $f(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-x^2/2\\right)$\n2. $f(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}$\n3. $f(x; k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2}$\n\n### Q2\n\nIf you know $\\mu$ and $\\sigma$, then you know the exact shape of the normal distribution.\n\n\\pspace\n\n1. True\n2. False\n\n### Q3\n\nA confidence interval for $\\theta$ contains all values $\\theta_0$ that would not be rejected by a hypothesis test (assume that both are at the same significance level).\n\n1. True\n2. False\n\n### Q4\n\nWhich of the following is the correct value for $E(\\hat\\theta)$, where $\\hat\\theta = \\sum_{i=1}^n\\left(a + bX_i\\right)$ and $E(X_i)=\\mu$ for all $i$?\n\n\\pspace\n\n1. \n2. $a + b\\mu$\n3. $b\\mu$\n4. $na + nb\\mu$\n\n### Q5\n\nWhich of the following is the definition of an estimator?\n\n1. A value calculated from data.\n2. A function that returns the estimate for a parameter.\n3. Any function of the data.\n4. A person who estimates.\n\n### Q6\n\nThe general approach to finding confidence intervals is to find a function of the statistic and the parameter it's estimating that follows a known distribution and then solve for the unknown parameter.\n\n\\pspace\n\n1.\n2. False\n3. True\n4. \n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}