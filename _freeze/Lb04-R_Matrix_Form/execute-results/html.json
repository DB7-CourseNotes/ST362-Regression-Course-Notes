{
  "hash": "01285b1e2ea1c30a8a0352b4dbc9a9bf",
  "result": {
    "markdown": "---\ntitle: \"Verifying Matrix Identities\"\noutput: html_document\n---\n\n\n## Verifying Matrix Results\n\nWe'll use the `mtcars` data for this. Here's what it looks like:\n\n\n::: {.cell hash='Lb04-R_Matrix_Form_cache/html/unnamed-chunk-1_67bd811f8bcef493b663e0fce68ad2ef'}\n\n```{.r .cell-code}\nx <- mtcars$disp\ny <- mtcars$mpg\n\nplot(y ~ x)\nabline(lm(y ~ x))\n```\n\n::: {.cell-output-display}\n![](Lb04-R_Matrix_Form_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nIt looks like the slope is negative, and the intercept will be somewhere between 25 and 35.\n\nLet's use the formulae from the previous course: $\\hat\\beta_1 = S_{XY}/S_{XX}$ and $\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x$.\n\n\n::: {.cell hash='Lb04-R_Matrix_Form_cache/html/unnamed-chunk-2_6bfca20fa0111cd2bd5f207a6e82897c'}\n\n```{.r .cell-code}\nb1 <- sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)\nb0 <- mean(y) - mean(x) * b1\n\nmatrix(c(b0, b1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]\n[1,] 29.59985476\n[2,] -0.04121512\n```\n:::\n:::\n\n\nTo make the matrix multiplication to work, we need $X$ to be a column of 1s and a column representing our covariate.\n\n\n::: {.cell hash='Lb04-R_Matrix_Form_cache/html/unnamed-chunk-3_ddaea4b2347cdb3c32230b940ecd3720'}\n\n```{.r .cell-code}\nX <- cbind(1, x)\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         x\n[1,] 1 160\n[2,] 1 160\n[3,] 1 108\n[4,] 1 258\n[5,] 1 360\n[6,] 1 225\n```\n:::\n:::\n\n\nThe estimates should be $(X^TX)^{-1}X^T\\underline y$. In R, we find the transpose with the `t()` function and we find inverse with the `solve()` function.\n\n\n::: {.cell hash='Lb04-R_Matrix_Form_cache/html/unnamed-chunk-4_adef1f8c28b822063034c080b8975bfb'}\n\n```{.r .cell-code}\nbeta_hat <- solve(t(X) %*% X) %*% t(X) %*% y\nbeta_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         [,1]\n  29.59985476\nx -0.04121512\n```\n:::\n:::\n\n\nIt works!\n\nNow let's check the ANOVA table!\n\n\n::: {.cell hash='Lb04-R_Matrix_Form_cache/html/unnamed-chunk-5_3fb2b7c28e87d8ae975e43e21d599ff8'}\n\n```{.r .cell-code}\nn <- length(x)\ndata.frame(source = c(\"Regression\", \"Error\", \"Total\"),\n    df = c(2, n-2, n),\n    SS = c(t(beta_hat) %*% t(X) %*% y, \n        t(y) %*% y - t(beta_hat) %*% t(X) %*% y,\n        t(y) %*% y)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      source df         SS\n1 Regression  2 13725.1513\n2      Error 30   317.1587\n3      Total 32 14042.3100\n```\n:::\n:::\n\n::: {.cell hash='Lb04-R_Matrix_Form_cache/html/unnamed-chunk-6_892895d62ed660fbed997baddd85a8d6'}\n\n```{.r .cell-code}\nanova(lm(y ~ x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value   Pr(>F)    \nx          1 808.89  808.89  76.513 9.38e-10 ***\nResiduals 30 317.16   10.57                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\ncolSums(anova(lm(y ~ x)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Df    Sum Sq   Mean Sq   F value    Pr(>F) \n  31.0000 1126.0472  819.4605        NA        NA \n```\n:::\n:::\n\n\nThey're slightly different? Why?\n\nBecause the equation in the textbook is for the *uncorrected* sum of squares, which basically means we're looking at estimating both $\\beta_0$ and $\\beta_1$ at the same time (hence the df of $n-2$). The usual ANOVA table is the corrected sum of squares, which the textbook labels $SS(\\hat\\beta_1|\\hat\\beta_1)$ to make it clear that it's estimating $\\beta_1$ only; $\\beta_0$ has already been estimated.\n\n\n::: {.cell hash='Lb04-R_Matrix_Form_cache/html/unnamed-chunk-7_6731dbfe79c5ee4aef3a9b17faf05132'}\n\n```{.r .cell-code}\nn <- length(x)\ndata.frame(source = c(\"Regression\", \"Error\", \"Total\"),\n    df = c(1, n-1, n),\n    SS = c(t(beta_hat) %*% t(X) %*% y - n * mean(y)^2, \n        t(y) %*% y - t(beta_hat) %*% t(X) %*% y,\n        t(y) %*% y - n * mean(y)^2)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      source df        SS\n1 Regression  1  808.8885\n2      Error 31  317.1587\n3      Total 32 1126.0472\n```\n:::\n:::\n\n\nThe matrix form for $R^2$ is a little different from what you might expect. It uses this idea of \"corrected\" sum-of-squares as well. For homework, verify that the corrected sum-of-squares works out to the same formula.\n\nHere's how to extract the $R^2$ value from R (note that the programming language R has nothing to do with the $R^2$; R is named after S, which was the programming language that came before it (both chronologically and alphabetically); you'll still find references to S and S-Plus).\n\n\n::: {.cell hash='Lb04-R_Matrix_Form_cache/html/unnamed-chunk-8_fd942807d910048e31474a3357d35072'}\n\n```{.r .cell-code}\nsummary(lm(y ~ x))$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7183433\n```\n:::\n:::\n\n\nIn the textbook, the formula is given as: $$\nR^2 = \\frac{\\hat{\\underline\\beta}^TX^T\\underline y - n\\bar y^2}{\\underline y^t\\underline y - n\\bar y^2}\n$$\n\n\n::: {.cell hash='Lb04-R_Matrix_Form_cache/html/unnamed-chunk-9_d609ef7ec72333555c169bf012f63fe4'}\n\n```{.r .cell-code}\nnumerator <- t(beta_hat) %*% t(X) %*% y - n * mean(y)^2\ndenominator <- t(y) %*% y - n * mean(y)^2\nnumerator / denominator\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]\n[1,] 0.7183433\n```\n:::\n:::\n\n\n## \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}