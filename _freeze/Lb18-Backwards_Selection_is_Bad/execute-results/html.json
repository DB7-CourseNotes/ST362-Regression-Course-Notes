{
  "hash": "5c363d80ff5a814e9b5f3069c9f44601",
  "result": {
    "markdown": "---\ntitle: \"Backwards p-values\"\nauthor: \"Devan Becker\"\ndate: \"09/05/2020\"\n---\n\n\n\n\n# Algorithmic model selection\n\n### Description\n\nBackwards and forward model selection are sequential model selection techniques where predictors are sequentially removed or added until all predictors are significant and no other predictors would be significant if they were added. \n\n### Distribution of a p-value\n\nUnder the null hypothesis, a p-value is **uniform**. That is, all p-values are equally likely. This is not a coincidence, this is how p-values are defined. The p-value is the area of the sampling distribution further away from the hypothesized mean than the observed data. The area \"further away\" means we are looking at the CDF of a distribution, and the CDF of any continuous distribution follows a normal distribution.\n\nTo demonstrate, let's simulate!\n\n\n::: {.cell hash='Lb18-Backwards_Selection_is_Bad_cache/html/sim1_ab94f7228b68383a88e6cfe5c6aca95e'}\n\n```{.r .cell-code}\nlibrary(magrittr) # pipes and `extract` function\nlibrary(dplyr)\nlibrary(ggplot2)\nx <- runif(30, 0, 10)\ny <- rnorm(30, 0, 1) # uncorrelated with x - null is true.\n\nsummary(lm(y ~ x))$coef[2, 4] # the p-value for x\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.93823\n```\n:::\n\n```{.r .cell-code}\nN <- 10000 # Increase for better results\np_vals <- double(N)\nfor (i in 1:N){\n    y <- rnorm(30, 0, 1)\n    p_vals[i] <- summary(lm(y ~ x))$coef[2, 4]\n}\n```\n:::\n\n::: {.cell hash='Lb18-Backwards_Selection_is_Bad_cache/html/unnamed-chunk-1_de08c5eb75a4b146e79ce91c53c8e2fc'}\n\n```{.r .cell-code}\nhist(p_vals, freq = FALSE)\nabline(h = 1) # theoretical pdf\n```\n\n::: {.cell-output-display}\n![](Lb18-Backwards_Selection_is_Bad_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n# Backwards Selection\n\nWe've established that P(p.value < 0.05) = 0.05, i.e. that p-values follow a uniform distribution. What about the probability of *at least one significant value* out of 2?\n\n\\begin{align*}\nF(z) &= P(\\text{at least one of }X_1\\text{ and }X_2\\text{ is less than }z)\\\\\n&= 1 - P(\\text{both }X_1\\text{ and }X_2\\text{ are greater than }z)\\\\\n&= 1 - P(X_1 > z, X_2 > z)\\\\\n&= 1 - P(X_1 > z)(X_2 > z)\\\\\n&= 1 - (1-z)^2\n\\end{align*}\n\nSo the pdf is the derivative of this, which is $f(z) = 2 - 2z$.\n\n\n::: {.cell hash='Lb18-Backwards_Selection_is_Bad_cache/html/sim2_44b85bcb56db8fa6321f2d3e849ef69a'}\n\n```{.r .cell-code}\nN <- 10000\nmin_p_vals <- double(N)\nfor (i in 1:N){\n    yxx <- data.frame(x1 = runif(30), x2 = runif(30), y = rnorm(30))\n    p_vals <- summary(lm(y ~ ., data = yxx))$coef[-1, 4]\n    max_p <- which.max(p_vals) # to be removed\n    min_p_vals[i] <- summary(lm(y ~ ., data = yxx[, -max_p]))$coef[2, 4]\n}\n```\n:::\n\n::: {.cell hash='Lb18-Backwards_Selection_is_Bad_cache/html/unnamed-chunk-2_6e8ba41f8af9dc43c92a06589b1470ff'}\n\n```{.r .cell-code}\nhist(min_p_vals, freq = FALSE)\nabline(a = 2, b = -2) # Theoretical pdf\n```\n\n::: {.cell-output-display}\n![](Lb18-Backwards_Selection_is_Bad_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nSo when we *remove* the largest p-value, we end up with a much larger chance that the remaining p-value is below 0.05 - *even though the null hypothesis is true.* This bears repeating - when we make a change in the model based on the p-value, the rest of the p-values must be interpreted **carefully**. They are *no longer* the probability of data at least as extreme under the null hypothesis - they have been artificially shrunk. \n\n### What about AIC?\n\nIn R, `step()` performs stepwise model selection (by default, backwards) until the AIC no longer decreases. I'm going to simulate under the null hypothesis: none of the predictors are actually related to the response.\n\n\n::: {.cell hash='Lb18-Backwards_Selection_is_Bad_cache/html/sim3_996319aa8802f33bfae1c0cd83ef1b78'}\n\n```{.r .cell-code}\nN <- 1000 # This will take a while\nmin_p <- double(N)\nmax_p <- double(N)\np1 <- double(N)\nnum_signif <- double(N)\n\nt0 <- Sys.time()\nfor (i in 1:N){\n    # First columns is y, the rest are x1...x20\n    bigdf <- matrix(data = rnorm(30 * 11), nrow = 30, ncol = 11)\n    colnames(bigdf) <- c(\"y\", paste(\"x\", 1:10))\n    bigdf <- as.data.frame(bigdf)\n\n    # Regress y against everything\n    newmod <- step(lm(y ~ ., data = bigdf), trace = 0)\n\n    # Extract the p-values\n    mycoefs <- summary(newmod)$coef[-1, 4]\n    num_signif[i] <- length(mycoefs)\n    min_p[i] <- min(mycoefs)\n    max_p[i] <- max(mycoefs)\n    p1[i] <- mycoefs[1] # should be representative of p-values\n}\nSys.time() - t0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 19.71106 secs\n```\n:::\n:::\n\n::: {.cell hash='Lb18-Backwards_Selection_is_Bad_cache/html/unnamed-chunk-3_22eff7661ce48bf33ca4c82151dced8e'}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nhist(min_p, breaks = seq(0, 0.25, 0.01))\nabline(v = 0.05, col = \"red\")\nhist(max_p, breaks = seq(0, 0.55, 0.01))\nabline(v = 0.05, col = \"red\")\nbarplot(table(num_signif), main = \"Number of remaining predictors\")\nhist(p1, breaks = seq(0, 0.45, 0.01))\nabline(v = 0.05, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](Lb18-Backwards_Selection_is_Bad_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nIn the plots above, keep in mind that **none** of the predictors were related to the response. We see that the p-values are far from being uniform, and sometimes we took *all* of the predictors. \n\nThe bottom right plot shows the distribution of the p-value for the first predictor. There's nothing special about this predictor, so this plot is representative of the distribution of p-values resulting from stepwise AIC model selection with 20 predictors, none of which are actually related to the response. \n\n# So what's the right way?\n\nBest: Domain knowledge. Start with the model that you think will be the final model, then experiment with adding/removing predictors. For instance, if your model has weather predictors, start with the ones that are most meaningful (e.g. daily high temperature), then look at what changes when you switch it out for something else (e.g., daily average or low temperature). Check the residual diagnostic plots each time - ignore the p-values until the very end!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}