{
  "hash": "48fded0419df18d83c417e3ee9bec8c7",
  "result": {
    "markdown": "---\ntitle: \"Assessing Fit\"\ninstitute: \"Jam: **Left Overs** by Joe Bonamassa\"\n---\n\n\n\n## The residual: what's left over\n\n### $R^2$: percent of variance explained by the regression model\n\n:::: {.columns}\n::: {.column width=\"30%\"}\n\\vspace{2cm}\n\n$$\nR^2 = \\frac{SSReg}{SST} = \\frac{S_{XY}^2}{S_{XX}S_{YY}}\n$$\n\n:::\n::: {.column width=\"70%\"}\n\n\n::: {.cell hash='L03-Residuals_cache/html/unnamed-chunk-1_646003392f6dc41e3bd7f81bf22b8ff4'}\n\n```{.r .cell-code}\nlayout(mat = matrix(c(1,2,3), nrow = 1), widths = c(0.5,1,1))\nset.seed(18)\nx <- runif(25, 0, 10)\ny <- rnorm(25, 2 + 5*x, 6)\n\nplot(rep(1, 25), y, xlab = \"y\", ylab = \"y has variance\", xaxt = \"n\")\nabline(h = mean(y))\naxis(2, mean(y), bquote(bar(y)), las = 1)\n\nplot(x, y, ylab = \"There's variance around the line\")\nabline(lm(y~x))\nabline(h = mean(y))\naxis(2, mean(y), bquote(bar(y)), las = 1)\n\nmids <- predict(lm(y~x))\nfor(i in seq_along(mids)){\n    lines(x = rep(x[i], 2), y = c(y[i], mids[i]), col = 1)\n}\n\nmids <- predict(lm(y~x))\nplot(mids ~ x, type = \"n\", ylab = \"The line varies around the mean of y!\")\nfor(i in seq_along(mids)){\n    lines(x = rep(x[i], 2), y = c(mean(y), mids[i]))\n}\naxis(2, at = mean(y), labels = bquote(bar(y)), las = 1)\nabline(h = mean(y))\n```\n\n::: {.cell-output-display}\n![](L03-Residuals_files/figure-html/unnamed-chunk-1-1.png){width=576}\n:::\n:::\n\n:::\n::::\n\n### Residual Assumptions\n\n- **Residual**: what's left over\n    - $\\hat\\epsilon_i = y_i - \\hat y_i$\\lspace\n- Assumptions (from before):\n    - $E(\\epsilon_i) = 0$\n    - $V(\\epsilon_i) = \\sigma^2$\n    - $\\epsilon_i \\sim N(0,\\sigma^2)$\n\n\\pspace\n\nWe must check our assumptions!\n\n- There are statistical tests, but they'll never tell you as much as a plot!\n\n### Residuals versus *fitted* values: $\\hat{\\underline\\epsilon}$ versus $\\hat{\\underline{y}}$\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\\vspace{1cm}\n\nWhy $\\hat{\\underline{y}}$ instead of $\\underline y$?\n\n- See text. Try a regression of $\\hat{\\underline\\epsilon}$ versus $\\underline{y}$ yourself (mathematically and with code).\n\n\\pspace \n\nWhy not $\\underline x$?\n\n- For simple linear regression, $\\hat{\\underline{y}}$ is like a unit change for $\\underline x$, so it doesn't matter. \n    - For multiple linear regression, it's easier to have one variable for the $x$ axis.\n\n:::\n::: {.column width=\"50%\"}\n\n\n::: {.cell hash='L03-Residuals_cache/html/unnamed-chunk-2_a41902005cefa8807424bcb0d89e3499'}\n\n```{.r .cell-code}\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(patchwork)\nlibrary(broom)\nlibrary(palmerpenguins)\n\npenguins <- penguins[complete.cases(penguins),]\n\ng1 <- ggplot(penguins) + \n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\",\n        title = \"y versus x\")\n\nplm <- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\np2 <- augment(plm)\n\ng2 <- ggplot(p2) + \n    aes(x = .fitted, y = .resid) + \n    geom_point() +\n    geom_hline(yintercept = 0, col = \"grey\") +\n    labs(x = \"Fitted\", y = \"Residuals\", title = \"Residuals versus Fitted\")\n\n\ng1 / g2\n```\n\n::: {.cell-output-display}\n![](L03-Residuals_files/figure-html/unnamed-chunk-2-1.png){width=480}\n:::\n:::\n\n:::\n::::\n\n### Residual Plots and Assumption Checking\n\nMathematics is the process of making assumptions and seeing if we can break them.\n\n- $E(\\epsilon_i) = 0$ is a given since $\\sum_{i=1}^n\\hat\\epsilon_i=0$.\\lspace\n- $V(\\epsilon_i) = \\sigma^2$\n    - Check if the variance looks stable.\\lspace\n- $\\epsilon_i \\sim N(0,\\sigma^2)$ is harder to see\n    - Expect more points close to 0, fewer further away, no outliers\n\n### Residual plots: unstable error\n\n\n::: {.cell hash='L03-Residuals_cache/html/unnamed-chunk-3_230deb700bdabc5e088228c60b17dd67'}\n\n```{.r .cell-code}\nx <- runif(200, 0, 10)\ny0 <- 2 - 3*x\ny <- y0 + rnorm(length(x), 0, 2 * x)\n\ng1 <- ggplot() + \n    aes(x = x, y = y) + \n    geom_point() + \n    geom_smooth(se = FALSE, method = \"lm\", formula = y~x) +\n    labs(x = NULL, y = NULL, title = \"y versus x\")\n\nxydf <- augment(lm(y ~ x))\n\ng2 <- ggplot(xydf) +\n    aes(x = .fitted, y = .resid) + \n    geom_point() +\n    geom_hline(yintercept = 0, col = \"grey\") +\n    labs(x = \"Fitted\", y = \"Residuals\", title = \"Residuals versus Fitted\")\n\ng1 + g2\n```\n\n::: {.cell-output-display}\n![](L03-Residuals_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n### Residual plots: non-linear trend?\n\n\n::: {.cell hash='L03-Residuals_cache/html/unnamed-chunk-4_552986226fc9602649cd4c0929cab163'}\n\n```{.r .cell-code}\n## fig-height: 4\n## fig-width: 8\ng1 <- ggplot(mtcars) + \n    aes(x = disp, y = mpg) + \n    geom_point() + \n    geom_smooth(se = FALSE, method = \"lm\", formula = y~x) +\n    labs(x = \"Engine Displacement\", y = \"Miles per Gallon\", title = \"y versus x\")\n\nmtdf <- augment(lm(mpg ~ disp, data = mtcars))\n\ng2 <- ggplot(mtdf) +\n    aes(x = .fitted, y = .resid) + \n    geom_point() +\n    geom_smooth(se = FALSE) +\n    geom_hline(yintercept = 0, col = \"grey\") +\n    labs(x = \"Fitted\", y = \"Residuals\", title = \"Residuals versus Fitted - non-linear trend?\")\n\ng1 + g2\n```\n\n::: {.cell-output-display}\n![](L03-Residuals_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n### Testing Normality: Quantile-Quantile Plots\n\n:::: {.columns}\n::: {.column width=\"58%\"}\n\\vspace{0.15cm}\n\nConsider the data (2,3,3,4,5,5,6).\\vspace{0.15cm}\n\n- 50\\% of the data is below the median. \n    - For a $N(0,1)$ distribution, 50% of the data is below 0.\n    - Put the median on the y axis, 0 on the x axis.\n- 25\\% of the data is below Q1.\n    - For a $N(0,1)$ distribution, 25% is below `qnorm(0.25)` = -0.67\n    - Put a point at x = -0.67, y = Q1.\n- 75\\% of the data is below Q3.\n    - For a $N(0,1)$ distribution, 75% is below `qnorm(0.75)` = 0.67\n    - Put a point at x = 0.67, y = Q3.\n- ... and so on for the rest of the quantiles\n\n\n\n:::\n::: {.column width=\"40%\"}\n\nIf perfectly normal, expect a straight line!\n\n\n::: {.cell hash='L03-Residuals_cache/html/unnamed-chunk-5_980cf636f4a656b5f1a2ce617081ccd4'}\n\n```{.r .cell-code}\nmydata <- c(2, 3, 3, 4, 5, 5, 6)\nquants <- qnorm(c(\n    0.125, 0.25, 0.375, 0.5, \n    0.625, 0.75, 0.875))\nplot(mydata ~ quants)\n```\n\n::: {.cell-output-display}\n![](L03-Residuals_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n:::\n::::\n\n### Other Residual Plots: \n\n**Scale-Location**\n\n- Scale: Standardized residual\n- Location: Fitted value\n- More on standardized residuals in Ch08\n\n\\pspace\n\n**Cook's Distance**\n\n- Basically, an outlier detection method. \n- More in Ch08\n\n\\pspace\n\n**Leverage**\n\n- More in Ch08\n\n## Participation Quiz\n\n### Q01\n\nWhich of the following is not an assumption of linear models?\n\n\\pspace\n\n1. $$\n2. The variance is constant across all values of $X$.\n3. The height of the line is the mean value of $Y$ for a given $X$.\n4. None of the above.\n\n### Q02\n\nWhich of the following is a confidence interval for $s$?\n\n\\pspace\n\n1. $\\chi^2_n(0.055) \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi^2_n(0.945)$\n2. $\\chi^2_n(0.945) \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi^2_n(0.055)$\n3. $s^2 \\pm \\text{Critical Value } * \\text{ se}(s^2)$\n4. $s \\pm \\text{Critical Value } * \\text{ se}(s)$\n5. None of the above\n\n### Q03\n\nWhich of the following is *not* a random variable?\n\n\\pspace\n\n1. $Y$\n2. $\\hat\\epsilon_i$\n3. $\\hat Y$\n4. $\\underline\\epsilon$\n\n### Q04\n\n$F = t^2$\n\n1. True\n2. False\n3. Sometimes true\n\n### Q05\n\nWhich of the following is the definition of a residual?\n\n1. $y_i - \\hat y_i$\n2. $(y_i - \\hat y_i)^2$\n3. $\\hat y_i - y_i$\n4. $(\\hat y_i - y_i)^2$\n\n### Q06\n\nWhich of the following statements about $R^2$ is *false*?\n\n1. $R^2 = SSReg / SST$\n2. $R^2 = r^2$, where $r$ is the correlation between $\\underline x$ and $\\underline y$\n3. $R^2$ compares the variance of the line to the variance of $y$ alone\n4. $R^2$ is not a random variable\n\n## Maximum Likelihood\n\n### Main Idea\n\nFind the values $\\hat\\beta_0$, $\\hat\\beta_1$, and $\\hat\\sigma^2$ that **maximize the likelihood of seeing our data**.\n\n\\pspace\n\nUnder the assumptions that $X$ is fixed, $Y = \\beta_0 + \\beta_1X + \\epsilon_i$, and $\\epsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2)$,\n$$\nY \\sim N(\\beta_0 + \\beta_1X, \\sigma^2)\n$$ \n\n### The Likelihood\n\nThe **probability** of observering a data point is:\n$$\nf_Y(y_i|x_i, \\beta_0, \\beta_1, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1x_i)^2\\right)\n$$\n\n\\pspace\n\nThe **likelihood** of the parameters, given the data, is:\n$$\nL(\\beta_0, \\beta_1, \\sigma^2|x_i, y_i) = \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1x_i)^2\\right)\n$$\n\n- It's just a shift in perspective!\n\n### Simple Coin Flip Example\n\nSuppose we flipped 10 bottle caps and got 6 \"crowns\". Assume the probability of \"crown\" ($C$) is unknown, labelled $p$.\n\n\\pspace\n\n- The **probability** of one cap flip is $P(C|p) = p$.\\lspace\n- The **probability** of this is $P(C = 6|p) = p^6(1-p)^4$. \n    - This is just $P(\\underline y|p) = \\prod_{i=1}^nP(Y = y_i)$.\\lspace\n- The **likelihood** is $L(p|\\underline y) = \\prod_{i=1}^nP(Y = y_i)$.\n\n### Maximizing the Likelihood in LM\n\n$$\nL(\\beta_0, \\beta_, \\sigma^2) = \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1x_i)^2\\right)\n$$\n\n- To maximize w.r.t $\\beta_0$, we set the derivative w.r.t $\\beta_0$ to 0 and solve for $\\beta_0$.\n    - $\\frac{\\partial L(\\beta_0, \\beta_, \\sigma^2)}{\\partial \\beta_0} = 0$.\n- Repeat for $\\frac{\\partial L(\\beta_0, \\beta_, \\sigma^2)}{\\partial \\beta_1} = 0$ and $\\frac{\\partial L(\\beta_0, \\beta_, \\sigma^2)}{\\partial \\sigma^2} = 0$\n\n\\pspace\n\nHWK: Show that the estimates for $\\hat\\beta_0$ and $\\hat\\beta_1$ are the same as the OLS estimates. The estimate for $\\hat\\sigma^2$ should come out to:\n$$\n\\hat\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n\\left(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i\\right)^2\n$$\n\n### Bias-Variance Decomposition\n\nOn the next slide, we'll show that $E\\left((Y_i - \\hat Y_i)^2\\right)$ can be decomposed into $V(\\epsilon_i)$, squared bias, and the variance of the fitted model.\n\n\\pspace\n\n- $V(\\epsilon_i)$ is the variance of the *true* errors.\\lspace\n- Bias is the difference between the true model and the estimated one.\n    - **Systematic** difference, not just random errors\n        - e.g. fitting a linear model to a non-linear trend\\lspace\n- Variance of the fitted model *across all possible samples*\n\n\\pspace\n\nNote that this is a slight deviation from how the text presents it.\n\n### Derivation\n\nSuppose the true value is $Y_i = g(x_i) + \\epsilon_i$ (not necessarily linear), where $E(\\epsilon_i) = 0$, $E(Y_i) = g(x_i)$, and $V(\\epsilon_i) = \\sigma^2$.\n\\begin{align*}\nE\\left((Y_i - \\hat Y_i)^2\\right) &= E(Y_i - 2Y_i\\hat Y_i + \\hat Y_i^2)\\\\\n&= E(Y_i^2) - 2E(Y_i\\hat Y_i) + E(\\hat Y_i^2)\\\\\n&= E((g(x_i) + \\epsilon)^2) - 2E(g(x_i) + \\epsilon_i)E(\\hat Y_i) + E(\\hat Y_i^2)\\\\\n&= g^2(x_i) + 2g(x_i)E(\\epsilon_i) + E(\\epsilon_i^2) - 2g(x_i)E(\\hat Y_i) + E(\\hat Y_i^2)\\\\\n&= g^2(x_i) + \\sigma^2 - 2g(x_i)E(\\hat Y_i) + E(\\hat Y_i^2) + E(\\hat Y_i)^2 - E(\\hat Y_i)^2\\\\\n&= \\sigma^2 + (g(x_i) - E(\\hat Y_i))^2 + V(\\hat Y_i)\\\\\n&= \\text{Error Variance} + \\text{Bias}^2 + \\text{Fitted Model Variance}\n\\end{align*}\n\n### Not Interpreting the MSE\n\n$$\nE((Y_i - \\hat{Y_i})^2) = \\sigma^2 + (g(x_i) - E(\\hat Y_i))^2 + V(\\hat Y_i)\n$$\n\n\\pspace\n\n- We don't know any of the numbers on the right!!!\n\n\\pspace\n\nThe decomposition is theoretical, we can't tease apart $s^2$ into these terms.\n\n### Interpreting the MSE\n\nThe basic question of statistics: \"How big is this number?\"\n\n\\pspace\n\n- Compare to previous studies - is MSE larger than $\\sigma^2$?\n    - Implies that Bias$^2$ and Fitted Model Variance are larger than expected.\n    - F-test\\lspace\n- Compare to \"pure error\" - direct estimate of $\\sigma^2$.\n    - i.e. the variance in repeated trials on the same covariate values\n    - Textbooks devotes a lot to this, but it's often not plausible.\n        - Won't be on tests!\\lspace\n- Compare to another model \n    - We'll focus on this (later)!\n\n\n### Compare to Previous Studies\n\nHypothesis test for $\\sigma^2 = \\sigma_0^2$ versus $\\sigma^2 > \\sigma_0^2$, where $\\sigma_0$ is the value from a previous study.\n\n\\pspace\n\n- If significant, some of your error is coming from the study design!\n\n### Compare to other models\n\nIt can be shown that $E(MSReg) = \\sigma^2 + \\beta_1S_{XX}$.\n\n\\pspace\n\nConsider the Null hypothesis $\\beta_1 =0$ (why is this a good null?).\n\n- Under this null, $\\frac{MSReg}{s^2}\\sim F_{1, n-2}$.\n    - Obvious CI from this. \\lspace\n- This is equivalent to the t-test for $\\beta_1$! (See text.)\n\n\n### MSE of a Parameter: Bias of $s^2$\n\nFrom a previous class, we know that\n$$\n\\frac{(n-2)s^2}{\\sigma^2}\\sim\\chi^2_{n-2}\n$$\n\nFrom wikipedia, we know that the mean of a $\\chi^2_k$ distribution is $k$. Therefore,\n$$\nE\\left(\\frac{(n-2)s^2}{\\sigma^2}\\right) = n-2 \\Leftrightarrow E(s^2) = \\sigma^2\n$$\nand thus $s^2$ is unbiased.\\pause\n\nThis does *not necessarily* mean that $s^2$ is the best estimator for $\\sigma^2$!\n\n### MSE of a Parameter: Bias of $s$\n\nEven though $s^2$ is an unbiased estimator, $s = \\sqrt{s^2}$ is biased! Specifically, $E(s) < \\sigma$\n\nTo see why, first note that \n$$\nV(s) = E(s^2) - (E(s))^2 \\Leftrightarrow E(s) = \\sqrt{E(s^2) - V(s)}\n$$\nsince $V(s) > 0$, $E(s^2) - V(s) < E(s^2)$, and therefore\n$$\nE(s) < \\sqrt{E(s^2)} = \\sqrt{\\sigma^2} = \\sigma\n$$\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}