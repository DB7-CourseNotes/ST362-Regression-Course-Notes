{
  "hash": "c143d9519b48d724d9d67591e9abaaea",
  "result": {
    "markdown": "---\ntitle: \"Degrees of Freedom\"\ninstitute: \"**Jam TBD**\"\nexecute: \n    echo: true\n---\n\n\n\n\n## Generating Data (for the Project)\n\n### `model.matrix()` and Matrix Multiplication\n\n:::notes\nYou may find it convenient to use the `model.matrix()` function to set up dummy variables and polynomial terms for you. It will make sure that there is a reference category, which it will choose alphabetically.\n\nAlso note that you'll want to set `raw = TRUE` to ensure that people can find the coefficient values that you set. With `raw = FALSE` they will get the exact same predictions (and thus the same error), but not the same coefficient values.\n\nIn the code below, I also demonstrate a log-transform for $y$. To get a log on the left side of the equation, we use an exponential on the right.\n:::\n\n\n::: {.cell hash='L20-Degrees_of_Freedom_cache/html/unnamed-chunk-2_2c9dd3241dc91b423456be97e6016af7'}\n\n```{.r .cell-code}\nmycat <- c(\"category\", \"category\", \"dogegory\", \"category\", \"birdegory\")\nmycont <- c(12, 14, 4, 10,  20 )\n\nX <- model.matrix(~ mycat + poly(mycont, 2, raw = TRUE))\nX\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept) mycatcategory mycatdogegory poly(mycont, 2, raw = TRUE)1\n1           1             1             0                           12\n2           1             1             0                           14\n3           1             0             1                            4\n4           1             1             0                           10\n5           1             0             0                           20\n  poly(mycont, 2, raw = TRUE)2\n1                          144\n2                          196\n3                           16\n4                          100\n5                          400\nattr(,\"assign\")\n[1] 0 1 1 2 2\nattr(,\"contrasts\")\nattr(,\"contrasts\")$mycat\n[1] \"contr.treatment\"\n```\n:::\n\n```{.r .cell-code}\n# Names are just for my own purposes, they don't need to be included.\n# The names help me keep track of which column in X they correspond to.\nbetas <- c(intercpt = 10, cat = 20, dog = 0, c1 = 0, c2 = 0.05)\nX %*% betas\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [,1]\n1 37.2\n2 39.8\n3 10.8\n4 35.0\n5 30.0\n```\n:::\n\n```{.r .cell-code}\nmydf <- data.frame(\n    y = exp(X %*% betas + rnorm(5, 0, 0.01)), \n    mycat = mycat, \n    mycont = mycont\n)\ncoef(lm(log(y) ~ mycat + poly(mycont, 2, raw = TRUE), data = mydf))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 (Intercept)                mycatcategory \n                9.712145e+00                 1.978742e+01 \n               mycatdogegory poly(mycont, 2, raw = TRUE)1 \n                5.406038e-04                 8.415754e-02 \npoly(mycont, 2, raw = TRUE)2 \n                4.652628e-02 \n```\n:::\n:::\n\n\nFor the project, you ould only need to submit the `mydf` object; the others should be able to recover your coefficient values from that alone!\n\n\n## df\n\n### Degrees of Freedom\n\nA measure of how much information you chose to put in the model.\n\n- Continuous predictors add 1\\lspace\n- Categorical predictors add $k-1$\\lspace\n- Each term in a polynomial adds 1\\lspace\n- Interactions add 1\\lspace\n- etc.\n\n### Choose it and Use it\n\n- Decide on the df, try to use all of them\n    - Choice is based on how much information you think you can extract.\n    - Many rows = much information; you can probably use more predictors.\n\n### Bad use of df\n\n- Discretising a categorical variable\n    - You better have a *reeeaaaalllllyyyy* good justification\n    - For example, discretising age to match up with insurance categories.\n    - You should almost never choose to discretise based on your own logic; only to fit in with other analyses or use cases.\\lspace\n- Polynomial terms when interactions would work.\\lspace\n- Redundant categories\n    - Categories that are redundant\n    - If you have redundant categories, then you have categories that are redundant\n    - For example, if the \"JobTitle\" column has entries like \"Data Scientist\" as well as \"Data Scientist and Machine Learning Expert\", you could just code those both as \"DS\". \n\n\n\n\n### Saving df\n\n- Combine categories\n    - Just \"Data Scientist\" or \"Software Engineer\"\\lspace\n- Combine predictors\n    - Volume of beak = $\\pi r^2h/3$?\\lspace\n- Transform response rather than add polynomial terms\n    - Not always recommended.\n\n:::notes\nHere's an example of using transformations to (1) match the context of the problem and (2) get better results with fewer degrees of freedom.\n\nWe'll use the `trees` dataset that's built into R. The \"Girth\" column is actually the diameter, and it's the only column measured in inches rather than feet. I'm going to make a new predictor based on Girth that's more useful for later models.\n\n\n::: {.cell hash='L20-Degrees_of_Freedom_cache/html/unnamed-chunk-3_bbc43704285db23378c5d53f86f8b5b7'}\n\n```{.r .cell-code}\n# Saving df\nhead(trees) # \"Girth\" is actually diameter, according to help file\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n```\n:::\n\n```{.r .cell-code}\ntrees$Radius <- trees$Girth/24\n```\n:::\n\n\nA naive model might be a basic multiple linear regression.\n\n\n::: {.cell hash='L20-Degrees_of_Freedom_cache/html/unnamed-chunk-4_648233dc4597a6dd5175ef2fa365326b'}\n\n```{.r .cell-code}\nmultiple_lm <- lm(Volume ~ Radius + Height, data = trees)\nsummary(multiple_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Volume ~ Radius + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nRadius      112.9959     6.3424  17.816  < 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948,\tAdjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nWe could make this fit better by blindly adding polynomial terms and doing a transformation:\n\n\n::: {.cell hash='L20-Degrees_of_Freedom_cache/html/unnamed-chunk-5_0fbb34abdd9bacdb863d083a5f71b1c9'}\n\n```{.r .cell-code}\ntransformed_and_poly <- lm(log(Volume) ~ poly(Girth, 2) + poly(Height, 2), data = trees)\nsummary(transformed_and_poly)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Volume) ~ poly(Girth, 2) + poly(Height, 2), \n    data = trees)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.16094 -0.04023 -0.00295  0.05474  0.13434 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       3.27273    0.01492 219.360  < 2e-16 ***\npoly(Girth, 2)1   2.51150    0.09732  25.808  < 2e-16 ***\npoly(Girth, 2)2  -0.26046    0.09206  -2.829  0.00887 ** \npoly(Height, 2)1  0.54845    0.09746   5.628 6.47e-06 ***\npoly(Height, 2)2 -0.05518    0.09191  -0.600  0.55349    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08307 on 26 degrees of freedom\nMultiple R-squared:  0.9784,\tAdjusted R-squared:  0.9751 \nF-statistic: 294.5 on 4 and 26 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n- It is interesting that the squared term for height is *not* significant. Why is this so interesting to me? Look at the next equation in this lesson...\n\nA slightly better model might be one of the form\n$$\nV = \\pi r^2h\n$$\nwhich assumes that trees are perfect cylinders. This can be accomplished by modelling:\n\\begin{align*}\n\\log(V) &= \\beta_0 + \\beta_1\\log(r) + \\beta_2\\log(h) + \\epsilon\\\\\n\\implies V &= \\exp(\\beta_0)r^\\beta_1h^\\beta_2\\exp(\\epsilon)\n\\end{align*}\nand expecting that $\\exp(\\beta_0)$ is close to $\\pi$, $\\beta_1 = 2$, and $\\beta_2 = 1$.^[It makes me very happy that we're taking the \"log\" when talking about lumber.]\n\n\n::: {.cell hash='L20-Degrees_of_Freedom_cache/html/unnamed-chunk-6_f1cc3fe43894e360418052bfd4c30875'}\n\n```{.r .cell-code}\nvolume_logs <- lm(log(Volume) ~ log(Girth) + log(Height), data = trees)\nsummary(volume_logs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Volume) ~ log(Girth) + log(Height), data = trees)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.168561 -0.048488  0.002431  0.063637  0.129223 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.63162    0.79979  -8.292 5.06e-09 ***\nlog(Girth)   1.98265    0.07501  26.432  < 2e-16 ***\nlog(Height)  1.11712    0.20444   5.464 7.81e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08139 on 28 degrees of freedom\nMultiple R-squared:  0.9777,\tAdjusted R-squared:  0.9761 \nF-statistic: 613.2 on 2 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n- We get something close to our hopes!\n    - Except for $\\beta_0$, which we'll talk about later.\n- With this model, we could do a hypothesis test for $\\beta_1 = 2$ and $\\beta_2 = 1$. \n    - If these are reasonable values, loggers could confidently calculate the volume of a tree assuming that it's a cylinder!\n\nWe could also assume these values from the start, and include a \"naive\" volume.\n\n\n::: {.cell hash='L20-Degrees_of_Freedom_cache/html/unnamed-chunk-7_6dcc14dc702d5de4254e1d98b8d5778b'}\n\n```{.r .cell-code}\ntrees$naive_volume <- pi * trees$Radius^2 * trees$Height\n```\n:::\n\n\nWe could then model this according to\n\\begin{align*}\nV & = \\beta_0N + \\epsilon\n\\end{align*}\nwhere $N$ is our \"naive\" volume. We might have the expectation that $\\beta_0 = 1$ if the naive volume is correct.\n\n\n::: {.cell hash='L20-Degrees_of_Freedom_cache/html/unnamed-chunk-8_358190f275a22b71e52dd9fa3fdd756c'}\n\n```{.r .cell-code}\n# Assuming trees are cylinders\ndiff_from_cylinder <- lm(Volume ~ -1 + naive_volume, data = trees)\nsummary(diff_from_cylinder)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Volume ~ -1 + naive_volume, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6696 -1.0832 -0.3341  1.6045  4.2944 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \nnaive_volume 0.386513   0.004991   77.44   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.455 on 30 degrees of freedom\nMultiple R-squared:  0.995,\tAdjusted R-squared:  0.9949 \nF-statistic:  5996 on 1 and 30 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nThis tells us that the estimated usable lumber from a given tree is about 40\\% of what we would expect if the tree were a perfect cylinder.\n\nImportantly for this lecture, we have an $R^2$ of 0.9949 on a single degree of freedom! $R^2$ is not the greatest measure, but it's informative in this case:\n\n\n::: {.cell hash='L20-Degrees_of_Freedom_cache/html/unnamed-chunk-9_18cc5d323382feee8e87dbbbe28ebaaa'}\n::: {.cell-output-display}\n|model                |        R2| df|\n|:--------------------|---------:|--:|\n|multiple_lm          | 0.9442322|  2|\n|transformed_and_poly | 0.9750853|  4|\n|volume_logs          | 0.9760840|  2|\n|diff_from_cylinder   | 0.9948560|  1|\n:::\n:::\n\n\nBy choosing our transformations carefully, we have a model that is both *better* and *simpler*! The coefficient estimate also relates to a physical quantity that is useful to us - the percent of usable wood we can get from a tree! Statistics is amazing.^[T-shirt idea: \"If you don't think stats is lit af then you ain't woke, fam!\"]\n:::\n\n### Researcher Degrees of Freedom\n\nYou add information that isn't measured by df!\n\n- Choosing one predictor rather than another.\n    - Bill length *or* bill depth?\\lspace\n- Transforming a predictor/response\\lspace\n- Removing outliers\\lspace\n- Using/not using autoregressive error structures\\lspace\n- etc.\n\nThis is why we use RMarkdown/Quarto/Jupyter - all of this is (should be) recorded!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}