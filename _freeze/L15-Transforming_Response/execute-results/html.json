{
  "hash": "eb25af6584e89e70d1bcfcc9b7a4c6b5",
  "result": {
    "markdown": "---\ntitle: \"Transforming the Response\"\ninstitute: \"Jam: **Stabilise** by Nil√ºfer Yanya\"\n---\n\n\n## Transformations\n\n### Transforming the Predictors\n\nSuppose we found that the following second order polynomial model was a \"good\" fit:\n$$\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_{11}x_{i1}^2 +\\beta_2x_{i2} + \\beta_{22}x_{i2}^2 + \\beta_{12}x_{i1}x_{i2} + \\epsilon_i\n$$\n\n\\pause Now consider the model:\n$$\n\\ln(y_i) = \\beta_0 + \\beta_1x_{i1} + \\beta-2x_{i2} + \\epsilon_i\n$$\n\n- 3 parameters instead of 6!\n    - No interaction term!\n- If we're okay with the log scale for $y$, easier to interpret.\n\n### Transforming the predictors\n\n:::: {.columns}\n::: {.column width=\"50%\"}\nOriginal scale \n\n::: {.cell hash='L15-Transforming_Response_cache/html/unnamed-chunk-1_df31af42fe886300fde40e8f2e5ab983'}\n::: {.cell-output-display}\n![](L15-Transforming_Response_files/figure-html/unnamed-chunk-1-1.png){width=480}\n:::\n:::\n\n\n:::\n::: {.column width=\"50%\"}\nLog scale\n\n::: {.cell hash='L15-Transforming_Response_cache/html/unnamed-chunk-2_b489a24dccd21bf5ff4e1874f89b6ffa'}\n::: {.cell-output-display}\n![](L15-Transforming_Response_files/figure-html/unnamed-chunk-2-1.png){width=480}\n:::\n:::\n\n:::\n::::\n\nThe logarithm made it more linear, even if it's not quite right.\n\n### Consequences of Logarithms\n\nConsider the simple model $E(y_i) = \\beta x^2$. Taking the logarithm of both sides:\n$$\n\\ln(E(y_i)) = \\ln(\\beta) + 2\\ln(x) = \\beta_0 + \\beta_1 \\ln(x)\n$$\nand we have something that looks more like a linear model.\n\n\\pspace\n\n- Note that, instead of $x^2$, $x^{2.1}$ would also work as a model.\n    - The power of $x$ can be estimated.\\lspace\n- It's also possible that the log scale is the *correct* scale for $y$\n    - $E(\\ln(y_i)) = \\beta_0 + \\beta_1x$\n    - In other words, don't get too bogged down by whether we take the ln of $x$.\n\n\n### Logarithms and Errors\n\nIf we believe that the log scale is a better scale for $y$, we may postulate the model:\n$$\n\\ln(y_i) = \\beta_0 + \\beta_1\\ln x_{i1} + \\beta_2\\ln x_{i2} + \\epsilon\n$$\nwhich implies that the orginal scale for $y$ has the form:\n$$\ny_i = e^{\\beta_0}x_{i1}^{\\beta_1}x_{i2}^{\\beta_2}e^{\\epsilon_i}\n$$\n*The errors are multiplicative!!!*\n\n- Option 1: Accept this\n    - Allows us to use least squares.\\lspace\n- Option 2: Use the model $y_i = e^{\\beta_0}x_{i1}^{\\beta_1}x_{i2}^{\\beta_2} + \\epsilon_i$\n    - Might be better, but requires a bespoke estimation algorithm.\n\n### General Practice\n\nWe often simply use the model:\n$$\n\\ln \\underline y = X\\beta + \\underline \\epsilon\n$$\nand do *everything* on the log scale.\n\n- Simpler, but still useful.\\lspace\n- Good predictions of $\\ln y_i$ can be transformed to good predictions of $y_i$.\\lspace\n\n\\pspace\n\nIn general: Decide on a functional relationship between $f(y)$ and $X$, then use additive errors on the scale of $f(y)$.\n\nThis has *consequences*:\n\n## Residuals in Transformed Space\n\n### Variance Stabilization\n\nThe two main purposes of transformations:\n\n1. Fit non-linear functional forms.\\lspace\n2. Stabilize the variance!\n    - Scale-Location plot in the R defaults.\n\n\\pspace\n\nFor example, the log function brings large values down a lot, small values down a little.\n\n- The scale of large residuals is decreased more than the scale of small residuals.\n\n\n### $f(\\underline y) = X\\beta + \\underline\\epsilon$\n\nThe estimated residuals are $\\hat\\epsilon_i = f(y_i) - \\widehat{f(y_i)}$\n\n- Note the awkwardly long hat! \n    - We're estimating the value of the function, not the value of $y_i$.\n    - If $f(y_i) - \\widehat{f(y_i)} = f(y_i - \\widehat{y_i})$, then the original function must have been linear (and a transformation was useless).\\lspace\n- We're assuming $\\epsilon_i\\stackrel{iid}{\\sim} N(0, \\sigma^2)$, which is difficult to translate to $f^{-1}(X\\beta + \\underline\\epsilon)$.\n    - In the special case of $\\ln$, $\\exp{\\epsilon_i} \\sim \\text{LogNormal}(0, \\sigma^2)$.\n    - No assumption of **independence** on the original scale!!!\\lspace\n- We assume that the residuals have the same variance on the *transformed* scale.\n    - Likely not true for the original scale of $y$. \n\n### Some Good News\n\nIf $(a,b)$ is a $(1-\\alpha)$ CI on the scale of $f(y)$, then $(f^{-1}(a), f^{-1}(b))$ is a valid CI on the scale of $y$.\n\n\\pspace\n\n- It's not the only valid CI!\n    - Note that it's not a symmetric CI!\\lspace\n- Works for $y$ as well as the $\\beta$ parameters.\n    - Transformation might induce dependence among the parameters.\n    - A CI for $\\beta_1$ is useless if there's high covariance with $\\beta_2$. \n\n## Choosing Transformations\n\n### Methods for Choosing Transformations\n\n1. Theory.\n    - If theory says that the log transform makes sense, use that.\n        - Don't even consider the next steps. Just go with theory.\n    - Example: Forest fire burn sizes are right skewed, the log-transform makes sense.\n        - In my research, I used the lognorman lodel for the residuals to acheive the same effect.\\lspace\n2. Experimentation after looking at the Scale-Location plot.\n    - If log or sqrt don't work, move on to step three.\\lspace\n3. The Box-Cox Transformation\n    - Finds an appropriate transformation using maximum likelihood.\n\n### Box-Cox\n\nWe use the transformation:\n$$\nV = \\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda \\dot{Y}^{\\lambda - 1}} & \\text{if }\\lambda \\ne 0\\\\\n\\dot{Y}\\ln(Y) & \\text{if }\\lambda = 0\n\\end{cases}\n$$\nwhere $\\dot Y$ is the geometric mean of $y$.\n\n\\pspace\n\n$\\lambda$ is chosen through maximum likelihood\n\n- Essentially, refit with each value of $\\lambda$ and see which minimizes the residual variance.\n    - Plot the likelihhods and choose the highest.\n\n### Simpler Box-Cox\n\nThe textbook recommends the previous formula, however R uses:\n$$\nW = \\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda} & \\text{if }\\lambda \\ne 0\\\\\n\\ln(Y) & \\text{if }\\lambda = 0\n\\end{cases}\n$$\n\n### Variance of $\\lambda$\n\nIf we had a different data set, we'd get a different value of $\\lambda$!\n\n\\pspace\n\nR reports the the log-likelihood values, along with the top 5%.\n\n- Anything in the top 5% is reasonable.\n    - It's not an exact science.\\lspace\n- Usually, we check the best $\\lambda$ values and round to something nice.\n    - log, sqrt, squared, inverse, etc.\n\n### Summary\n\n- Choosing a transformation:\n    1. Theory\n    2. Exploration\n    3. Round the value from Box-Cox.\\lspace\n- Working with a transformation:\n    - Choose functional form, assume additive errors (usually, not always!)\n    - Stay on the transformed scale\n        - All assumptions about residuals apply to the transformed scale!\n\n\\pspace\n\nTo be useful, all transformations should consider the context of the problem!\n\n## Participation Questions\n\n### Q1\n\nIf the true relationship has the form $y = f(X) + \\epsilon_i$, we can always find a transformation $f^{-1}$ to make it linear.\n\n\\pspace\n\n1. True\n2. False\n\n### Q2\n\nIf we find that $\\lambda = 2$ is the best transformation, then the following models are equivalent:\n$$\n\\frac{Y^2 - 1}{2} = X\\beta\\quad\\text{and}\\quad Y^2 = 2X\\beta + 1\n$$\n\n\\pspace\n\n1. True\n2. False\n3. Technically not, but good enough in practice.\n\n### Q3\n\nA transformation of the form $y = f(X) + \\epsilon$ leads to multiplicative errors.\n\n\\pspace\n\n1. True \n2. False\n\n### Q4\n\nWhich of the following is *not* a good reason to investigate transformations?\n\n\\pspace\n\n1. If the variance looks unstable.\n2. If the theory says a transformation is necessary.\n3. If a transformation might lead to a much simpler model.\n4. If $y$ doesn't look normal.\n\n### Q5\n\nThe default residual plots in R can help diagnose the need for a transformation.\n\n1. True\n2. False\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}