{
  "hash": "3947b39598783bffa0b8506e3177af53",
  "result": {
    "markdown": "---\ntitle: \"Regularization Methods\"\ninstitute: \"Jam: **Ted Lasso Theme** by Marcus Mumford and Tom Howe\"\n---\n\n\n\n\n## Loss Functions\n\n### Why are we minimizing the *sum of squares*?\n\nThe MSE is defined as:\n$$\nMSE(\\underline\\beta) = \\frac{1}{n}\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\n$$\nThis is the Maximum Likelihood Estimate, which seeks to model the *mean* of $Y$ at each value of $X$, $E(Y) = X\\underline\\beta$, with Gaussian errors. \\pause\n\nMSE, seen as a function of $\\underline\\beta$, is a **loss function**, i.e. the function we minimize to find our estimates.\n\n\\quad\\pause\n\nBut it's FAR from the only loss function.\n\n### Other loss functions\n\nBy minimizing \n$$\nMAE(\\underline\\beta) = \\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right|\n$$\nwe end up estimating the $median$ of $Y$. \\pause\n\nOthers:\n\n- Mean Absolute Log Error\n    - Lower penalty for larger errors.\n    - More robust to outliers?\n- Mean Relative Error\n    - Penalize errors relative to size of $y$ (larger errors at large $y$ values aren't as big of a deal).\n    - Assumes that variance depends on mean (kinda like Poisson).\n- etc.\n\n### Examples\n\n- 0.5 hectares verus 4 hectares can make a huge difference\n    - 100 versus 150, congrats on the great prediction!\\newline\n- Predicting an income of 15,000 versus 25,000 is big\n    - Modelling the average income is not usually reasonable.\n\n### Loss Function Summary\n\n- Minimize the loss function with respect to the parameters of interest.\\newline\n- For the same parameters, there can be many loss functions.\\newline\n- Other names:\n    - Likelihood function (special case of loss function)\n    - Cost function (synonym)\n\n\n## Regularization\n\n### Regular Methods\n\nOrdinary least squares is a minimization problem:\n$$\nRSS = \\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\n$$\n\n\nWhat if I don't like how big the $\\underline\\beta$ values are?\n\n### Regularization Constraints\n\nLet's arbitrarily say that $\\sum_{j=1}^p \\beta_j = 10$.\n\n\\pause\\quad\n\nWith this constraint, one large $\\beta_j$ can be countered by a large negative $\\beta_k$.\n\n### Regularizing with Norms\n\nThe $L_p$-norm of a vector is:\n$$\n||\\beta||_p = \\left(\\sum_{j=1}^p|\\beta|^p\\right)^{1/p}\n$$\n\n- When $p = 2$, this is the Euclidean distance.\n    - Pythagoras strikes again!\\newline\n- When $p = 1$, this is the sum of absolute values.\\newline\n- When $p=\\infty$, this ends up being max($|\\underline\\beta|$).\n    - Not useful for our purposes, but interesting!\n\n### Why choose $||\\underline\\beta||_p = 10$?\n\nOr, in general, why choose a particular value of $s$ in $||\\underline\\beta||_p = s$?\n\n:::notes\nThere's no good reason to choose a *particular* value of $s$, but regularizing stops us from having steep slopes for predictors that aren't actually related. \n\nIn other words, we ignore spurious patterns!\n\nToo little regularization and we just have the OLS estimate. Too much regularization and we restrict the parameters too much.\n:::\n\n### Choosing $s$ in $||\\underline\\beta||_p = s$\n\n- Recall: more flexible models are able to estimate more subtle patterns, but may find patterns that aren't there.\n    - Too flexible = bad out-of-sample prediction.\\newline\n- For linear models, the *least flexible* model is one where all $\\beta_j$ values are given a fixed value.\n    - For example, all are 0. \\newline\n\n\\quad\\pause\n\nFor a linear model, restricting the values with $||\\underline\\beta||_p = s$ *reduces flexibility*, which can *improve out-of-sample* prediction performance. \n\n### MLE estimates of $\\underline\\beta$ are *unbiased*\n\n... therefore constrained estimates are biased. \n\n\n### But what about the scales of the features?\n\nWhat a great question! Thank you so much for asking! You must be smart.\n\n\\quad\n\nFor $||\\underline\\beta||_p$ to make sense, the predictors must all have the same scale. \n\nThis is accomplished by **standardizing** the features: Replace each $x_{ij}$ with\n$$\n\\frac{x_{ij} - \\bar{\\mathbf{x}_{j}}}{\\frac{1}{n}\\sum_{i=1}^n(x_{ij} - \\bar{\\mathbf{x}_{j}})^2}\n$$\n\n### Choosing $\\lambda$ via cross-validation\n\n- For each value of $\\lambda$:\n    - Split data into 5 \"folds\". \n    - For each \"fold\":\n        - Set aside the data points in the current fold.\n        - Fit the model to data in all other \"folds\" using your value of $\\lambda$.\n        - Predict the missing points, record the average error.\n\nChoose the lambda with the lowest out-of-sample prediction error.\n\n### Cross-Validation\n\n![](figs/kfold.png)\n\n### Special Cases of Regularization: $L_1$ or $L_2$?\n\nSo far, we've been talking about general $L_p$ norms, i.e. $||\\underline\\beta||_p$.\n\n\\pspace\n\n- $L_1$: LASSO\\lspace\n- $L_2$: Ridge\n\n### Geometric Interpretation (Contours of the RSS)\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n![](figs/Reg_Geom.png)\n:::\n::: {.column width=\"40%\"}\n\n\\vspace{2cm}\n\n- LASSO will set coefficients to 0.\n    - \"Least Absolute Shrinkage and Selection Operator\"\\newline\n- Ridge has less variance (why?)\n:::\n::::\n\n### Langrangian Multipliers and Estimation\n\nWikipedia screenshot:\n\n![](figs/Lagrange.png)\n\n### Lagrangian Multipliers and Estimation\n\n\\centering\nMinimize $MSE(\\underline\\beta)$ subject to $||\\underline\\beta||_p$.\n\nis equivalent to\n\nMinimize $MSE(\\underline\\beta) + \\lambda||\\underline\\beta||_p$\n\n\\quad\\pause\\raggedright\n\nFor the rest of your life, this is the way you'll see Ridge and LASSO.\n\n- Ridge: Analytical solution, can calculate an arbitrary number of $\\lambda$ values at once.\n- LASSO: Non-iterative numerical technique\n\n### Ridge Regularization\n\n\\begin{center}\n\\includegraphics[width=0.75\\textwidth]{figs/Ridge_Reg.png}\n\\end{center}\n\n- One of the coefficients *increases* with a tighter constraint!\n\n### LASSO Feature Selection as we Vary $\\lambda$\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n\\vspace{2cm}\n- As $\\lambda$ increases, more coefficients are allowed to be non-zero.\\newline\n- If $\\lambda$ doesn't constrain, we get the least squares estimate.\n    - Denoted as $\\hat\\beta$ in the plot.\n:::\n::: {.column width=\"50%\"}\n\n\\includegraphics[width=\\textwidth]{figs/Feature_Selection.png}\n\n:::\n::::\n\n## Participation Questions\n\n### Q1\n\nFor Ridge regression (L2 norm): as $\\lambda\\rightarrow\\infty$, $\\sum_{j=1}^p|\\beta_j| \\rightarrow\\infty$.\n\n\\pspace\n\n1. True\n2. False\n\n### Q2\n\nWhen we set the restriction on the sum of the absolute value of the coefficients, the intercept is included.\n\n\\pspace\n\n1. True\n2. False\n\n### Q3\n\nWhy would we restrict the value of the parameters?\n\n\\pspace\n\n1. Depending on the context of the problem, we might have a specific maximum value for the sum of the coefficients.\n2. We want to avoid overfitting the data, and restricting the parameter stops us from modelling irrelevant patterns.\n3. Because in any given regression problem there are always predictors that should have a slope of 0.\n4. Because some parameter estimates are impossible.\n\n### Q4\n\nWhat's the primary practical difference between Ridge and LASSO?\n\n\\pspace\n\n1. LASSO has a tighter restriction on the parameters than Ridge.\n2. LASSO will set parameters to 0, whereas Ridge will just shrink them towards 0 without making them exactly 0.\n3. Because coefficients are set to 0, LASSO has higher variance in its estimates. \n4. The only difference is the norm that they use, which has no meaningful impact on the results.\n\n### Q5\n\nWhat is cross-validation trying to minimize?\n\n\\pspace\n\n1. The out-of-sample prediction error. \n2. The bias in the estimates.\n3. The loss function (MSE).\n\n### Q6\n\nWhen should you use regularization?\n\n\\pspace\n\n1. When you want un unbiased estimate of the population parameters.\n2. When you want to avoid overfitting.\n3. When you want to be regular/normal/usual/typical.\n\n\n\n### Q7\n\nWhen should you use LASSO instead of Ridge?\n\n\\pspace\n\n1. When you want to do subset selection in a way that minimizes out-of-sample prediction error.\n2. When you want most of the coefficients to be 0.\n3. When you want to minimize out-of-sample prediction error at all costs.\n4. When you only want coefficients with significant p-values left over in your model.\n\n### Personal Opinion Time\n\nWith the existence of LASSO, there's no reason to do automated feature selection.\n\nBest subset selection can be written as:\n$$\n\\text{Minimize } MSE(\\underline\\beta)\\text{ subject to }\\sum_{j=1}^pI(\\beta\\ne 0) \\le s\n$$\nThis can minimize out-of-sample error, but results in something that could be mistaken for inference.\n\n\\quad\n\nWith LASSO, you know the estimates are biased and you know why. Best subset tricks you into thinking your $\\underline\\beta$ estimates are accurate - *they are not*.\n\n### Implementation in R: `glmnet`\n\n- The `glm` in `glmnet` is because it fits all GLMs.\n    - Including Logistic Regression.\n    - The `family = binomial` argument works as in `glm()`\n        - However, `family = \"binomial\"` is an optimized version.\\newline\n- The `net` in `glmnet` refers to elasticnet.\n    - Next slide or two.\n\n### Elastic Net: Like a lasso, but more \"flexible\" {.t}\n\n$$\n\\text{Minimize } MSE(\\underline\\beta) + \\lambda\\left[\\alpha||\\underline\\beta||_1 + (1-\\alpha)||\\underline\\beta||_2\\right] \n$$\n\n\\quad\n\nElastic Net is \"doubly regularized\".\n\nElastic net needs more time to fit and needs more data.\n\n### Elasticnet and LASSO/Ridge {.t}\n\n$$\n\\text{Minimize } MSE(\\underline\\beta) + \\lambda\\left[\\alpha||\\underline\\beta||_1 + (1-\\alpha)||\\underline\\beta||_2\\right] \n$$\n\n\\pspace\n\n- $\\alpha = 0 \\implies$ Ridge\n- $\\alpha = 1 \\implies$ LASSO\n\n\n:::notes\n\nHere's an example of LASSO in R. We'll load in the `Wage` data from `ISLR2` package^[ISLR stands for Introduction to Statistical Learning with R, a fantastic (and free) book if you want to learn more advanced topics in predictive modelling!].\n\nThis data set has a column for `wage` and a column for `logwage`. We're going to use `wage` as our response, and removing `wage` makes it easier to tell R to use all columns other than `logwage`. I also remove `region` since there are some regions with too few observations and I am not going to set up cross-validation appropriately for this scenario.\n\n\n::: {.cell hash='L21-Regularization_cache/html/unnamed-chunk-2_505c026d6c65833467c1fba44c5d1191'}\n\n```{.r .cell-code}\nlibrary(glmnet) # cv.glmnet() and glmnet()\nlibrary(ISLR2) # Wage data set\n\nWage <- ISLR2::Wage\n# From names(Wage), I want to remove \"region\" and \"wage\"\nWage <- Wage[, -c(6, 11)]\n```\n:::\n\n\n`glmnet` doesn't use the formula notation (`y ~ x`); we have to manually set up the design matrix (including dummy variables) and the response vector.\n\n\n::: {.cell hash='L21-Regularization_cache/html/unnamed-chunk-3_b20ebeb7ac6f91e6247e7dc78d407caa'}\n\n```{.r .cell-code}\nX <- model.matrix(logwage ~ ., data = Wage)[,-1]\ny <- as.numeric(Wage$logwage)\n```\n:::\n\n\nThe first step to fitting a LASSO model is choosing $\\lambda$ via cv. The `cv.glmnet()` function does this for us. The results are *not* a final model; the resultant object gives us an idea of which value of $\\lambda$ is appropriate. \n\n\n::: {.cell hash='L21-Regularization_cache/html/unnamed-chunk-4_30304bf77055562e9c9d12371f1824dd'}\n\n```{.r .cell-code}\ncv_check <- cv.glmnet(x = X, y = y, alpha = 1)\nplot(cv_check)\n```\n\n::: {.cell-output-display}\n![](L21-Regularization_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThe first dotted line indicates the value of $\\lambda$ that minimizes the \"loss function.\" However, across different samples we would get different values of $\\lambda$. Because we know there's randomness, we know that a slightly larger (more restrictive) value of $\\lambda$ would also be consistent with our data. Since cross-validation emulates the idea of having many samples, we can get an estimate of the **standard error** of $\\lambda$. We can then choose the value of $\\lambda$ that is within 1 standard error of the minimum. This gives a much simpler model while still having a plausible $\\lambda$.^[This is similar to the Box-Cox transformation, where we find a bunch of plausible transformations, and go with a simple one like `\\log()` or `sqrt()`.]\n\nNow that we have a way of telling R what value we want for lambda, we can fit the model.\n\n\n::: {.cell hash='L21-Regularization_cache/html/unnamed-chunk-5_4cb80920bc8e70db6535ddcf18364d5f'}\n\n```{.r .cell-code}\nmy_lasso <- glmnet(X, y, lambda = cv_check$lambda.1se)\nmy_lasso\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:  glmnet(x = X, y = y, lambda = cv_check$lambda.1se) \n\n  Df  %Dev Lambda\n1  9 35.59 0.0127\n```\n:::\n:::\n\n\nThe output isn't very informative, but the model can make predictions via the `predict()` function and these will be comparable or better than the predictions from an unconstrained linear model.\n\nLet's compare the coefficient values to see the shrinkage in action! Of course, glmnet standardizes by default, so we need to ensure that the linear model is based on standardized predictors.\n\nIn the output, I include a column for the difference in the coefficients. Specifically, it's lm minus lasso, so we may expect \"shrinkage\" to mean that the lasso estimates are smaller.\n\n\n::: {.cell hash='L21-Regularization_cache/html/unnamed-chunk-6_957513886439db3c940c04b7375e02b9'}\n\n```{.r .cell-code}\nstandardized_X <- apply(X, 2, scale)\nstandardized_lm <- lm(y ~ standardized_X)\ncoef_mat <- cbind(coef(my_lasso),\n    coef(standardized_lm))\n\nres <- cbind(\n        coef_mat, \n        apply(coef_mat, 1, function(x) abs(x[2]) - abs(x[1]))\n    ) |> \n    round(3)\ncolnames(res) <- c(\"lasso\", \"lm\", \"|lm|-|lasso|\")\nres\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n17 x 3 sparse Matrix of class \"dgCMatrix\"\n                             lasso     lm |lm|-|lasso|\n(Intercept)                 -7.830  4.654       -3.176\nyear                         0.006  0.026        0.019\nage                          0.002  0.029        0.027\nmaritl2. Married             0.128  0.076       -0.052\nmaritl3. Widowed             .      0.004        0.004\nmaritl4. Divorced            .      0.012        0.012\nmaritl5. Separated           .      0.017        0.017\nrace2. Black                 .     -0.012        0.012\nrace3. Asian                 .     -0.005        0.005\nrace4. Other                 .     -0.007        0.007\neducation2. HS Grad          .      0.038        0.038\neducation3. Some College     0.058  0.075        0.018\neducation4. College Grad     0.166  0.119       -0.047\neducation5. Advanced Degree  0.313  0.151       -0.162\njobclass2. Information       0.017  0.013       -0.004\nhealth2. >=Very Good         0.041  0.027       -0.014\nhealth_ins2. No             -0.189 -0.089       -0.100\n```\n:::\n:::\n\n:::\n\nThe estimates aren't all smaller! Lasso chose to set some to 0, which freed up some coefficient \"budget\" to spend elsewhere.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}