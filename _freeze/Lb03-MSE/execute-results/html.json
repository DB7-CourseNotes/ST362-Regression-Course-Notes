{
  "hash": "52fdd6bd73b4910ab829f7a4c5c9ca3c",
  "result": {
    "markdown": "---\ntitle: \"Bias/variance of $\\\\sigma^2$\"\noutput: html_document\n---\n\n\n## Best estimator of $\\sigma^2$\n\nWe saw in the notes that $E(s^2) = \\sigma^2$. Let's explore why this might not be the best estimator.\n\nWe define the MSE of an estimator $\\theta$ as $E((\\theta - \\hat\\theta^2)^2)$. For the variance, this is $E((\\sigma^2 - s^2)^2)$.\n\nLet's simulate a bunch of linear models, calculate the standard deviations, and then calculate this quantity.\n\nWe're going to focus on multiplicative bias, and you'll see why at the end. This means that we'll focus on estimators of the form $as^2$.\n\n\n::: {.cell hash='Lb03-MSE_cache/html/unnamed-chunk-1_e31bf3f083de452425071bb3bf49596e'}\n\n```{.r .cell-code}\nset.seed(2112)\n\nn <- 30\nx <- runif(n, 0, 10)\nbeta0 <- -3\nbeta1 <- -4\nsigma <- 3\n\nreps <- 1000\nesst <- double(reps)\n\nfor (i in 1:reps) {\n    y <- beta0 + beta1*x + rnorm(n, 0, sigma)\n    beta1 <- sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)\n    beta0 <- mean(y) - beta1 * mean(x)\n    yhat <- beta0 + beta1 * x\n    e <- y - yhat\n    esst[i] <- sum(e^2)\n}\n\na <- (n-4):(n+4)\nmse <- double(length(a))\nbias2 <- double(length(a))\nfor (i in seq_along(a)) {\n    mse[i] <- mean((sigma^2 - esst / a[i])^2)\n    bias2[i] <- (sigma^2 - mean(esst / a[i]))^2\n}\n\npar(mfrow = c(1,2))\nplot(a - n, mse, main = \"MSE = Bias^2 + Variance\")\nplot(a - n, bias2, main = \"Bias^2\")\nabline(h = 0)\n```\n\n::: {.cell-output-display}\n![](Lb03-MSE_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nFrom these plots, we see that the lowest MSE, i.e. the lowest value of $E((\\sigma^2 - as^2)^2)$, is at $a = 1/n$. Note that this corresponds to the MLE of $\\sigma^2$. However, this is a biased estimate, and the unbiased estimate occurs at $a=1/(n-2)$. \n\nWhat's happening here? Shouldn't unbiased be best? Well, yes, if our criteria is minimizing bias! If we want to minimize $E((\\sigma^2 - \\hat\\sigma^2)^2)$, we have to account for the variance of the estimator across all possible samples as well!\n\nHWK: Modify the code to show that the bias of the constant model ($\\beta_1 = 0$) is minimized at $n-1$, with the MSE being minimized at $n+1$. It's bizarre, but that's how it works!\n\n## Residuals\n\nThe `plot.lm()` function makes most of the plots you'll need.\n\n\n::: {.cell hash='Lb03-MSE_cache/html/unnamed-chunk-2_e15e60078b22b0a33bc51c26c772ee77'}\n\n```{.r .cell-code}\nmylm <- lm(mpg ~ disp, data = mtcars)\nplot(mylm, which=1:6)\n```\n\n::: {.cell-output-display}\n![](Lb03-MSE_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Lb03-MSE_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Lb03-MSE_files/figure-html/unnamed-chunk-2-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Lb03-MSE_files/figure-html/unnamed-chunk-2-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Lb03-MSE_files/figure-html/unnamed-chunk-2-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Lb03-MSE_files/figure-html/unnamed-chunk-2-6.png){width=672}\n:::\n:::\n\n\nThe `broom` package will be very useful in the future. In particular, the `augment()` function results in a tidy data frame with columns that are very relevant to our analyses.\n\n\n::: {.cell hash='Lb03-MSE_cache/html/unnamed-chunk-3_4630cc56142e2060871e82bcd5b47844'}\n\n```{.r .cell-code}\nlibrary(broom)\n\nhead(augment(mylm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 9\n  .rownames           mpg  disp .fitted .resid   .hat .sigma .cooksd .std.resid\n  <chr>             <dbl> <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n1 Mazda RX4          21     160    23.0  -2.01 0.0418   3.29 0.00865     -0.630\n2 Mazda RX4 Wag      21     160    23.0  -2.01 0.0418   3.29 0.00865     -0.630\n3 Datsun 710         22.8   108    25.1  -2.35 0.0629   3.28 0.0187      -0.746\n4 Hornet 4 Drive     21.4   258    19.0   2.43 0.0328   3.27 0.00983      0.761\n5 Hornet Sportabout  18.7   360    14.8   3.94 0.0663   3.22 0.0558       1.25 \n6 Valiant            18.1   225    20.3  -2.23 0.0313   3.28 0.00782     -0.696\n```\n:::\n:::\n\n::: {.cell hash='Lb03-MSE_cache/html/unnamed-chunk-4_0bbccc39916b8dcfc33a25566329e20b'}\n\n```{.r .cell-code}\nglance(mylm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.718         0.709  3.25      76.5 9.38e-10     1  -82.1  170.  175.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n:::\n\n::: {.cell hash='Lb03-MSE_cache/html/unnamed-chunk-5_6f5d048f91e807fca3852c4130dfab65'}\n\n```{.r .cell-code}\nanova(mylm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value   Pr(>F)    \ndisp       1 808.89  808.89  76.513 9.38e-10 ***\nResiduals 30 317.16   10.57                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell hash='Lb03-MSE_cache/html/unnamed-chunk-6_a778d912b508e2d6b7642785cb5d7af2'}\n\n```{.r .cell-code}\nx <- rnorm(1000); qqnorm(x); qqline(x)\n```\n\n::: {.cell-output-display}\n![](Lb03-MSE_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}