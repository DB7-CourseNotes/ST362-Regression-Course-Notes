{
  "hash": "1700ebd65a947799ebc80aa00170cfde",
  "result": {
    "markdown": "---\ntitle: \"The General Regression Situation\"\ninstitute: \"Jam: **I Predict a Riot** by The Kaiser Chefs\"\n---\n\n\n\n\n## Chapter Summary\n\n### The Normal Equations\n\n\\begin{align*}\n\\underline\\epsilon^T\\underline\\epsilon &= (Y - X\\underline\\beta)^T(Y - X\\underline\\beta)\\\\\n&= ... = Y^TY - 2\\underline\\beta^TX^TY + \\underline\\beta^TX^TX\\underline\\beta\n\\end{align*}\nWe then take the derivative with respect to $\\underline\\beta$. Note that $X^TX$ is symmetric and $Y^TX\\underline\\beta$ is a scalar..\n\\begin{align*}\n\\frac{\\partial}{\\partial\\underline\\beta}\\underline\\epsilon^T\\underline\\epsilon &= 0 - 2X^TY + 2X^TX\\underline\\beta\n\\end{align*}\n\n- For the 1 predictor case, make sure the equations look the same!\n\nSetting to 0, rearranging, and plugging in our data gets us the Normal equations:\n\\begin{align*}\nX^TX\\underline{\\hat\\beta} &= X^T\\underline y\n\\end{align*}\n\n### Facts\n\n$$X^TX\\underline{\\hat\\beta} = X^T\\underline y$$\n\n1. No distributional assumptions.\\lspace\n2. If $X^TX$ is invertible, $\\hat{\\underline\\beta} = (X^TX)^{-1}X^T\\underline y$.\n    - $\\hat{\\underline\\beta}$ is a linear transformation of $\\underline y$!\n    - This is the same as the MLE.\\lspace\n3. $E(\\hat{\\underline\\beta}) = \\underline\\beta$ and $V(\\hat{\\underline\\beta}) = \\sigma^2(X^TX)^{-1}$.\n    - This is the smallest variance amongst all unbiased estimators of $\\underline\\beta$.\n\n### Example Proof Problems\n\n1. Prove that $\\sum_{i=1}^n\\hat\\epsilon_i\\hat y_i = 0$.\n2. Prove that $(1/n)\\sum_{i=1}^n\\hat\\epsilon_i = 0$.\n3. Prove that $X^TX$ is symmetric. Is $A^TA$ symmetric in general?\n\n\\small\n\n::: {.cell hash='L05-General_Regression_cache/html/unnamed-chunk-2_e7f12321c6135325909563a827cd0b99'}\n\n```{.r .cell-code}\n## Demonstration that they're true (up to a rounding error)\nmylm <- broom::augment(lm(mpg ~ disp, data = mtcars))\nsum(mylm$.resid * mylm$.fitted)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.241496e-12\n```\n:::\n\n```{.r .cell-code}\nmean(mylm$.resid)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.77236e-15\n```\n:::\n\n```{.r .cell-code}\nX <- model.matrix(mpg ~ disp, data = mtcars)\nall.equal(t(X) %*% X, t(t(X) %*% X))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\n### Analysis of Variance (Corrected)\n\n| Source | $df$ | $SS$ |\n|--------|------|------|\n| Regression (corrected) | $p - 1$ | $\\underline{\\hat{\\beta}}^TX^T\\underline y - n\\bar{\\underline y}^2$  |\n| Error | $n - p$ | $\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y$ |\n| Total (corrected) | $n - 1$ | $\\underline y^t\\underline y - n\\bar{\\underline y}^2$ |\n\n- Note that $p$ is the number of parameters, not the index of the largest param.\n    - $\\underline\\beta = (\\beta_0, \\beta_1, ..., \\beta_{p-1})$\\lspace\n- We'll always be using corrected sum-of-squares.\n    - Especially next chapter!\n\n### $F$-test for overall significance\n\nIf SSReg is significantly larger than SSE, then fitting the model was worth it!\n\n- This is a test for $\\beta_1 = \\beta_2 = ... = \\beta_{p-1} = 0$, versus any $\\beta_j\\ne 0$.\n\nAs before, we find a quantity with a known distribution, then use it for hypothesis tests.\n\n$$\nF = \\frac{MS(Reg|\\hat\\beta_0)}{MSE} = \\frac{SS(Reg|\\hat\\beta_0)/(p-1)}{SSE/(n-p)} \\sim F_{p-1, n-p}\n$$\n\nAgain, note that a regression with no predictors always has $\\hat\\beta_0 = \\bar y$.\n\n### Example: Significance of `disp`\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n\\small\n\n::: {.cell hash='L05-General_Regression_cache/html/unnamed-chunk-3_3e2ba3730bd132dc62bc8d439cd5c6f5'}\n\n```{.r .cell-code}\nanova(lm(mpg ~ disp, data = mtcars)) |> \n    knitr::kable()\n```\n\n::: {.cell-output-display}\n|          | Df|   Sum Sq|   Mean Sq|  F value| Pr(>F)|\n|:---------|--:|--------:|---------:|--------:|------:|\n|disp      |  1| 808.8885| 808.88850| 76.51266|      0|\n|Residuals | 30| 317.1587|  10.57196|       NA|     NA|\n:::\n:::\n\n\n\\normalsize\n\n- $p = 2$, $n = 32$. \n    - $df_R' + df_E' = df_T'$, where $df'$ is the df for corrected SS.\n- Verified these numbers in the last lecture\n\n\n:::\n::: {.column width=\"40%\"}\n\n\n::: {.cell hash='L05-General_Regression_cache/html/unnamed-chunk-4_9e8d553d1c25dc5665d6097ae274a742'}\n\n```{.r .cell-code}\nplot(mpg ~ disp, data = mtcars)\nabline(lm(mpg ~ disp, data = mtcars))\n```\n\n::: {.cell-output-display}\n![](L05-General_Regression_files/figure-html/unnamed-chunk-4-1.png){width=384}\n:::\n:::\n\n:::\n::::\n\n### Example: $F_{1,p-1} = t^2_{p-1}$\n\n\\small\n\n::: {.cell hash='L05-General_Regression_cache/html/unnamed-chunk-5_fc9bf28fca9239bba9a19adce37cca72'}\n\n```{.r .cell-code}\nanova(lm(mpg ~ qsec, data = mtcars)) |> \n    knitr::kable()\n```\n\n::: {.cell-output-display}\n|          | Df|   Sum Sq|   Mean Sq|  F value|   Pr(>F)|\n|:---------|--:|--------:|---------:|--------:|--------:|\n|qsec      |  1| 197.3919| 197.39193| 6.376702| 0.017082|\n|Residuals | 30| 928.6553|  30.95518|       NA|       NA|\n:::\n\n```{.r .cell-code}\nsummary(lm(mpg ~ qsec, data = mtcars))$coef |>\n    knitr::kable()\n```\n\n::: {.cell-output-display}\n|            |  Estimate| Std. Error|    t value| Pr(>&#124;t&#124;)|\n|:-----------|---------:|----------:|----------:|------------------:|\n|(Intercept) | -5.114038| 10.0295433| -0.5098974|          0.6138544|\n|qsec        |  1.412125|  0.5592101|  2.5252133|          0.0170820|\n:::\n:::\n\n\n\\vspace{-3mm}\n\\normalsize\nWhat do you notice about these two tables?\n\n### Example: Significance of Regression ($F_{2,p-1} \\ne t^2_{p-1}$)\n\n\\scriptsize\n\n::: {.cell hash='L05-General_Regression_cache/html/unnamed-chunk-6_953c4232c8be9c0896a551070c1d197e'}\n\n```{.r .cell-code}\nanova(lm(mpg ~ qsec + disp, data = mtcars)) |> \n    knitr::kable()\n```\n\n::: {.cell-output-display}\n|          | Df|   Sum Sq|   Mean Sq|  F value|    Pr(>F)|\n|:---------|--:|--------:|---------:|--------:|---------:|\n|qsec      |  1| 197.3919| 197.39193| 18.25740| 0.0001898|\n|disp      |  1| 615.1185| 615.11850| 56.89424| 0.0000000|\n|Residuals | 29| 313.5368|  10.81161|       NA|        NA|\n:::\n\n```{.r .cell-code}\nsummary(lm(mpg ~ qsec + disp, data = mtcars))$coef |>\n    knitr::kable()\n```\n\n::: {.cell-output-display}\n|            |   Estimate| Std. Error|    t value| Pr(>&#124;t&#124;)|\n|:-----------|----------:|----------:|----------:|------------------:|\n|(Intercept) | 25.5045079|  7.1840940|  3.5501356|          0.0013359|\n|qsec        |  0.2122880|  0.3667758|  0.5787951|          0.5671961|\n|disp        | -0.0398877|  0.0052882| -7.5428272|          0.0000000|\n:::\n:::\n\n\nWe'll learn more about the ANOVA table next lecture.\n\n### $R^2$ again\n\n$$\nR^2 = \\frac{SS(Reg|\\hat\\beta_0)}{Y^TY - SS(\\beta_0)} = \\frac{\\sum(\\hat y_i - \\bar y)^2}{\\sum(y_i - \\bar y)^2}\n$$\n\nWorks for multiple dimensions!\\pause... kinda.\n\n### $R^2$ is bad?\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\\small\n\n::: {.cell hash='L05-General_Regression_cache/html/unnamed-chunk-7_3e87b15b00bab330c6be449efc0cc860'}\n\n```{.r .cell-code}\nnx <- 10 # Number of uncorrelated predictores\nuncorr <- matrix(rnorm(50*(nx + 1)), \n    nrow = 50, ncol = nx + 1)\n## First column is y, rest are x\ncolnames(uncorr) <- c(\"y\", paste0(\"x\", 1:nx))\nuncorr <- as.data.frame(uncorr)\n\nrsquares <- NA\nfor (i in 2:(nx + 1)) {\n    rsquares <- c(rsquares,\n        summary(lm(y ~ ., \n            data = uncorr[,1:i]))$r.squared)\n}\nplot(1:10, rsquares[-1], type = \"b\",\n    xlab = \"Number of Uncorrelated Predictors\",\n    ylab = \"R^2 Value\",\n    main = \"R^2 ALWAYS increases\")\n```\n:::\n\n\n:::\n::: {.column width=\"40%\"}\n\n::: {.cell hash='L05-General_Regression_cache/html/unnamed-chunk-8_c7c750f984775b069b445f6a62128d78'}\n::: {.cell-output-display}\n![](L05-General_Regression_files/figure-html/unnamed-chunk-8-1.png){width=384}\n:::\n:::\n\n:::\n::::\n\n### Adjusted (Multiple) $R^2$\n$$\nR^2_a = 1 - (1 - R^2)\\left(\\frac{n-1}{n-p}\\right)\n$$\n\n- Penalizes added predictors - won't always increase!\n    - Still might increase by chance alone!\n        - F-test\n    - $R^2_a = R^2$ when $p=1$ (intercept model)\\lspace\n- Still not perfect!\n    - Works for comparing different models on same data\n    - Works (poorly) for comparing different models on different data.\\lspace\n- In general you should use $R^2_a$, but always be careful.\n\n\n## Prediction and Confidence Intervals (Again)\n\n\n### $R^2$ and $F$\n\nRecall that\n$$\nF = \\frac{MS(Reg|\\hat\\beta_0)}{MSE} = \\frac{SS(Reg|\\hat\\beta_0)/(p-1)}{SSE/(n-p)} \\sim F_{p-1, n-p}\n$$\n\nFrom the definition of $R^2$,\n\\begin{align*}\nR^2 &= \\frac{SS(Reg|\\hat\\beta_0)}{SST}\\\\\n&= \\frac{SS(Reg|\\hat\\beta_0)}{SS(Reg|\\hat\\beta_0) + SSE}\\\\\n&= \\frac{(p-1)F}{(p-1)F + (n-p)}\n\\end{align*}\nConclusion: Hypothesis tests/CIs for $R^2$ aren't useful. Just use $F$!\n\n\n### Correlation of $\\hat\\beta_0$, $\\hat\\beta_1$, $\\hat\\beta_2$, etc.\n\nWith a different sample, we would have gotten slightly different numbers!\n\n\\pspace\n\n- If the slope changed, the intercept must change to fit the data\n    - (and \\emph{vice-versa})\n    - The parameter estimates are *correlated*!\\lspace\n- Similar things happen with multiple predictors!\\lspace\n- This correlation can be a problem for **confidence regions**\n\n### Uncorrelated $\\hat\\underline{\\beta}$\n\n$$\nV(\\hat{\\underline\\beta}) = \\sigma^2(X^TX)^{-1}\n$$\n\nIn simple linear regression,\n$$\n(X^TX)^{-1} = \\frac{1}{nS_{XX}}\\begin{bmatrix}\\sum x_i^2 & -n\\bar x\\\\-n \\bar x & n\\end{bmatrix}\n$$\n\nso the correlation is 0 when $\\bar x = 0$!\n\n### Prediction and Confidence Intervals for $Y$\n\n$\\hat Y = X\\hat\\beta$.\n\n- A confidence interval around $\\hat Y$ is based on the variance of $\\hat\\beta$.\n- $\\hat Y \\pm t * se(X\\hat\\beta)$\n\n\\pspace\n\n$Y_{n+1} = X\\beta + \\epsilon_{n+1}$\n\n- A prediction interval around $Y_{n+1}$ is based on the variance of $\\hat\\beta$ *and* $\\epsilon$!\n- $\\hat Y_{n+1} \\pm t * se(X\\hat\\beta + \\epsilon_{n+1})$\n\n\n## Participation Questions\n\n### Q1\n\nWhich of the following are the Normal equations?\n\n1. $X^TX\\underline\\beta = X^T\\underline y$\\lspace\n2. $X^TX\\underline{\\hat\\beta} = X^T\\underline y$\\lspace\n3. $\\hat{\\underline\\beta} = (X^TX)^{-1}X^T\\underline y$\\lspace\n4. $f(\\epsilon_i) = \\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp\\left(\\frac{-1}{2}\\epsilon_i^2\\right)$\n\n### Q2\n\nWhen is $X^TX$ not invertible?\n\n1. One of the predictors can be written as a linear combination of the others.\n2. There are more predictors than observations.\n3. One of the predictors has 0 variance.\n4. All of the above\n\n### Q3\n\nWhat does a significant F-test for the overall regression mean?\n\n1. The variance in the line is significantly larger than the variance in the data.\n2. The estimate of $\\beta_1$ is significantly different from $\\beta_0$,\n3. The variance of the line is significantly different from 0.\n4. At least one of the predictors in the model will have  significant $t$-test.\n\n### Q4\n\n$R^2$ is best used for:\n\n1. Determining whether a new predictor is worth including.\n2. Comparing models with different numbers of predictors.\n3. Comparing models based on different data sets.\n4. None of the above.\n\n### Q5\n\nWhich of the following describes a Prediction Interval?\n\n1. The CI for the predicted value of the line\n2. The CI for the predicted value of the line, including unobserved error at an $X$ value\n3. The CI for the predicted value of the line, including unobserved error at an $X$ value that was not observed in the data\n4. The CI for the predicted value of the line, including unobserved error at an $X$ value that was not observed in the data, using the true value of $\\sigma^2$\n\n### Q6\n\nWhich ANOVA table does the `anova()` function calculate?\n\n\\footnotesize\n\n1.\n\n| Source | $df$ | $SS$ |\n|--------|------|------|\n| Regression | 1 | $\\hat{\\underline\\beta}^TX^T\\underline y - n\\bar{\\underline y}^2$  |\n| Error | $n-2$ | $\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y$ |\n| Total | $n - 1$ | $\\underline y^t\\underline y - n\\bar{\\underline y}^2$ |\n\n\n2.\n\n| Source | $df$ | $SS$ |\n|--------|------|------|\n| Regression | 1 | $\\hat{\\underline\\beta}^TX^T\\underline y$  |\n| Error | $n-2$ | $\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y$ | \n| Total | $n - 1$ | $\\underline y^t\\underline y$ |\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}