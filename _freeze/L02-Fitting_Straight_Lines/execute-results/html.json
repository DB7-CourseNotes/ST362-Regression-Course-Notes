{
  "hash": "16d44c1f75e95b660c4c621427cd1599",
  "result": {
    "markdown": "---\ntitle: \"Fitting Straight Lines\"\ninstitute: \"Jam: **Great Expectations** by The Gaslight Anthem\"\n---\n\n\n\n## Why Fit Models?\n\n### The Quote.\n\n\"All models are wrong, some are useful.\" - George Box\n\n### All Models Are Wrong, But Some Are Useful\n\nModel: A mathematical equation that explains something about the world.\n\n- Gravity: 9.8 $m/s^2$ right?\n    - This is a *model* - it's a relationship that explains something in the world.\n    - Varies across the surface of the Earth.\n    - Varies according to air resistance.\\lspace\n- Every additional cigarette decreases your lifespan by 11 minutes.\n    - Very, very much wrong.\n    - Very, very useful for communication.\n\n### All Linear Models are Wrong\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {.cell hash='L02-Fitting_Straight_Lines_cache/html/unnamed-chunk-1_dc391d3c0fbd79af686aa99ca9c95106'}\n\n```{.r .cell-code}\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(palmerpenguins)\npenguins <- penguins[complete.cases(penguins),]\n\nggplot(penguins) +\n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(title = \"A line fits reasonably well\",\n        x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\")\n```\n\n::: {.cell-output-display}\n![](L02-Fitting_Straight_Lines_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n:::\n::: {.column width=\"50%\"}\n\n\n::: {.cell hash='L02-Fitting_Straight_Lines_cache/html/unnamed-chunk-2_bee496116d3b31b79e1b6b158933c92f'}\n\n```{.r .cell-code}\nggplot(mtcars) +\n    aes(x = disp, y = mpg) + \n    geom_point(size = 2) + \n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) + \n    geom_smooth(method = \"lm\", se = FALSE, formula = \"y~poly(x, 2)\", colour = 2) +\n    labs(title = \"A straight line misses the pattern\",\n        subtitle = \"A polynomial might fit better\",\n        x = \"Engine Displacement (1000 cu.in)\", \n        y = \"Fuel Efficiency (mpg)\")\n```\n\n::: {.cell-output-display}\n![](L02-Fitting_Straight_Lines_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n:::\n::::\n\n### Some Linear Models are Useful\n\n\n::: {.cell hash='L02-Fitting_Straight_Lines_cache/html/unnamed-chunk-3_6260eda0a710df66ff2394e5f0643399'}\n\n```{.r .cell-code}\nxpred <- 200\nypred <- predict(\n    lm(body_mass_g ~ flipper_length_mm, data = penguins),\n        newdata = data.frame(flipper_length_mm = xpred))\n\nggplot(penguins) +\n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(title = \"The height of the line is an okay prediction for Body Mass\",\n        x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\") +\n    annotate(geom = \"point\", shape = \"*\", size = 30, colour = 2,\n        x = xpred, \n        y = ypred) + \n    annotate(geom = \"segment\", colour = 2, size = 2, \n        linetype = \"dashed\",\n        x = xpred, xend = xpred, \n        yend = -Inf, y = ypred) + \n    annotate(geom = \"segment\", colour = 2, size = 2, \n        linetype = \"dashed\",\n        x = -Inf, xend = xpred, \n        yend = ypred, y = ypred)\n```\n\n::: {.cell-output-display}\n![](L02-Fitting_Straight_Lines_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n### A Model is an Equation\n\n$$\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\Leftrightarrow Y = X\\underline\\beta + \\underline\\epsilon \\Leftrightarrow E(Y|X) = X\\underline \\beta;\\; V(Y|X)=\\sigma^2\n$$\n\n- For a one unit increase in $x_i$, $y_i$ increases by $\\beta_1$.\n    - If our model fits well and this is statistically significant, we can apply this to the population.\\lspace\n- The estimated value of $y_i$ when $x_i=0$ is $\\beta_0$.\n    - Not always interesting, but usually$^*$ necessary.\\lspace\n\nOur prediction for $y_i$ at any given value of $x_i$ is calculated as:\n$$\n\\hat y_i = \\hat\\beta_0 + \\hat \\beta_1x_i\n$$\nwhere the \"hats\" mean we've found estimates for the $\\beta$ values.\n\n### Aside: My notation differs from the text\n\nI will make mistakes, but in general:\n\n- $Y$ is a random variable (usually a vector) representing the **response**.\n    - $Y_i$ is also a random variable (not a vector)\\lspace\n- $\\underline y$ is the response vector; the observed values of $Y$\\lspace\n- $\\underline x$ is the vector of **covariates** in **simple** linear regression\\lspace\n- $X$ is a matrix of covariates\n    - *Not* a random variable!!! Capital letters could be either random or matrix (or both). I will specify when necessary.\n    - I will avoid the notation $X_i$.\n    - When necessary, the first column is all ones so that $X\\underline\\beta=\\beta_0 + \\beta_1\\underline x_1 + \\beta_1\\underline x_2 + ...$.\n        - Context should make this clear.\n\n### The Mean of $Y$ at any value of $X$\n\n$$\nE(Y|X) = X\\underline\\beta\n$$\n\n\\includegraphics[width=0.9\\textwidth]{figs/mean.png}\n\n\n\n### A Linear Model is Linear in the *Parameters*\n\nThe following are linear:\n\n- $y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i$\n- $y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i$\n- $y_i = \\beta_0 + \\beta_1\\sin\\left(x_i^{x_i}\\right) + \\beta_2x_i^{\\sin(x_i)} + \\epsilon_i$\n\n\\pspace\n\nThe following are *not* linear:\n\n- $y_i = \\beta_0 + \\beta_1^2x_i +\\epsilon_i$\n- $y_i = \\beta_0 + \\beta_0\\beta_1x_i +\\epsilon_i$\n- $y_i = \\beta_0 + \\sin(\\beta_1)x_i +\\epsilon_i$\n\n## Estimation\n\n### Goal: Find $\\beta$ values which minimize the error\n\nModel: $y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i$\n\nError: $\\hat\\epsilon_i = y_i - \\hat y_i$\n\n\\pspace\n\nWhy is this a bad way to do it?\n\n### Least Squares\n\nGoal: Minimize $\\sum_{i=1}^n\\hat\\epsilon_i^2=\\sum_{i=1}^n(y_i - \\hat y_i)^2$, the **sum of squared errors**.\n\n\\pspace\n\nThis ensures errors don't cancel out. It also penalizes large errors more.\n\n\\pspace\n\nCould we have used $|\\epsilon_i|$, or some other function? $|ly|$!\n\n### Least Squares Estimates\n\nI assume that you can do the Least Squares estimate in your sleep (be ready for tests). \n\nThe final results are:\n\\begin{align*}\n\\hat\\beta_0 &= \\bar y - \\hat\\beta_1\\bar x\\\\\n\\hat\\beta_1 &= \\frac{\\sum_{i-1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i-1}^n(x_i - \\bar x)^2}\\stackrel{def}{=}\\frac{S_{XY}}{S_{XX}}\n\\end{align*}\n\n\\pspace\n\nThe textbook was written in 1998 and gives formulas to make the calculation easier to do on pocket calculators. You will not need a pocket calculator for this course.\n\n### Let's Add Assumptions\n\nAssumptions allow great things, but only when they're correct!\n\n\\pspace\n\n1. $E(\\epsilon_i) = 0$, $V(\\epsilon_i) = \\sigma^2$.\\lspace\n2. $cov(\\epsilon_i, \\epsilon_j) =0$ when $i\\ne j$. \n    - Implies that $E(y_i) = \\beta_0 + \\beta_1x_i$ and $V(y_i) = \\sigma^2$.\\lspace\n3. $\\epsilon_i\\stackrel{iid}{\\sim} N(0,\\sigma^2)$\n    - This is a strong assumption, but often works!\n\n\\pspace\n\nInterpretation: the model looks like a line with completely random errors. (\"Completely random\" doesn't mean \"without structure\"!)\n\n### Mean of $\\hat\\beta_1$\n\nIt is easy to show that \n$$\n\\hat\\beta_1 = \\frac{\\sum_{i-1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i-1}^n(x_i - \\bar x)^2} = \\frac{\\sum_{i-1}^n(x_i - \\bar x)y_i}{\\sum_{i-1}^n(x_i - \\bar x)^2}\n$$\nsince $\\sum(x_i - \\bar x) = 0$. \n\nHomework: show that $E(\\hat\\beta_1) = \\beta_1$ (*Hint: use the fact that $E(y_i) = \\beta_0 + \\beta_1x_i$ and the \"pocket calculator\" expansions*). \n\n### Variance of $\\hat\\beta_1$\n\n$$\n\\hat\\beta_1 = \\frac{\\sum_{i-1}^n(x_i - \\bar x)y_i}{\\sum_{i-1}^n(x_i - \\bar x)^2}\n$$\ncan be re-written as \n$$\n\\hat\\beta_1 = \\sum_{i=1}^na_iy_i\\text{, where }a_i = \\frac{x_i - \\bar x}{\\sum_{i=1}^n(x_i - \\bar x)^2}\n$$\n\nThus the variance of $\\hat\\beta_1$ is\n$$\nV(\\hat\\beta_1) = \\sum_{i=1}^na_i^2V(y_i) = ... = \\frac{\\sigma^2}{S_{XX}} \\stackrel{plug-in}{=} = \\frac{s^2}{S_{XX}}\n$$\n\n### Confidence Interval and Test Statistic for $\\hat\\beta_1$\n\nThe test statistic can be found as:\n$$\nt = \\frac{\\hat\\beta_1 - \\beta_1}{se(\\hat\\beta_1)} = \\frac{(\\hat\\beta_1 - \\beta_1)\\sqrt{S_{XX}}}{s} \\sim t_\\nu\n$$\n\nSince this follows a $t$ distribution, we can get the CI:\n$$\n\\hat\\beta_1 \\pm t_\\nu(\\alpha/2)\\sqrt{\\frac{s^2}{S_{XX}}}\n$$\n\n\n## Participation Questions\n\n### Q1\n\nAll models are useful, but some are wrong.\n\n\\pspace\n\n1. True\\lspace\n2. False\n\n### Q2\n\nFor a one unit increase in $x$, $y$ increases by\n\n\\pspace\n\n1. $\\beta_0$\\lspace\n2. $\\beta_1$\\lspace\n3. $\\beta_1x$\\lspace\n4. $\\beta_0 + \\beta_1x$\\lspace\n\n### Q3\n\nThe standard error of $\\beta_1$ refers to:\n\n\\pspace\n\n1. The variance of the population slope\\lspace\n2. The amount by which the slope might be off\\lspace\n3. The variance of the estimated slope across different samples\\lspace\n4. A big chicken. Not, like, worryingly big, but big enough that you'd be like, \"Wow, that's a big chicken!\"\\lspace\n\n### Q4\n\nWhich is *not* an assumption that we usually make in linear regression?\n\n\\pspace\n\n1. $V(\\epsilon_i) = \\sigma^2$\\lspace\n2. $E(\\epsilon_i) = 0$\\lspace\n3. $E(X) = 0$\n4. $\\epsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2)$\n\n### Q5\n\nWhich of the following is *not* a linear model?\n\n\\pspace\n\n1. $y_i = \\beta_0 + \\beta_1^2x_i + \\epsilon_i$\\lspace\n2. $y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i$\\lspace\n3. $y_i = \\beta_0 + \\beta_0x_i +\\epsilon_i$\\lspace\n4. $y_i = \\beta_0 + \\beta_1\\sin\\left(x_i^{x_i}\\right) + \\beta_2x_i^{\\sin(x_i)} + \\epsilon_i$\\lspace\n\n### Q6\n\nThe sum of squared errors is the best way to estimate the model parameters.\n\n\\pspace\n\n1. True\\lspace\n2. False\n\n\n\n\n## Analysis of Variance\n\n### Statistics is the Study of Variance\n\n- Given a data set with variable $\\underline y$, $V(\\underline y) = \\sigma^2_y$.\n    - This is just the variance of a single variable.\\lspace\n- Once we've incorporated the linear relationship with $\\underline x$, $V(\\hat\\beta \\underline x + \\underline\\epsilon)=0+V(\\underline\\epsilon) = \\sigma^2$ \n    - Mathematically, $\\sigma^2 \\le \\sigma_y^2$.\n\n\\pspace\n\nThe variance in $Y$ is **explained** by $X$!\n\n\n### A Useful Identity, and its Interpretations\n\n\\begin{align*}\n\\hat\\epsilon_i &= y_i - \\hat y_i \\\\&= y_i - \\bar y - (\\hat y_i - \\bar y)\\\\\n\\implies \\sum_{i=1}^n\\hat\\epsilon_i^2 &= \\sum_{i=1}^n(y_i-\\bar y)^2 - \\sum_{i=1}^n(\\hat y_i - \\bar y)^2\n\\end{align*}\nwhere we've simply added and subtracted $\\bar y$. The final line skips a few steps (try them yourself)!\n\n\\pspace\n\nThe last line is often written as: $SS_E = SS_T - SS_{Reg}$\n\n### Sums of Squares\n\n$$\nSS_E = SS_T - SS_{Reg}\n$$\n\n- $SS_E$: Sum of Squared Errors\n- $SS_T$: Sum of Squares Total (i.e., without $\\underline x$)\n- $SS_{Reg}$: Sum of Squares due to the regression.\n    - It's the variance of the line (calculated at observed $\\underline x$ values) around the mean of $\\underline y$???\n        - This is incredibly useful, but weird.\n    - I use $SS_{Reg}$ instead of $SS_R$ because some textbooks use $SS_R$ as the Sume of Squared *Residuals*, which is confusing.\n\n### Aside: Degrees of Freedom\n\nDef: The number of \"pieces of information\" from $y_1, y_2, ..., y_n$ to construct a new number.\n\n- If I have $x = (1,3,2,1,3,???)$ and I know that $\\bar x = 2$, I can recover the missing piece.\n    - The mean \"uses\" (accounts for) one degree of freedom\\lspace\n- If I have $x = (1,2,3,1,???,???)$ and I know $\\bar x = 2$ and $s_x^2=1$, I can recover the *two* missing pieces.\n    - The variance accounts for two degrees of freedom.\n        - One $df$ is required to compute it.\n\n\\pspace\n\nEstimating one parameter takes away one degree of freedom for the rest!\n\n- Can find $\\bar x$ when $x = (1)$, but can't find $s_x^2$ because there aren't enough $df$!\n\n### Sums of Squares\n\n| Source of Variation | Degress of Freedom $df$ | Sum of Squares ($SS$) | Mean Square ($MS$) |\n|---|---|---|---|\n| Regression | 1 | $\\sum_{i=1}^n(\\hat y_i - \\bar y)^2$ | $MS_{Reg}$ |\n| Error | $n-2$ | $\\sum_{i=1}^n(y_i - \\hat y_i)^2$ | $MS_E=s^2$ |\n| Total | $n-1$ | $\\sum_{i=1}^n(y_i - \\bar y)^2$ | $s_y^2$ |\n\n\\pspace\n\n- Notice that $SS_T = SS_{Reg} + SS_E$, which is also true for the $df$ (but not $MS$). \\lspace\n- Why is $df_E = n-2$? What two parameters have we estimated?\n    - $df_{Reg}$ is trickier to explain. It suffices to know that $df_{Reg} = df_T-df_E$.\n\n### Using Sums/Means of Squares\n\n- If $\\hat y_i = \\bar y$ for all $i$, then we have a horizontal line!\n    - That is, there is *no relationship* between $\\underline x$ and $\\underline $y$.\n    - In this case, $SS_{reg} = \\sum_{i=1}^n(\\hat y_i - \\bar y)^2 = 0$.\n\n\\pspace\n\nOkay, so, just test for $SS_{Reg} = 0$?\n\n\\pspace\\pause\n\nBut how??? We need some measure of *how far from 0* is statistically significant!!!\n\nRecall that $SS_E = \\sum_{i=1}^n(y_i - \\hat y_i)^2$.\n\n- We can compare the variation around the line to the variation *of* the line.\n    - This is $MS_{Reg}/MS_E$, and it follows an $F$ distribution!!\n\n### The F-test for Significance of Regression\n\n$$\nF_{df_{Reg}, df_E} = \\dfrac{MS_{Reg}}{MS_E} = \\dfrac{MS_{Reg}}{s^2}\n$$\n\n- Recall that $SS_{Reg} = SS_E + SS_T > SS_E$, but the $df$ make a difference.\\lspace\n- For homework, show that $E(MS_{Reg}) = \\sigma^2 + \\beta_1^2\\sum_{i=1}^n(x_i-\\bar x)^2$\\lspace\n- This implies that $E(MS_{Reg}) > E(MS_E) = \\sigma^2$.\n\n\n### Exercise A from Chapter 3 (Ch03 Exercises cover Ch1-3)\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\\vspace{0.25cm}\n\nA study was made on the effect of temperature on the yield of a chemical process. The data are shown to the right.\n\n\\pspace\n\n1. Assuming $y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i$, what are the least squares estimates of $\\beta_0$ and $\\beta_1$? \n2. Construct the analysis of variance table and test the hypothesis $H_0: \\beta_1=0$ at the  0.05 level.\n3. What are the confidence limits (at = 0.05) for $\\beta_1$?\n\n:::\n::: {.column width=\"50%\"}\n\n\n::: {.cell hash='L02-Fitting_Straight_Lines_cache/html/unnamed-chunk-4_396783e1a32453d959021fc1ee48e31d'}\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nlibrary(aprean3) # Data from textbook\nhead(dse03a) # Data Set Exercise Ch03A\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   x  y\n1 -5  1\n2 -4  5\n3 -3  4\n4 -2  7\n5 -1 10\n6  0  8\n```\n:::\n\n```{.r .cell-code  code-line-numbers=\"false\"}\nnrow(dse03a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 11\n```\n:::\n:::\n\n:::\n::::\n\n\n\n:::notes\nGood textbook questions: A, K, O, P, T, U, X (using `data(anscombe)` in R), Z, AA, CC. Note that all data sets in the textbook are available in an R package found [here](https://search.r-project.org/CRAN/refmans/aprean3/html/00Index.html).\n\nPlease see [Lab 2](Lb02-OLS_Estimates.html).\n:::\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}