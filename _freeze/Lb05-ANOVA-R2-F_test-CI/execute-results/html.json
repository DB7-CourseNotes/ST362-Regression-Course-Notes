{
  "hash": "f4ee666b243b05c48bf5829470143723",
  "result": {
    "markdown": "---\ntitle: \"ANOVA\"\noutput: html_notebook\n---\n\n\n## Basic ANOVA\n\n\n::: {.cell hash='Lb05-ANOVA-R2-F_test-CI_cache/html/unnamed-chunk-1_185927e02cb6098a61d6ed1c8656cb81'}\n\n```{.r .cell-code}\nset.seed(2221)\nplot(mpg ~ disp, data = mtcars)\nabline(lm(mpg ~ disp, data = mtcars))\n```\n\n::: {.cell-output-display}\n![](Lb05-ANOVA-R2-F_test-CI_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='Lb05-ANOVA-R2-F_test-CI_cache/html/unnamed-chunk-2_244f854bcdaa9a286d7941b2a14c80cb'}\n\n```{.r .cell-code}\nanova(lm(mpg ~ qsec, data = mtcars))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value  Pr(>F)  \nqsec       1 197.39 197.392  6.3767 0.01708 *\nResiduals 30 928.66  30.955                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell hash='Lb05-ANOVA-R2-F_test-CI_cache/html/unnamed-chunk-3_786fe58608e9b0e2467d02d692debcda'}\n\n```{.r .cell-code}\nsummary(lm(mpg ~ qsec, data = mtcars))$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Estimate Std. Error    t value   Pr(>|t|)\n(Intercept) -5.114038 10.0295433 -0.5098974 0.61385436\nqsec         1.412125  0.5592101  2.5252133 0.01708199\n```\n:::\n:::\n\n\nNotice the p-values! Also notice that the $F$-value is the square of the $t$-value! It's like magic! Math is cool.\n\n## $R^2$ always increases with new predictors\n\n\n::: {.cell hash='Lb05-ANOVA-R2-F_test-CI_cache/html/unnamed-chunk-4_6aaddb5d1e5eac0e52cc672e3baa5054'}\n\n```{.r .cell-code}\nnx <- 10 # Number of uncorrelated predictors\nuncorr <- matrix(rnorm(50*(nx + 1)), \n    nrow = 50, ncol = nx + 1)\n## First column is y, rest are x\ncolnames(uncorr) <- c(\"y\", paste0(\"x\", 1:nx))\nuncorr <- as.data.frame(uncorr)\n\nrsquares <- NA\nfor (i in 2:(nx + 1)) {\n    rsquares <- c(rsquares,\n        summary(lm(y ~ ., data = uncorr[,1:i]))$r.squared)\n}\nplot(1:10, rsquares[-1], type = \"b\",\n    xlab = \"Number of Uncorrelated Predictors\",\n    ylab = \"R^2 Value\",\n    main = \"R^2 ALWAYS increases\")\n```\n\n::: {.cell-output-display}\n![](Lb05-ANOVA-R2-F_test-CI_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThe one exception is when one of the predictors is a linear combination of the previous predictors. In this case, $R^2$ will not change!\n\n\n::: {.cell hash='Lb05-ANOVA-R2-F_test-CI_cache/html/unnamed-chunk-5_9cf6f8f173d6058f83ba9b44c8cb7d2c'}\n\n```{.r .cell-code}\nuncorr[, nx + 2] <- uncorr[,2] + 3*uncorr[,3]\nrsquares <- c(rsquares, summary(lm(y ~ ., data = uncorr))$r.squared)\nrsquares\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]          NA 0.003898013 0.056398466 0.056407225 0.069142075 0.073155890\n [7] 0.105824965 0.122449599 0.145307322 0.168746574 0.172758068 0.172758068\n```\n:::\n\n```{.r .cell-code}\nplot(rsquares, type = \"b\")\n```\n\n::: {.cell-output-display}\n![](Lb05-ANOVA-R2-F_test-CI_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='Lb05-ANOVA-R2-F_test-CI_cache/html/unnamed-chunk-6_98d2ed2ac57515a9533538b0e2fa8d29'}\n\n```{.r .cell-code}\nx <- runif(20, 0, 10)\nb0 <- 2; b1 <- 5; sigma <- 10\nb0s <- b1s <- double(1000)\nplot(NA, pch = 0, \n    xlim = c(-2, 12), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nabline(h = b0 + b1*mean(x))\nabline(v = mean(x))\nfor (i in 1:1000) {\n    y <- b0 + b1*x + rnorm(20, 0, sigma)\n    abline(lm(y ~ x), col = rgb(0,0,0,0.1))\n}\n```\n\n::: {.cell-output-display}\n![](Lb05-ANOVA-R2-F_test-CI_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nLet's do that again, but record the values and only show the 89% quantiles!\n\n\n::: {.cell hash='Lb05-ANOVA-R2-F_test-CI_cache/html/unnamed-chunk-7_b4f7cbb28412e78639937741e516866b'}\n\n```{.r .cell-code}\nx <- runif(20, 0, 10)\nb0 <- 2; b1 <- 5; sigma <- 10\nall_lines <- replicate(1000, {\n    y <- b0 + b1*x + rnorm(20, 0, sigma)\n    predict(lm(y ~ x), newdata = list(x = seq(0, 10, 0.1)))\n})\n\neightnine <- apply(all_lines, 1, quantile, probs = c(0.045, 0.945))\n```\n:::\n\n::: {.cell hash='Lb05-ANOVA-R2-F_test-CI_cache/html/unnamed-chunk-8_dff82bf6ecc13896ed871f83648af324'}\n\n```{.r .cell-code}\nplot(NA, pch = 0, \n    xlim = c(0, 10), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nlines(seq(0, 10, 0.1), eightnine[1, ])\nlines(seq(0, 10, 0.1), eightnine[2, ])\n```\n\n::: {.cell-output-display}\n![](Lb05-ANOVA-R2-F_test-CI_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nNote that the theoretical calculation of these bounds is built into R:\n\n\n::: {.cell hash='Lb05-ANOVA-R2-F_test-CI_cache/html/unnamed-chunk-9_30e536737cf1eb5a2b7b9dce09eacca6'}\n\n```{.r .cell-code}\nplot(NA, pch = 0, \n    xlim = c(0, 10), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nlines(seq(0, 10, 0.1), eightnine[1, ])\nlines(seq(0, 10, 0.1), eightnine[2, ])\n\n## Add the TRUE relationship\nxseq <- seq(0, 10, 0.1)\nlines(xseq, b0 + b1*xseq, col = 3)\n\n## New sample from the data generating process\nx <- runif(20, 0, 10)\ny <- b0 + b1*x + rnorm(20, 0, sigma)\n\n## Extract the CI\nmylm <- lm(y ~ x)\nxbeta <- predict(mylm, interval = \"confidence\",\n    newdata = list(x = xseq))\n#lines(xseq, xbeta[,\"fit\"], col = 4)\nlines(xseq, xbeta[,\"upr\"], col = 4)\nlines(xseq, xbeta[,\"lwr\"], col = 4)\n```\n\n::: {.cell-output-display}\n![](Lb05-ANOVA-R2-F_test-CI_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nNote that the intervals won't exactly align - the samples are going to be different each time! In 95% of the samples we collect from this data generating process, the CI we construct from the sample will contain the true (green) line. This is a basic definition for confidence intervals, but it's neat to see it around a line.\n\nNotice how the CI is curved. This is completely, 100% expected. Recall that $(\\bar x, \\bar y)$ is always a point on the line. If $x$ is the same for all samples, then the variance in the height at $\\bar y$ is just the variance in $y$. However, we can rotate the line around this point and still fit most of the data \"pretty well\", which is where the curved nature of the line comes from!\n\n### Aside\n\nWhy did I use the same $x$ values for all of the simulations? Because that's part of the assumptions (this isn't an important point to make). Again, notice how the point $(\\bar x, \\bar y)$ is always on the line, and how the variance at the point $\\bar x$ is minimized. If $\\bar x$ is randomly moved, then there's extra variance in the line. \n\n\n::: {.cell hash='Lb05-ANOVA-R2-F_test-CI_cache/html/unnamed-chunk-10_a93edabc2b8a04b2d131179e0de307bb'}\n\n```{.r .cell-code}\nb0 <- 2; b1 <- 5; sigma <- 10\nall_lines2 <- replicate(1000, {\n    x <- runif(20, 0, 10)\n    y <- b0 + b1*x + rnorm(20, 0, sigma)\n    predict(lm(y ~ x), newdata = list(x = seq(0, 10, 0.1)))\n})\n\neightnine2 <- apply(all_lines2, 1, quantile, probs = c(0.045, 0.945))\n```\n:::\n\n::: {.cell hash='Lb05-ANOVA-R2-F_test-CI_cache/html/unnamed-chunk-11_938e982af7449605ed34f2bf62a4bc7e'}\n\n```{.r .cell-code}\nplot(NA, pch = 0, \n    xlim = c(0, 10), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nlines(seq(0, 10, 0.1), eightnine[1, ])\nlines(seq(0, 10, 0.1), eightnine[2, ])\nlines(seq(0, 10, 0.1), eightnine2[1, ], col = 2)\nlines(seq(0, 10, 0.1), eightnine2[2, ], col = 2)\nlegend(\"topleft\", legend = c(\"non-random x\", \"random x\"), col = 1:2, lty = 1)\n```\n\n::: {.cell-output-display}\n![](Lb05-ANOVA-R2-F_test-CI_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThe textbook for this course also covers models that incorporate randomness in $X$, but this is not covered in this course.\n\n\n## Covariance of the parameters\n\n\n::: {.cell hash='Lb05-ANOVA-R2-F_test-CI_cache/html/unnamed-chunk-12_a09d0b730e4b7e3f9fc6de4d3f791896'}\n\n```{.r .cell-code}\nb0 <- 2; b1 <- 5; sigma <- 10\nall_params <- replicate(1000, {\n    x <- runif(20, 0, 10)\n    y <- b0 + b1*x + rnorm(20, 0, sigma)\n    coef(lm(y ~ x))\n})\n```\n:::\n\n::: {.cell hash='Lb05-ANOVA-R2-F_test-CI_cache/html/unnamed-chunk-13_8a8d6868fe54eae9e20c9c55ed461d42'}\n\n```{.r .cell-code}\nplot(t(all_params))\n```\n\n::: {.cell-output-display}\n![](Lb05-ANOVA-R2-F_test-CI_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nIntuition check: were you expecting a negative slope? Does this make sense? If you increase $\\beta_0$, why would $\\beta_1$ decrease?\n\nFor homework, try a negative intercept and see what happens! What about a negative slope?\n\n## Joint Normality of the $\\underline{\\hat\\beta}$s\n\nJoint normality leads to marginal normality! This means we can create a confidence interval based on the marginal. However, if the joint distribution has a strong correlation, the marginal confidence intervals might contain unlikely points!\n\n\n::: {.cell hash='Lb05-ANOVA-R2-F_test-CI_cache/html/unnamed-chunk-14_4b475eb7c1cea0d038f06d6900130f08'}\n\n```{.r .cell-code}\npar(mfrow = c(1, 3))\n\n## Marginal distribution of beta_0\nplot(density(all_params[1,]),\n    main = \"Distribution of b_0\",\n    xlab = \"b_0\")\nabline(v = quantile(all_params[1,], c(0.055, 0.945)), col = 2)\nabline(v = 8, col = 3) # Hypothesized beta_0\n\n## Marginal distribution of beta_1\nplot(density(all_params[2,]),\n    main = \"Distribution of b_1\",\n    xlab = \"b_1\")\nabline(v = quantile(all_params[2,], c(0.055, 0.945)), col = 2)\nabline(v = 6, col = 3) # Hypothesized beta_1\n\n## Joint distribution\nplot(t(all_params), main = \"Joint Distribution\",\n    xlab = \"b_0\", ylab = \"b_1\")\nabline(v = quantile(all_params[1,], c(0.055, 0.945)), col = 2)\nabline(h = quantile(all_params[2,], c(0.055, 0.945)), col = 2)\npoints(x = 8, y = 6, col = 3, pch = 16, cex = 1.5)\n```\n\n::: {.cell-output-display}\n![](Lb05-ANOVA-R2-F_test-CI_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nNotice how the rectangular confidence region in the joint distribution contains regions where there are no points! For example, a hypothesis test for whether $\\beta_0=8$ *and* $\\beta_1 = 6$ (the green lines/points) would not be rejected if we checked the two confidence intervals separately, but likely should be rejected given the joint distribution! This is exactly what happens when the F-test is significant but none of the t-tests for individual predictors is significant.\n\nIn general, the CIs for each individual $\\hat\\beta$ are missing something - especially if there's correlation in the predictors!\n\nIn these examples, we repeatedly sampled from the true relationship to obtain simulation-based confidence intervals. The normality assumption allows us to make inferences about the distribution of the parameters - including the joint distribution - from a single sample! Inference is powerful!\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}