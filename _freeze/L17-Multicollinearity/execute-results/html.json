{
  "hash": "d0e0ee050a86ccb453f03bbc83c5ec77",
  "result": {
    "markdown": "---\ntitle: \"Multicollinearity\"\ninstitute: \"**Jam TBD**\"\n---\n\n\n\n\n## The Problem\n\n### The Problem with Multicollinearity\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\\vspace{1cm}\n\n- Multiple regression fits a hyperplane\\lspace\n- If the points form a \"tube\", an infinite number of hyperplanes work.\n    - Rotate plane around axis of tube.\n\n:::\n::: {.column width=\"50%\"}\n\n\n::: {.cell hash='L17-Multicollinearity_cache/html/unnamed-chunk-2_cbaad353b6a734ca17054dc92f1795cc'}\n::: {.cell-output-display}\n![](L17-Multicollinearity_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n:::\n::::\n\n### Consequences of the Problem\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\\vspace{1cm}\n\nHigh cor. in $X$ $\\implies$ high cor. in $\\hat{\\underline\\beta}$.\n\n\\pspace\n\n- Many combos of $\\hat{\\underline\\beta}$ are equally likely\\lspace\n- No meaningful CIs\\lspace\n\n\n:::\n::: {.column width=\"50%\"}\n\n\n::: {.cell hash='L17-Multicollinearity_cache/html/unnamed-chunk-3_ea422bb245dbfbe11bfe8021894a8adb'}\n\n```{.r .cell-code}\nset.seed(2112)\nreplicate(1000, {\n    y <- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n    coef(lm(y ~ x1 + x2))[-1]\n}) |> \n    t() |> \n    plot(xlab = expression(hat(beta)[1]), \n        ylab = expression(hat(beta[2])),\n        main = \"Estimated betas for correlated\\npredictors, many samples\")\n```\n\n::: {.cell-output-display}\n![](L17-Multicollinearity_files/figure-html/unnamed-chunk-3-1.png){width=384}\n:::\n:::\n\n:::\n::::\n\n### Another Formulation of the Problem\n\nConsider the model $y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\epsilon_i$, where\n$$\nx_{i1} = a + bx_{i2} + z_i\n$$\nwhere $z_i$ represents some extra uncertainty. \n\n\\pspace\n\nFitting the model, we could:\n\n- Set $\\hat\\beta_1$ to 0, let $x_2$ model all of the variance.\n- Set $\\hat\\beta_2$ to 0, let $x_1$ model all of the variance.\n- Let $x_1$ model any proportion of the variance, let $x_2$ model the rest.\n\nThe parameter estimates are **not unique**.\n\n### The Source of the Problem\n\n$$\n\\hat{\\underline{\\beta}} = (X^TX)^{-1}X^TY,\\quad V(\\hat{\\underline{\\beta}}) = (X^TX)^{-1}\\sigma^2\n$$\n\n- If two columns of $X$ are **linearly dependent**, then $X^TX$ is **singular**.\n    - Constant predictor value (linearly dependent with column of 1s).\n    - Unit change (one column for Celcius, one for Fahrenheit).\\lspace\n- If two columns of $X$ are **nearly linearly dependent**, then some elements of $(X^TX)^{-1}$ are *humungous*.\n    - Two proxy measure for the same thing (e.g., daily high and low temperatures).\n    - Nearly linear transformation (e.g., polynomial or BMI)\n\n### Detecting the Problem\n\nThe variance-covariance matrix of $X$ can be useful:\n$$\nCov(X) = \\begin{bmatrix}\n0 & 0 & 0 & 0 & \\cdots\\\\\n0 & V(X_1) & Cov(X_1, X_2) & Cov(X_1, X_3) & \\cdots\\\\\n0 & Cov(X_1, X_2) & V(X_2) & Cov(X_2, X_3) & \\cdots\\\\\n0 & Cov(X_1, X_3) & Cov(X_2, X_3) & V(X_3) & \\cdots\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n\\end{bmatrix}\n$$\nWhy are the first column/row 0?\n\n### Plotting $Cor(X)$\n\n\n::: {.cell hash='L17-Multicollinearity_cache/html/unnamed-chunk-4_c155f19b7ad9099f1f38b6c36aa34f24'}\n\n```{.r .cell-code}\nlibrary(palmerpenguins); library(GGally)\nggcorr(penguins)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in ggcorr(penguins): data in column(s) 'species', 'island', 'sex' are\nnot numeric and were ignored\n```\n:::\n\n::: {.cell-output-display}\n![](L17-Multicollinearity_files/figure-html/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n### Detecting the Problem: $V(\\hat{\\underline\\beta})$\n\nUnfortunately, the var-covar matrix is hard to get from R.\n\n\\pspace\n\n- We can look at the SE column of the summary output!\n    - Very very very much not conclusive.\n- The **Variance Inflation Factor**\n\n### The Variance Inflation Factor\n\nWe can write the variance of each estimated coefficeint as:\n$$\nV(\\hat\\beta_i) = VIF_i\\frac{\\sigma^2}{S_{ii}}\n$$\nwhere $S_{ii} = \\sum_{k=1}^n(x_{ki} - \\bar{x_i})^2$ is the \"SS\" for the $i$th column of $X$.\n\n\\pspace\n\n- If there is no \"Variance Inflation\", then VIF = 1\n    - \"Inflation\" comes from the idea of rotating a plane around a \"tube\".\n    - Also interpreted as a measure of linear dependence with other columns of $X$.\n\n### Interpreting the Variance Inflation Factor\n\nConsider a regression of $X_i$ against all other columns of $X$.\n\n- The $R^2$ measures how well the other predictors can model $X_i$\n    - Label this $R_i^2$ to indicate it's the $R^2$ for $X_i$ against other columns.\n- Important: We're not considering $\\underline y$ at all!\n\n\\pspace\n\nThe VIF can be calculated as:\n$$\nVIF_i = \\frac{1}{1 - R_i^2}\n$$\n\n- If $R_i^2=0$, then $VIF_i = 1$\n- If $R_i^2\\rightarrow 1$, then $VIF_i \\rightarrow \\infty$\n\n## Will Scaling Fix the Problem\n\n### Scaling the Predictors\n\nIf we subtract the mean and divide by the sd, *some of* the correlation goes away.\n\n- This is actually kinda bad - we've hidden some multicollinearity from ourselves!\n\n\\pspace\n\nIf $Z$ is the **standardized** version of $X$, then\n$$\nCor(X) = Z^TZ/(n-1)\n$$\n\nIf $Z$ is the **mean-centered** version of $X$, then\n$$\nCov(X) = Z^TZ/(n-1)\n$$\n\n## Fixing The Problem\n\n### One way to fix the problem\n\nDon't.\n\n\\pause\\pspace\n\nWe can't get good estimates of the $\\hat\\beta$s, but we can still get good predictions.\n\n- This *only* works if the new values are in the same \"tube\" as the others.\\lspace\n- If the multicollinearity is real, what estimates do you expect?\n    - Without a controlled experiment, there *isn't* a good way to estimate the effect of $X_1$ on it's own!\\lspace\n\n### Removing predictors\n\nIf two predictors are measuring the same thing, then just include one?\n\n\\pspace\n\n- This might lose some information!\n    - It also might not!\\lspace\n- The estimated $\\beta$ won't be meaningful.\n    - Inferences will be difficult.\n\n## Participation Questions\n\n### Q1\n\nMulticollinearity can come from:\n\n\\pspace\n\n1. Unit changes\n2. Polynomial terms\n3. Proxy measures\n4. All of the above\n\n### Q2\n\nMulticollinearity is a problem because\n\n\\pspace\n\n1. Strong correlation in $X$ makes estimates of $\\beta$ invalid.\n2. Strong correlation in $X$ means there are many values of $\\underline\\beta$ that are equally probable.\n3. There's no way to fix strong correlation in $X$.\n\n\n### Q3\n\nWhen multicollinearity is present, which of the following is still valid?\n\n\\pspace\n\n1. Inferences about the effect of one of the predictors.\n2. Confidence intervals for a single coefficients.\n3. Predictions.\n4. Overall F test for significance of any slope parameter.\n\n### Q4\n\nThe VIF is defined as:\n\n\\pspace\n\n1. The amount that the MSE increases due to the variance in $\\hat{\\underline\\beta}$.\n2. The coefficient of determination of $X_i$ against all other predictors.\n3. The correlation between $X_i$ and all other predictors.\n4. The $R^2$ value for $Y$ against $X_i$.\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}