{
  "hash": "cfb307ee9676be699b0ac5265da0f8c4",
  "result": {
    "markdown": "---\ntitle: \"OLS Estimates\"\noutput: pdf_document\n---\n\n\nThe code in this notebook demonstrates what we've seen in class.\n\n\n::: {.cell hash='Lb02-OLS_Estimates_cache/html/unnamed-chunk-1_a05fddaa80ff24541894c349e3c5b13f'}\n\n```{.r .cell-code}\nset.seed(2112)\nn <- 30\nsigma <- 1\nbeta0 <- 1\nbeta1 <- 5\nx <- runif(n, 0, 10)\n\ndgp <- function(x, beta0 = 1, beta1 = 5, \n                sigma = 1) {\n    epsilon <- rnorm(length(x), mean = 0, sd = sigma)\n    y <- beta0 + beta1 * x\n    return(data.frame(x = x, y = y + epsilon))\n}\n\nset.seed(2112)\nmydata <- dgp(x = x, beta0 = beta0, beta1 = beta1,\n    sigma = sigma)\n\nplot(mydata)\n```\n\n::: {.cell-output-display}\n![](Lb02-OLS_Estimates_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nLet's find the estimate for $\\beta$ as well as the variance. \n\n\n::: {.cell hash='Lb02-OLS_Estimates_cache/html/unnamed-chunk-2_f3071ca1456ab9f8123b327f8ec61ae7'}\n\n```{.r .cell-code}\nSdotdot <- function(x, y) sum( (x - mean(x)) * (y - mean(y)) )\n\nSxx <- Sdotdot(mydata$x, mydata$x)\nSxy <- Sdotdot(mydata$x, mydata$y)\nSyy <- Sdotdot(mydata$y, mydata$y)\n\nb1 <- Sxy / Sxx\nb1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.925759\n```\n:::\n\n```{.r .cell-code}\nb0 <- mean(mydata$y) - b1 * mean(mydata$x)\nb0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.528286\n```\n:::\n\n```{.r .cell-code}\nerrors <- mydata$y - (b0 + b1 * mydata$x)\ns <- sd(errors)\nb1_var <- s^2 / Sxx\nb1_var\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.005382233\n```\n:::\n:::\n\n\nOur estimates of $\\beta_0$ and $\\beta_1$ are very close to the truth!\n\nNow, let's generate new data a bunch of times and see if our estimate of the *variance* of $\\hat\\beta_1$ is accurate. This is not a proof, just a demonstration.\n\nIn the code below, I'm also keeping track of $\\hat\\beta_0$ from each sample. This will be useful later.\n\n\n::: {.cell hash='Lb02-OLS_Estimates_cache/html/unnamed-chunk-3_ca32e156cf5a59b8c1322a55c91f08c4'}\n\n```{.r .cell-code}\nR <- 1000\nbeta1s <- rep(NA, R)\nbeta0s <- rep(NA, R)\nfor (i in 1:R) {\n    new_data <- dgp(x, beta0 = beta0, beta1 = beta1, sigma = sigma)\n    Sxx <- Sdotdot(new_data$x, new_data$x)\n    Sxy <- Sdotdot(new_data$x, new_data$y)\n    beta1s[i] <- Sxy / Sxx\n    beta0s[i] <- mean(new_data$y) - b1 * mean(new_data$x)\n}\n\nvar(beta1s)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.005407588\n```\n:::\n:::\n\n\nIn this example, we know the data generating process (dgp), so these randomly generated values are samples from the population.\n\nAs we know, the standard error is the variance of the estimator over all possible samples from the population. We only took 1000, but that's probably close enough to infinity to draw the sampling distribution.\n\n\n::: {.cell hash='Lb02-OLS_Estimates_cache/html/unnamed-chunk-4_9eab42b64dad4f6cb2bbb939ff1531d3'}\n\n```{.r .cell-code}\nhist(beta1s)\nabline(v = 5, col = 2, lwd = 10) # true value of beta\n```\n\n::: {.cell-output-display}\n![](Lb02-OLS_Estimates_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nAs you can see, $\\hat\\beta_1$ is unbiased and has some variance (at this stage, there's nothing to compare this variance to so we can't really call it \"large\" or \"small\").\n\nLet's use this to calculate an 89\\% Confidence Interval!\n\n\n::: {.cell hash='Lb02-OLS_Estimates_cache/html/unnamed-chunk-5_7703203f6bfec14da148d6cbde3f0443'}\n\n```{.r .cell-code}\n## Empirical Ci\nquantile(beta1s, c(0.055, 0.945))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    5.5%    94.5% \n4.883143 5.120293 \n```\n:::\n\n```{.r .cell-code}\n## Theoretical CI\nSxx <- Sdotdot(mydata$x, mydata$x)\n5 + c(-1, 1) * qt(0.945, df = nrow(mydata) - 2) * 1/sqrt(Sxx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.882763 5.117237\n```\n:::\n\n```{.r .cell-code}\n## Sample CI\nb1 + c(-1, 1) * qt(0.945, nrow(mydata) - 2) * s/sqrt(Sxx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.804666 5.046851\n```\n:::\n:::\n\n\nThe empirical and the theoretical CIs line up pretty well! However, the sample CI is fundamentally different. The sample CI is centered at the estimated value, and we expect it to *contain* the true population value 89\\% of the time!\n\nWe practically never know the actual DGP, so this is just a demonstration that the math works. \n\nNote that we treated $\\underline x$ as if it were fixed. The value $S_{XX}$ will be different for different $\\underline x$, and we don't make any assumptions about what distribution $\\underline x$ follows.\n\n## Analysis of Variance\n\nThe code below demonstrates how the ANOVA tables are calculated.\n\n\n::: {.cell hash='Lb02-OLS_Estimates_cache/html/unnamed-chunk-6_38d5d87837022f20b6e036e2f02cea70'}\n\n```{.r .cell-code}\ny <- mydata$y\nyhat <- b0 + b1*mydata$x\nybar <- mean(y)\n\nc(sum((yhat - ybar)^2), sum((y - yhat)^2), sum((y - ybar)^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4809.33809   30.93852 4840.27661\n```\n:::\n\n```{.r .cell-code}\nANOVA <- data.frame(\n    Source = c(\"Regression\", \"Error\", \"Total (cor.)\"),\n    df = c(1, nrow(mydata) - 2, nrow(mydata)),\n    SS = c(sum((yhat - ybar)^2), sum((y - yhat)^2), sum((y - ybar)^2))\n)\nANOVA$MS <- ANOVA$SS / ANOVA$df\nANOVA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Source df         SS          MS\n1   Regression  1 4809.33809 4809.338089\n2        Error 28   30.93852    1.104947\n3 Total (cor.) 30 4840.27661  161.342554\n```\n:::\n:::\n\n\nThis is equivalent to what R's built-in functions do!\n\n\n::: {.cell hash='Lb02-OLS_Estimates_cache/html/unnamed-chunk-7_2ec53fa1903ad92fb3df2aba85e92131'}\n\n```{.r .cell-code}\nanova(lm(y ~ x, data = mydata))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nx          1 4809.3  4809.3  4352.5 < 2.2e-16 ***\nResiduals 28   30.9     1.1                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## Dependence and Centering\n\nSomething not touched on in class is that $cov(\\hat\\beta_0, \\hat\\beta_1)$ is not 0! This should be clear from the formula got $\\hat\\beta_0$, which is $\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x$. \n\nThe code below repeats what we did before, but with higher variance to better demonstrate the problem.\n\nIt also records the estimates based on *centering* $\\underline x$. Notice how the formula for $\\hat\\beta_0$ is no longer dependent on $\\hat\\beta_1$ if the mean of $\\underline x$ is 0!\n\n\n::: {.cell hash='Lb02-OLS_Estimates_cache/html/unnamed-chunk-8_e4a3f97a466908b61cf986dd676058cd'}\n\n```{.r .cell-code}\nb1s <- double(R)\nb0s <- double(R)\nb1cs <- double(R)\nb0cs <- double(R)\nn <- 25\nx <- runif(n, 0, 10)\nxc <- x - mean(x) # centered\n\nfor (i in 1:R) {\n    y <- 2 - 2 * x + rnorm(25, 0, 4)\n    b1 <- Sdotdot(x, y) / Sdotdot(x, x)\n    b0 <- mean(y) - b1 * mean(x)\n    b1s[i] <- b1\n    b0s[i] <- b0\n\n    # Centered\n    y <- 2 - 2 * xc + rnorm(25, 0, 4)\n    b1c <- Sdotdot(xc, y) / Sdotdot(xc, xc)\n    b1cs[i] <- b1c\n    b0c <- mean(y) - b1 * mean(xc)\n    b0cs[i] <- b0c\n}\n\npar(mfrow = c(1,2))\nplot(b1s, b0s)\nplot(b1cs, b0cs)\n```\n\n::: {.cell-output-display}\n![](Lb02-OLS_Estimates_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}