{
  "hash": "160afba5eb508e3ec3da727979fdbdd7",
  "result": {
    "markdown": "---\ntitle: \"Serial Correlation in the Residuals\"\ninstitute: \"Jam: **Like Yesterday** by Ryan Adams and The Cardinals\"\n---\n\n\n## Serial Correlation in the Residuals\n\n### Assumptions and Definitions\n\nThe time of each observation is recorded, and they are equally spaced.\n\n- In other words, $x_1$ is observed at time 1, $x_2$ is observed at time 2, etc.\n\n\\pspace\\pause\n\n**Serial Correlation**: $cor(\\epsilon_{t-1}, \\epsilon_t)\\ne 0$\n\n- *Serial Correlation is not causation.*\n    - Knowledge of one gives you more knowledge of the other.\n- Serial correlation can be *negative*\n    - Example: didn't hit quota today, so big push tomorrow.\n\n### Visualizing Serial Correlation in the Residuals\n\nR\n\n## Durbin-Watson\n\n### Strong Assumption about Correlation\n\nThe usual model: $Y = X\\underline{\\beta} + \\underline \\epsilon$.\n\n\\pspace\n\nAssume that $cor(\\epsilon_{t-1}, \\epsilon_t) = \\rho$, $cor(\\epsilon_{t-2}, \\epsilon_t)= \\rho^2$, $cor(\\epsilon_{t-3}, \\epsilon_t) = \\rho^3$, etc.\n\n- The correlation is proportional to the distance in time.\n\n\\pspace\n\nThis can be written as:\n$$\n\\epsilon_t = \\rho\\epsilon_{t-1} + z_t\n$$\nwhere $z_t\\sim N(0,\\sigma^2)$. Note that $V(\\hat\\epsilon_t) = \\frac{\\sigma^2}{1 - \\rho^2}$.\n\n### The Durbin-Watson test statistic\n\nAs usual, we find a quantity with a known distribution:\n$$\nd = \\dfrac{\\sum_{s=2}^n(\\hat\\epsilon_s - \\hat\\epsilon_{s-1})^2}{\\sum_{s=1}^n\\hat\\epsilon_s^2}\\sim\\text{ some complicated distribution}\n$$\n\n\\pspace\n\n- Distribution has a closed form, but I hate it.\n- $d\\in [0, 4]$, with $d=2$ corresponding to the null.\n- R will calculate the values for you. \n    - Textbook has pages and pages of tables. Textbook was written before iPhones existed.\n\nI'll show an example of DW later.\n\n### Cautions with Durbin-Watson\n\n- Tests the hypotheses $H_a:\\;cor(\\epsilon_{t-s}, \\epsilon_t) = \\rho^s$ versus not that.\n    - There are many, many other $H_a$. DW has low power for these situations.\\lspace\n- Graphical summaries will reveal strong patterns; patterns found by DW might not be worrisome. \\lspace\n- It's more p-values to look at. We want to minimize the number of p-values we look at.\n\n## Graphical Methods\n\n### Empirical Autocorrelation\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\\vspace{1cm}\n\n- We can simply find the correlation between $\\hat \\epsilon_t$ and $\\hat \\epsilon_{t-1}$!\\lspace\n- We can do the same for $\\hat \\epsilon_t$ and $\\hat \\epsilon_{t-2}$.\n    - etc.\\lspace\n\n\n:::\n::: {.column width=\"50%\"}\n| t | $\\hat \\epsilon_t$ | $\\hat \\epsilon_{t-1}$ | $\\hat \\epsilon_{t-2}$ |\n|---|---|---|---|\n| 2 | $\\hat \\epsilon_1$ | NA | NA |\n| 2 | $\\hat \\epsilon_2$ | $\\hat \\epsilon_1$ | NA |\n| 3 | $\\hat \\epsilon_3$ | $\\hat \\epsilon_2$ | $\\hat \\epsilon_1$ |\n| 4 | $\\hat \\epsilon_4$ | $\\hat \\epsilon_3$ | $\\hat \\epsilon_2$ |\n| $\\cdots$ | $\\cdots$ | $\\cdots$ | $\\cdots$ |\n\n:::\n::::\n\n### The ACF: Empirical Autocorrelations of lag $k$\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\\vspace{1cm}\n\nACF: AutoCorrelation Function\n\n\\pspace\n\nThe x-axis shows the lag, the y axis shows the correlations\n\n\\pspace\n\nThe plot on the right shows an example of time series data.\n\n:::\n::: {.column width=\"50%\"}\n\n\n::: {.cell hash='L09-Serial_Correlation_cache/html/unnamed-chunk-1_532ebc91f6d7b706dc436c2a85ea14c6'}\n\n```{.r .cell-code}\npar(mfrow = c(2, 1))\nplot(co2, main = \"Time series of CO2 data\")\nacf(co2, main = \"ACF of CO2 data (not residuals)\")\n```\n\n::: {.cell-output-display}\n![](L09-Serial_Correlation_files/figure-html/unnamed-chunk-1-1.png){width=576}\n:::\n:::\n\n\n:::\n::::\n\n### ACF isn't ideal\n\nIn the model we saw for DW,\n$$\n\\epsilon_t = \\rho\\epsilon_{t-1} + z_t\n$$\nwhich means that \n$$\n\\epsilon_t = \\rho(\\rho\\epsilon_{t-2} + z_{t-1}) + z_t\n$$\n\nThe lag 2 correlation (the $\\rho^2$ term) includes the lag 1 correlation!\n\n### Partial Autocorrelations\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\\vspace{1cm}\n\nIf we extend the model to:\n$$\n\\epsilon_t = \\rho_1\\epsilon_{t-1} + \\rho_2\\epsilon_{t-2} + z_t,\n$$\nthen $\\rho_2$ is the correlation in the lag 2 terms, *accounting for lag 1 terms*!\n\n\\pspace\n\nThis is the PACF, and it's often much more useful. \n\n\\pspace\n\nThe plot on the right shows a cyclic trend.\n\n:::\n::: {.column width=\"50%\"}\n\n::: {.cell hash='L09-Serial_Correlation_cache/html/unnamed-chunk-2_10593d3c13efa91968bd84900b3e9136'}\n\n```{.r .cell-code}\npar(mfrow = c(2, 1))\nplot(co2, main = \"Time series of CO2 data\")\npacf(co2, main = \"PACF of CO2 data (not residuals)\")\n```\n\n::: {.cell-output-display}\n![](L09-Serial_Correlation_files/figure-html/unnamed-chunk-2-1.png){width=576}\n:::\n:::\n\n:::\n::::\n\n### DW, ACF, and PACF in practice\n\nMost of the time, just check the PACF.\n\n\\pspace\n\n- If you see something, check ACF.\\lspace\n- If you're in a field that requires p-values, show them the DW statistic.\n    - Or use a non-parametric test...\n\n\n### What to do if there is autocorrelation?\n\n- Correlation in the residuals might mean correlation in the $y_i$'s\n    - Try time series modelling!\\lspace\n- If it's simple (lag 1) autocorrlation, the data could potentially be transformed to remove the autocorrelation.\n    - $y_t - y_{t-1} = X\\underline \\beta$.\n    - Change estimation to account for autocorrelation.\\lspace\n- If it's complicated, get a PhD student to do it for you.\n\n## Participation\n\n### Q1\n\nSerial correlation can be tested for any data set.\n\n\\pspace\n\n1. \n2. True\n3. False\n4. \n\n### Q2\n\nA non-significant result from the DW test means there is no autocorrelation in the residuals.\n\n\\pspace\n\n1. True\n2. \n3. \n4. False\n\n### Q3\n\nThe quantity $d = \\dfrac{\\sum_{s=2}^n(\\hat\\epsilon_s - \\hat\\epsilon_{s-1})^2}{\\sum_{s=1}^n\\hat\\epsilon_s^2}$ is a statistic because:\n\n\\pspace\n\n1. It's a value calculated from the data, possibly including information from outside the data.\n2. It's a value calculated from the data only, with no other information.\n3. It's a value calculated from the data only, with no other information, and has a known distribution.\n4. It's not a statistic since it's not estimating a population parameter.\n\n### Q4\n\nThe \"partial\" in PACF refers to:\n\n1. The PACF plot only contains some of (partial) information.\n2. The PACF evaluates the lag $k$ correlation after controlling for lags 1 to $k-1$.\n3. The PACF is only evaluated for a portion of the data.\n4. The PACF for a lag of $k$ cannot use the first $k-1$ data points (they are the NAs in the table).\n\n### Q5\n\nWhich of the following is *not* an assumption of the DW test?\n\n1. The time points are all equally spaced.\n2. The correlation between two residuals is equal to $\\rho$.\n3. The further apart two residuals are, the less correlated they are.\n4. There is no missing data.\n\n### Q6\n\nAutocorrelation means that there are no possible insights into the data.\n\n1. True, the study was worthless.\n2. False, there are standard methods that will work for any situation.\n3. False, but we won't get into the details in this course.\n\n\n\n\n\n## Non-Parametric Test for Autocorrelation\n\n### Runs\n\n$\\sum_{i=1}^n\\hat\\epsilon_i = 0$, so some residuals are positive and some are negative. \n\nThe runs test just looks at the *sign* of the residuals. Consider the signs:\n\n```\n+ + - + - - - - + + - + + + + \n```\n\nThere are 7 runs in these residuals. Is this a lot of runs?\n\n### Defining \"A Lot Of Runs\"\n\np-value: Probability of a result *at least as extreme* as the one obtained, under the null hypothesis.\n\n- Null: random +'s and -'s.\n\n\\pspace\n\nFor small numbers, we can look at all sequences of +'s and -'s and count the runs!\n\n- P(7 or more runs) is an upper tailed test (-ive autocorrelation)\n- P(7 or fewer runs) is a test for +ive autocorrelation\n\n### Large Numbers: Of course it's Normal!\n\nGiven $n_1$ +'s and $n_2$ -'s, the mean and variance of the number of runs is:\n$$\n\\mu = \\frac{2n_1n_2}{n_1 + n_2} + 1\\text{, and }\\sigma^2 = \\frac{2n_1n_2(2n_1n_2 - n_1 - n_2)}{(n_1+n_2)^2(n_1 + n_2 - 1)}\n$$\n\nIn the actual distribution, $P(runs\\le \\mu) = P(runs\\le \\mu -1/2) = P(runs < \\mu + 1/2)$.\n\n\\pspace\n\nIn the normal distribution this isn't true, so we apply a correction factor:\n\n- Lower-tailed test: $runs\\sim N(\\mu + 1/2, \\sigma^2)$\n- Upper-tailed test: $runs\\sim N(\\mu - 1/2, \\sigma^2)$\n- Two-tailed test: $runs\\sim N(\\mu, \\sigma^2)$ and we hope it averages out.\n\n\n### Example\n\n\n::: {.cell hash='L09-Serial_Correlation_cache/html/unnamed-chunk-3_b5e903cc79b70afc7e54d8b111752bfa'}\n\n```{.r .cell-code}\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/SerialCorrelation\")\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}