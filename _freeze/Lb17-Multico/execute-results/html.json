{
  "hash": "de0bb99184cd4855454e317f7b234ff6",
  "result": {
    "markdown": "---\ntitle: \"Multicollinearity\"\noutput: html_notebook\n---\n\n\n\n\n## The problem with multicollinearity\n\nConsider the model $$\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\epsilon_i\n$$ with the added assumption that $x_{i1} = a + bx_{i2} + z_i$, where $z_i$ is some variation.\n\n(As a technical note, none of this is assumed to be random; just uncertain. We'll use the language of probability in this section, but it's just to quantify uncertainty rather than make modelling assumptions.)\n\n\n::: {.cell hash='Lb17-Multico_cache/html/unnamed-chunk-2_92cb6038cedd4f307637f9ccd48c716b'}\n\n```{.r .cell-code}\nlibrary(plot3D)\nn <- 100\nx1 <- runif(n, 0, 10)\nx2 <- 2 + x1 + runif(n, -1, 1)\ny <- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n\nscatter3D(x1, x2, y, phi = 30, theta = 15, bty = \"g\",\n    xlab = \"x1\", ylab = \"x2\", zlab = \"y\")\n```\n\n::: {.cell-output-display}\n![](Lb17-Multico_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe 3D plot looks like a tube! By \"tube\", we can think of the 3D plot as being a 2D plot that's in the wrong coordinate system. In other words, the data define a single axis, which is different from the x1, x2, and y axes. When fitting multiple linear regression, we're fitting a hyperplane. If the relationship is **multicollinear**, then we might imagine rotating a hyperplane around the axis defined by the \"tube\" in the plot above. Since any rotation of the plane around fits the line of data pretty well, the coefficients that define that plane are not well-estimated.\n\nIt's hard to imagine 3D sets of points, so let's do a 2D analogy. In the 3D example, we actually had a 2D relationship, so let's consider a 2D example that's almost 1D.\n\nIn the plot below, I've made $x$ *almost* one-dimensional. That is, I've given it a vary small range. I intentionally changed the x limits of the plot to emphasize this.\n\n\n::: {.cell hash='Lb17-Multico_cache/html/unnamed-chunk-3_5bf61dc64c693c571f951bf973c84d9b'}\n\n```{.r .cell-code}\nx <- c(rep(-0.025, 10), rep(0.025, 10))\ny <- x + rnorm(20, 0, 3)\nplot(y ~ x, xlim = c(-2, 2))\n```\n\n::: {.cell-output-display}\n![](Lb17-Multico_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nIf we were to fit a regression line to this, there are many many slopes that would make this work!\n\n\n::: {.cell hash='Lb17-Multico_cache/html/unnamed-chunk-4_dd4328e784252cc086504e381aaea808'}\n\n```{.r .cell-code}\nplot(y ~ x, xlim = c(-1, 1))\nfor(i in 1:20) abline(a = mean(y), b = runif(1, -20, 20))\n```\n\n::: {.cell-output-display}\n![](Lb17-Multico_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nAll of the lines I added will fit the data pretty well, even though they all have completely different slopes! Yes, there's one with a \"best\" slope, but slightly different data would have given us a very different slope:\n\n\n::: {.cell hash='Lb17-Multico_cache/html/unnamed-chunk-5_45fa71569e6faf367f21fafe7e319901'}\n\n```{.r .cell-code}\n# Set up empty plot\nplot(NA, xlab = \"x\", ylab = \"y\",\n    xlim = c(-2, 2), ylim = c(-6,6))\n# Generate data from same dgp as before\nfor(i in 1:100) {\n    y <- x + rnorm(20, 0, 3)\n    abline(lm(y ~ x))\n}\n```\n\n::: {.cell-output-display}\n![](Lb17-Multico_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nAll of these lines would have worked for our data!\n\nThis can easily be seen the *variance of the parameters*:\n\n\n::: {.cell hash='Lb17-Multico_cache/html/unnamed-chunk-6_f933a0152cfafd09585d6753a2fe8de0'}\n\n```{.r .cell-code}\nsummary(lm(y ~ x))$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              Estimate Std. Error    t value  Pr(>|t|)\n(Intercept) -0.6205096  0.6779798 -0.9152331 0.3721680\nx            6.0310840 27.1191916  0.2223917 0.8265128\n```\n:::\n:::\n\n\n## Exact Multicollinearity\n\nIn the example above, the line was very sensitive to a slight change in the data because we essentially had one dimension. If we *actually* had one dimension, then any line that cgoes through $\\bar y$ has the exact same error, regardless of the slope. We can see this in the design matrix: $X$ has a column of 1s for the intercept (which is good), but also has a column for $x_1$ that has zero variance. This means that it's a linear combination of the first column, and thus is rank-deficient.\n\n\n::: {.cell hash='Lb17-Multico_cache/html/unnamed-chunk-7_c8600459a314f5b0a03ae62f2f56e52a'}\n\n```{.r .cell-code}\nX <- cbind(rep(1, 20), rep(0, 20))\ntry(solve(t(X) %*% X))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in solve.default(t(X) %*% X) : \n  Lapack routine dgesv: system is exactly singular: U[2,2] = 0\n```\n:::\n:::\n\n\nSince one column is a linear combination of the other, $X^TX$ cannot be inverted. In this case, the variance fo $\\hat\\beta_1$ is infinite!\n\nThis can also happen if one column is \"close\" to being a linear combination of the others, such as:\n\n\n::: {.cell hash='Lb17-Multico_cache/html/unnamed-chunk-8_3b46b9a18887008eca0e930562479b03'}\n\n```{.r .cell-code}\n# Change the first element in the second column to 10^-23\nX[1, 2] <- 1e-23\ntry(solve(t(X) %*% X))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in solve.default(t(X) %*% X) : \n  system is computationally singular: reciprocal condition number = 4.75e-48\n```\n:::\n:::\n\n\nThe rows are mathematically different, but the difference is so small that computers cannot tell.\n\n\n::: {.cell hash='Lb17-Multico_cache/html/unnamed-chunk-9_d50dd14f42c61067e43f519f3d2280de'}\n\n```{.r .cell-code}\nx <- runif(20, 0, 1) / 1e7\nX <- cbind(1, x)\ntry(solve(t(X) %*% X))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            x\n   1.697904e-01 -2.406244e+06\nx -2.406244e+06  4.833450e+13\n```\n:::\n:::\n\n\nIn the next code chunk, try playing around with the power of 10 (i.e., try `10^50`, `10^-50`, etc., for both positive and negative powers). At some point, the matrix is not invertible and the coefficient table stops reporting the slope! At the other end, the line is a near perfect fit (why?).\n\n\n::: {.cell hash='Lb17-Multico_cache/html/unnamed-chunk-10_23c6c9218f06c7956a385ebc24104dae'}\n\n```{.r .cell-code}\nx <- runif(20, 0, 1) / (10^350)\ny <- x + rnorm(20, 0, 3)\nsummary(lm(y ~ x))$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Estimate Std. Error   t value  Pr(>|t|)\n(Intercept) 0.1092976  0.6173655 0.1770387 0.8613516\n```\n:::\n:::\n\n\n## Back to Regression\n\nWe can kinda detect multicollinearity mainly using the standard error of the estimates:\n\n\n::: {.cell hash='Lb17-Multico_cache/html/unnamed-chunk-11_845de9727a900891454945d102e8f0a9'}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\npeng_lm <- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n    data = penguins)\nsummary(peng_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm + bill_length_mm + \n    bill_depth_mm, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1054.94  -290.33   -21.91   239.04  1276.64 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -6424.765    561.469 -11.443   <2e-16 ***\nflipper_length_mm    50.269      2.477  20.293   <2e-16 ***\nbill_length_mm        4.162      5.329   0.781    0.435    \nbill_depth_mm        20.050     13.694   1.464    0.144    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 393.4 on 338 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7615,\tAdjusted R-squared:  0.7594 \nF-statistic: 359.7 on 3 and 338 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nIt looks like `bill_depth_mm` has a large standard error! Of course, this might be because:\n\n-   The variance of `bill_depth_mm` is high to begin with.\n-   The other predictors have explained most of the variance and any estimate for `bill_depth_mm` will do.\n-   Multicollinearity\n\nMultico. is just one of the possible reasons why the SE might be high, we need to look into it more to be sure.\n\n\n::: {.cell hash='Lb17-Multico_cache/html/unnamed-chunk-12_affdef7129d3e3831985ddf1cac6bc18'}\n\n```{.r .cell-code}\nlibrary(car)\nvif(peng_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nflipper_length_mm    bill_length_mm     bill_depth_mm \n         2.673338          1.865090          1.611292 \n```\n:::\n:::\n\n\nIt actually has quite a small VIF!\n\n## Centering and Scaling\n\n\n::: {.cell hash='Lb17-Multico_cache/html/unnamed-chunk-13_7bf68e40bcd560bfa926291f14002875'}\n\n```{.r .cell-code}\nX <- model.matrix(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n    data = penguins)\n\nZ <- cbind(1, apply(X[, -1], 2, scale))\nprint(\"correlation of X\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"correlation of X\"\n```\n:::\n\n```{.r .cell-code}\nround(cor(X), 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  (Intercept) flipper_length_mm bill_length_mm bill_depth_mm\n(Intercept)                 1                NA             NA            NA\nflipper_length_mm          NA            1.0000         0.6562       -0.5839\nbill_length_mm             NA            0.6562         1.0000       -0.2351\nbill_depth_mm              NA           -0.5839        -0.2351        1.0000\n```\n:::\n\n```{.r .cell-code}\nprint(\"Also correlation of X\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Also correlation of X\"\n```\n:::\n\n```{.r .cell-code}\nround(t(Z) %*% Z/ (nrow(Z) - 1), 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                         flipper_length_mm bill_length_mm bill_depth_mm\n                  1.0029            0.0000         0.0000        0.0000\nflipper_length_mm 0.0000            1.0000         0.6562       -0.5839\nbill_length_mm    0.0000            0.6562         1.0000       -0.2351\nbill_depth_mm     0.0000           -0.5839        -0.2351        1.0000\n```\n:::\n:::\n\n\nBy simulation:\n\n\n::: {.cell hash='Lb17-Multico_cache/html/unnamed-chunk-14_d753e8830221e68710b55399f000422e'}\n\n```{.r .cell-code}\nn <- 100\nx1 <- runif(n, 0, 10)\nx2 <- 2 + x1 + runif(n, -3, 3)\n\nreps_1 <- replicate(1000, {\n    y <- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n    coef(lm(y ~ x1 + x2))\n}) |> t()\ncor(reps_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)         x1         x2\n(Intercept)   1.0000000  0.1764180 -0.6112093\nx1            0.1764180  1.0000000 -0.8609998\nx2           -0.6112093 -0.8609998  1.0000000\n```\n:::\n:::\n\n::: {.cell hash='Lb17-Multico_cache/html/unnamed-chunk-15_50bd588c98788a9bc339e56e98224d02'}\n\n```{.r .cell-code}\nx1 <- scale(x1)\nx2 <- scale(x2)\n\nreps_2 <- replicate(1000, {\n    y <- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n    coef(lm(y ~ x1 + x2))\n}) |> t()\ncor(reps_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)          x1          x2\n(Intercept)  1.00000000 -0.01364783  0.03646404\nx1          -0.01364783  1.00000000 -0.85526575\nx2           0.03646404 -0.85526575  1.00000000\n```\n:::\n:::\n\n\nThe correlation is... slightly lower? It's much lower for the intercept, but it doesn't make much of a difference for the correlation between the slopes. (It will generally be lower, and should be with a large enough number of simulations.)\n\nJust for fun, here's the VIF for these data. I've added a parameter `x1_around_x2` to allow you to play around with the correlation of `x1` and `x2.`\n\n\n::: {.cell hash='Lb17-Multico_cache/html/unnamed-chunk-16_68d4bf7802511e1595883f8ee4675997'}\n\n```{.r .cell-code}\nx2_around_x1 <- 3\nx1 <- runif(n, 0, 10)\nx2 <- 2 + x1 + runif(n, -x2_around_x1, x2_around_x1)\ny <- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n\nvif(lm(y ~ x1 + x2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      x1       x2 \n4.494537 4.494537 \n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}