---
title: "ESS Exampless"
institute: "**Jam TBD**"
code-fold: false
code-line-numbers: false
---

```{r}
#| label: setup
#| include: false
set.seed(2112)
library(palmerpenguins)
peng <- penguins[complete.cases(penguins), ]
```


::: {.content-visible unless-profile="book"}
## Preamble

### Announcements

:::

## Review

### Extra Sum of Squares

From last time, we basically learned what the following means:

$$
\frac{SS(\hat\beta_{q+1}, ..., \hat\beta_p | \hat\beta_0, ... \hat\beta_q)}{(p-q)s^2} =\frac{S_1 - S(\hat\beta_0) - (S_2 - S(\hat\beta_0))}{(p-q)s^2}\sim F_{p-q, \max(p, q)}
$$
where $s^2$ is the MSE calculated from the larger model.

This allows us to do a test for whether $\beta_{q+1} = \beta_{q+2} = ... = \beta_p = 0$.


### Penguins Example

::: {.content-visible when-profile="book"}

The R code to do this test is as follows. In this code, we believe that the bill length and bill depth are strongly correlated, and thus we cannot trust the CIs that we get from `summary(lm())` (we saw "Confidence Regions" in the slides and code for L05).

:::

\scriptsize
```{r}
#| echo: true
#| label: nrow-peng
nrow(peng)
```

```{r}
#| echo: true
#| label: ess-bills
lm1 <- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,
    data = peng)
lm2 <- lm(body_mass_g ~ flipper_length_mm,
    data = peng)
anova(lm2, lm1)
```

### Where do these values come from?

Let's try and calculate these values ourselves in a couple different ways!

\scriptsize
```{r}
#| echo: true
#| label: anova-lm1
anova(lm1)
```
\normalsize

From this model, SSE is 50814912 on 329 degrees of freedom. This is the same as the SSE in the output of `anova(lm2, lm1)`.

### Calculating the SSE

\scriptsize
```{r}
#| echo: true
#| label: anova-lm2
anova(lm2)
```
\normalsize

Again, the SSE of 51211963 matches what we saw in `anova(lm2, lm1)`, and we have 331 degrees of freedom (as expected, the differences in degrees of freedom is 2).

Note that the F-value in `anova()` is just the ratio of the MSEs, but this is not the case here. Instead, we need to calculate $s^2$.

### Final Calculations

::: {.columns}
::: {.column}

$s^2$ is the MSE for the **larger** model:

\scriptsize
```{r}
#| echo: true
#| label: s2
s2 <- 50814912/329
s2
```
\normalsize

\pspace

And now we can calculate the F-value:

\scriptsize
```{r}
#| echo: true
#| label: calc-f
(51211963 - 50814912)/ (2 * s2)
```

```{r}
#| echo: true
#| label: calc-f-pval
print(1- pf(1.28539, 2, 329))
```
\normalsize

:::
::: {.column}

\scriptsize
```{r}
#| label: ess-again
#| echo: true
#| eval: true
#| code-line-numbers: false
anova(lm2, lm1)
```

:::
:::

\pspace

- Try to calculate these values based on matrix multiplication. 
    - With and without correction factors!

## ESS Algorithms

### Main Idea

- **ESS**: Test a subset of predictors for at least one significant coefficient.\lspace 
- We might want to check *all* predictors one-by-one. 
    - This is much less common than the textbook may lead you to believe

\quad

There are 3 ways to calculate the ESS for *all* predictors. They are very helpfully labelled Types I, II, and III.

### Type I: Sequential Sum-of-Squares (with interactions)

- Check $SS(\hat\beta_1|\hat\beta_0)$
- Check $SS(\hat\beta_2|\hat\beta_0, \hat\beta_1)$
- Check $SS(\hat\beta_2:\hat\beta_1|\hat\beta_0, \hat\beta_1, \hat\beta_2)$
    - $\hat\beta_2:\hat\beta_1$ is an interaction term, which means we use a formula like `y ~ x1 + x2 + x1*x2` (although we'll learn why R uses different notation than this).
- Check $SS(\hat\beta_3|\hat\beta_0, \hat\beta_1, \hat\beta_2)$
- Check all interactions between x1, x2, and x3,
- ...

\pspace

This will give us every possible sum-of-squares. This is very very dubious, and can lead to a major multiple comparisons problem!

### Type 2: Sequential Sum-of-Squares (R's Default)

- Check $SS(\hat\beta_1|\hat\beta_0)$
- Check $SS(\hat\beta_2|\hat\beta_0, \hat\beta_1)$
- Check $SS(\hat\beta_3|\hat\beta_0, \hat\beta_1, \hat\beta_2)$
- ...

\pspace

- Results in an ordered sequence of "is it worth adding x1?", "if we have x1, is it worth adding x2?", etc.\lspace
- *Only meaningful if the predictors are naturally ordered* (such as polynomial regression, see below).

### Type 3: Last-entry sum-of-squares

-   Check $SS(\hat\beta_1|\hat\beta_0, \hat\beta_2, \hat\beta_3)$
-   Check $SS(\hat\beta_2|\hat\beta_0, \hat\beta_1, \hat\beta_3)$
-   Check $SS(\hat\beta_3|\hat\beta_0, \hat\beta_1, \hat\beta_2)$

\pspace

- Whether adding predictor $x_i$ is worth it, given that all other predictors are already in the model.

### Back to Type 2 ANOVA (Sequental Sum-of-Squares)

By default, R does sequential sum-of-squares. This is a very important fact to know!

In Types I and II, *the order of the predictors matters*. In fact, you cannot make any conclusions about the significance that doesn't make reference to this fact.

### Type 2 ANOVA in R

\scriptsize
```{r}
#| echo: true
#| label: type-I
## Try changing the order to see how the significance changes!
mylm <- lm(mpg ~ qsec + disp + wt, data = mtcars)
anova(mylm)
summary(mylm)$coef # No obvious connection to anova
```

### Special Case: Polynomial Regression

$$
y_i = \beta_0 + \beta_1x_i + \beta_2x_i^2 + \beta_3x_i^3 + ... + \beta_{p-1}x_i^{p-1} + \epsilon_i
$$

\pspace

We only have one predictor $x$, but we have performed non-linear transformations (HMWK: why is it important that the transformations are non-linear?).

\pspace

What order of polynomial should we fit?

### Polynomial Regression Data Context

1. Given we have a linear model, is it worth making it quadratic?
2. Given that we have a quadratic model, is it worth making it cubic? 
3. Given that we have a cubic model...

::: {.content-visible when-profile="book"}

In the code below, I use the `I()` function (the `I` means identity) to make the polynomial model. The "formula" notation in R, `y ~ x + z`, has a lot of options. Including `x^2`, rather than `I(x^2)`, makes R think we want to do one of the more fancy things, but the `I()` tells it that we want to literally square it. In the future, we'll use a better way of doing this.

:::

```{r}
#| label: polynomial-type-1-plot
x <- runif(600, 0, 20)
y <- 2 - 3*x + 3*x^2 - 0.3*x^3 + rnorm(600, 0, 100)
plot(y ~ x)
```

### Sequential SS for Polynomial Regression

```{r}
#| label: polynomial-type-1-anova
mylm <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))
anova(mylm)
```

::: {.content-visible when-profile="book"}
From the table above, we can clearly see that this should just be a cubic model (which is the true model that we generated). Try changing things around to see if, say, it will still detect an order 5 polynomial if if there's no terms of order 3 or 4.
:::

### A note on calculations

Take a moment to consider the following. Suppose I checked the following two (Type II) ANOVA tables:

- `anova(lm(mpg ~ disp, data = mtcars))`
- `anova(lm(mpg ~ disp + wt, data = mtcars))`

Both tables will have the first row labelled "disp" and include its sum-of-squares along with the F-value. Do you expect these two rows to be exactly the same?

Think about it.

Think a little more.

What values do you expect to be used in the calculation?

Which sums-of-squares? Which variances?

Let's test it out.

### Change in ANOVA when adding predictors

\scriptsize
```{r}
#| echo: true
#| label: add-predictor-changes-anova
anova(lm(mpg ~ disp, data = mtcars))
anova(lm(mpg ~ disp + wt, data = mtcars))
```
\normalsize

They're different! 

::: {.content-visible when-profile="book"}
With both the polynomial and the `disp` example, we see that the interpretation of the anova table is highly, extremely, extraordinarily dependent on which predictors we choose to include *AND* the order in which we choose to include them. So, yeah. Be careful.
:::

### Type III SS in R

There isn't a built-in function to do this. To create this, we can either use our math (my preferred method) or test each one individually.

```{r}
#| echo: true
#| label: anova-type-3
anova(
    lm(mpg ~ disp + wt, data = mtcars),
    lm(mpg ~ disp + wt + qsec, data = mtcars)
)
anova(
    lm(mpg ~ disp + qsec, data = mtcars),
    lm(mpg ~ disp + wt + qsec, data = mtcars)
)
anova(
    lm(mpg ~ wt + qsec, data = mtcars),
    lm(mpg ~ disp + wt + qsec, data = mtcars)
)
```

## Modelling Strategies

### The Art of Modelling

- If you think to yourself "my predictors are logically ordered and I want to check for the significance of all of them one-by-one", you want Type II.
    - Type 1 error through the roof.\lspace
- If you think "they're not ordered but I want to check significance", check the overall F test for all predictors and then individual t-tests.\lspace
- If you think "what would happen if each predictor were the last one I put in the model", then you want Type III.
    - I can't think of a good for doing this - you're pretty much guaranteed to have a multiple comparisons issue.

### Choosing Predictor Sets

- These algorithms assume that you have a set of predictors that you already know you want to check.\lspace
- There are other predictors in the mtcars dataset that we did not consider!
    - We only looked at continuous predictors - we'll see categorical predictors later.

### Advice

-   Start with a lot of plots.\lspace
-   Based on the plots and your knowledge of the context, create a candidate set of predictors that you think will be the final model.\lspace
-   Check the model fit (p-values, residuals, etc).\lspace
-   Based on your knowledge of the context, check significance of groups of predictors that you think are highly correlated.\lspace
-   Your final model will be based on the tests for groups of (or individual) predictors that you suspect would be relevant.

### The Guiding Principles of Model Selection

**Minimize the number of p-values that you check.**

- ESS tests check a set of predictors, rather than each one individually.
    - Fewer p-values!
- The Type I, II, and III algorithms are sometimes important, but intuition and knowledge is better!

\pspace

**Minimize the number of degrees of freedom that you use**

- More degrees of freedom = more complexity.
    - A simpler model that explains the same amount of variance is preferred!
- Non-linear trends (e.g. polynomial models) use more degrees of freedom.
    - Polynomials are hard to interpret.

::: {.content-visible when-profile="book"}
## Exercises

Suggested textbook Ch06 exercises: E, I, J, **N, P, Q, R**, X

1. Calculate all ANOVA tables in this lesson using matrix multiplication.
2. Explain why Type II SS would not be meaningful for the model $y_i = \beta_{p-1}x_i^{p-1} + \beta_{p-2}x_i^{p-2} + ... + \beta_2x_i^2 + \beta_1x_i + \beta_0 + \epsilon_i$.
3. I performed a Type 3 SS ANOVA for the model $bodymass_i = \beta_0 + \beta_1flipperlength + \beta_2billlength$. A colleague told me that I should have included $billdepth$ in this analysis. Will the results from Type 3 SS ANOVA change when I add in this predictor?
4. Describe another situation (not polynomial regression) where Type 2 SS ANOVA would be appropriate. Simulate some relevant data and demonstrate.

<details>
<summary>**Solution Hints**</summary>
In order for Type 2 SS to be appropriate, the predictors need to have a logical order in which to enter the model. Some examples might include a case where researchers have a particular ranking for the predictors in their study, and want to stop adding predictors to the model as soon as they find the first non-significant predictor (I am still struggling to think of context for this). 
</details>
*****

5. For the penguins data, fit a model with flipper length, bill length, and bill depth (in that order), and check the output of `anova()`. Reverse the order of predictors and repeat. Which values in the ANOVA table stay the same? Which values change? Why do they change? What does this tell you about interpreting `anova()` output?
6. Give an example of a context where a collection of predictors might be tested for significance all at once.
:::

