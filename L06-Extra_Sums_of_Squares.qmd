---
title: "Extra Sum-of-Squares"
institute: "**Jam TBD**"
---

```{r}
#| include: false
set.seed(2112)
```


::: {.content-visible unless-profile="book"}
## Preamble

### Announcements

- Lab 3 on Wednesday!
:::

## Introduction

### Today's Main Idea

If you add or remove predictors, the variance of the residuals changes!

\pspace

- As always, we ask if it's a "big" change.
    - All sizes are relative to the standard error!\lspace
- Different predictors have a different effect on the residuals.\lspace
- A group of predictors can be tested for whether all parameters are 0.

\pspace\pause

Which predictors have a meaningful (significant?) effect on the residuals?

### Sum-of-Squares due to Regression

- Since SSE = SST - SSReg and SST never changes, we're focusing on SSReg.\lspace
- Recall: SSReg is the variance of the line itself!

$$
SS_{Reg} = \sum_{i=1}^n(\hat y_i - \bar y)
$$

### SSReg in two different penguin models

In the `penguins` data, we're determining which predictors are associated with body mass.

- $SS_1$ = SSReg for model 1
    - $\texttt{body\_mass\_g} = \beta_0 + \beta_1 \texttt{flipper\_length\_mm} + \beta_2 \texttt{bill\_length\_mm} + \beta_3 \texttt{bill\_depth\_mm}$
- $SS_2$ = SSReg for model 2
    - $\texttt{body\_mass\_g} = \beta_0 + \beta_1 \texttt{flipper\_length\_mm} + \beta_2 \texttt{bill\_length\_mm}$

Note: M2 is **nested** in M1; M1 has all the same predictors and then some.

\pspace

::: {.callout-important}
$\beta_1$ in the first model is **different** from $\beta_1$ in the second model.
:::

### Extra Sum-of-Squares

If M2 is **nested** within M1, \emph{e.g.}:

- M1: $\texttt{bodymass} = \beta_0 + \beta_1 \texttt{flipperlength} + \beta_2 \texttt{billlength} + \beta_3 \texttt{billdepth}$
- M2: $\texttt{bodymass} = \beta_0 + \beta_1 \texttt{flipperlength} + \beta_2 \texttt{billlength}$

Then the Extra sum of squares is defined as:
$$
SS(\hat\beta_3 | \hat\beta_0, \hat\beta_1, \hat\beta_2) = S_1 - S_2
$$
where $S_1$ is $SS_{Reg}$ for M1, similar for $S_2$.

\pspace

Convince yourself that $S_1 \ge S_2$.

### Special Case: Corrected Sum-of-Squares

We've already seen this notation:
$$
SSReg(\hat\beta_0) = n\bar{\underline y}^2
$$
and
$$
SS_{Reg}(corrected) = SS_{Reg}(\hat\beta_1, ..., \hat\beta_{p-1}|\hat\beta_0)= \hat{\underline\beta}^TX^T\underline y - n\bar{\underline y}^2
$$
which can be written as $SS_{Reg}(corrected) = S_1 - S_2$ where $S_2$ is the sum-of-squares for the null model!

### Unspecial Case: Correction doesn't matter!

Consider $S_{1c}$ and $S_{2c}$, the corrected versions of $S_1$ and $S_2$. In symbols:
$$
S_{1c} = SS_{Reg}(\hat\beta_1,\hat\beta_2,\hat\beta_3 | \hat\beta_0)\text{ and }S_{1} = SS_{Reg}(\hat\beta_0,\hat\beta_1,\hat\beta_2,\hat\beta_3)
$$
Then
\begin{align*}
S_{1c} - S_{2c} = (S_2 - n\bar{\underline y}^2) - (S_1 - n\bar{\underline y}^2) = S_1 - S_2
\end{align*}

In other words, the correction term doesn't matter. 

This is useful because R outputs the corrected versions.

### Unspecial Case: SSReg versus SSE doesn't matter!

Consider $SSE_1$ and $SSE_2$. Since $SST$ is the same for both models,

\begin{align*}
SSE_2 - SSE_1 = (SST - S_1) - (SST - S_2) = S_2 - S_1
\end{align*}

Notice that the order is switched - bigger variance in the line means smaller variance in the residuals!!!

### ANOVA Tests for ESS

Consider the models:

- M1: $\texttt{bodymass} = \beta_0 + \beta_1 \texttt{flipperlength} + \beta_2 \texttt{billlength} + \beta_3 \texttt{billdepth}$
    - $df_1 = 4$
- M2: $\texttt{bodymass} = \beta_0 + \beta_1 \texttt{flipperlength} + \beta_2 \texttt{billlength}$
    - $df_2 = 3$

\pspace

If we choose $H_0: \beta_3 = 0$ in model 1, then
$$
\frac{S_1 - S_2}{(4 - 3)s^2} \sim F_{1,\nu}
$$

where $s^2$ is the error variance (MSE) in the larger model with degress of freedom $\nu = df_1$.

This is almost identical to the F-test for only one predictor (with one important difference).

### In General

If M1 has $p$ df, M2 has $q$ df, and one is nested in the other, then $\nu = \max(p, q)$ and

$$
\frac{S_1 - S_2}{(p - q)s^2} \sim F_{|p-q|,\nu}
$$

\pspace

Note that it doesn't matter which is nested: $S_1 - S_2$ has the same sign as $p-q$, so the F-statistic always positive.

### The Big Idea: Omnibus Tests for Multiple Predictors

Suppose we want to test if *any* bill measurement is useful. 

- Bill length and depth are highly correlated - marginal CIs won't be valid.
- Confidence Regions are hard (and only work in 2D)

::: {.content-visible when-profile="book"}
To provide a little bit more context, we're going to consider the actual covariance of the predictors! Recall that the variance covariance matrix for $\hat{\underline\beta}$ (i.e. the variance of the joint sampling distribution) is defined as $V(\hat{\underline\beta}) = \sigma^2(X^TX)^{-1}$.

```{r}
#| label: var-covar-bills
library(palmerpenguins)
peng <- penguins[complete.cases(penguins),]

mylm <- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,
    data = peng)

X <- model.matrix(mylm)
s2 <- summary(mylm)$sigma^2

var_beta <- s2 * solve(t(X) %*% X)
var_beta
```

We can see that there's a lot of covariance going on in our data~ We'll focus on bill length and bill depth for now. The code below is not testable.

```{r}
#| label: penguins-contour
library(mvtnorm)

beta_length <- coef(mylm)["bill_length_mm"]
beta_depth <- coef(mylm)["bill_depth_mm"]
se_length <- summary(mylm)$coef["bill_length_mm", "Std. Error"]
se_depth <- summary(mylm)$coef["bill_depth_mm", "Std. Error"]
var_bills <- var_beta[3:4, 3:4]
x <- seq(beta_length - 2 * se_length, beta_length + 2*se_length,
    length.out = 100)
y <- seq(beta_depth - 2 * se_depth, beta_depth + 2 * se_depth,
    length.out = 100)
z <- outer(x, y, function(xi, yi) dmvnorm(cbind(xi, yi), mean = c(beta_length, beta_depth),
    sigma = var_bills))
contour(x, y, z, main = "Joint Normal Distribution of Bill Measurement Coefficients",
    xlab = "Coefficient for bill length",
    ylab = "Coefficient for bill depth")

confints <- confint(mylm)
abline(v = confints["bill_length_mm",], col = 2)
abline(h = confints["bill_depth_mm",], col = 2)
points(x = -6, y = -4, pch = 16, cex = 2, col = 2)
```

The plot above shows the estimated sampling distribution for the Bill Length and Bill Depth coefficients (this is similar to the normal distribution that we use for the sample mean, but it's in two dimensions now and we have to deal with the correlation of the coefficients). Since the coefficients are correlated, we get an elliptical region.

I've also added the "marginal" confidence intervals; that is, the confidence intervals you get if you only consider the coefficient for bill depth. There's a red dot at (-6, -4), which represents the joint hypothesis $\beta_3 = -6$ and $\beta_4 = 4$ (that is, the hypothesis that both of these are true at the same time). If we simply test the hypothesis that $\beta_3 = -6$, we are well within the confidence limits. If we just test the hypothesis that $\beta_4 = -4$, we're also within those confidence limits. However, the contour plot above shows that this is actually *not* a reasonable value - it's far in the tail of the joint distribution! The marginal confidence intervals don't tell the whole story.

:::

\pspace

Instead, we can use the ESS to test for a subset of predictors!

- M1: $\texttt{bodymass} = \beta_0 + \beta_1 \texttt{flipperlength} + \beta_2 \texttt{billlength} + \beta_3 \texttt{billdepth}$
- M2: $\texttt{bodymass} = \beta_0 + \beta_1 \texttt{flipperlength}$

$S_1 = S_2$ is equivalent to $\beta_2 = \beta_3 = 0$, and it accounts for their covariance!

\pspace

If significant, then at least one of $(\beta_2, \beta_3)$ is not 0.

### ESS In R

```{r}
#| eval: true
#| echo: true
library(palmerpenguins)
peng <- penguins[complete.cases(penguins),]
m1 <- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,
    data = peng)
m2 <- lm(body_mass_g ~ flipper_length_mm,
    data = peng)
anova(m1, m2) |> knitr::kable()
```

::: {.content-visible unless-profile="book"}
## Participation Questions

### Q1

In the following table, `bill_length_mm` anf `bill_depth_mm` both have non-significant p-values. This means we should drop both of them from the model.

```{r}
#| echo: false
summary(m1)$coef |> knitr::kable()
```

\pspace

a. TRUE\lspace
b. FALSE

<!--- B --->

### Q2

If M1 is nested within M2, then

a. All the predictors in M1 are contained in M2 (with M2 having extra)\lspace
b. All the predictors in M2 are contained in M1 (with M1 having extra)\lspace
c. M1 and M2 both contain the same predictors.\lspace
d. The M1 bird has built a nest within M2. It's going to lay eggs and start a family!

<!--- A --->

### Q3

Extra Sum of Squares refers to:

\pspace

a. The aditional variation in the residuals that we get from adding predictors\lspace
b. The additional variation in the regression line that we get from adding predictors.\lspace
c. The additional variation in the sampling distribution that we get from adding predictors. \lspace
d. Extra calculcations that we didn't actually need to do.
 
<!--- B --->

### Q4

We can only use ESS if one model is nested within another.

\pspace

a. TRUE\lspace
b. FALSE

<!--- A --->

### Q5

An ANOVA test for overall regression is equivalent to an ESS for a full model versus a model with just an intercept.

\pspace

a. TRUE\lspace
b. FALSE

<!--- A --->

### Q5

The following table shows the output of `mpg ~ hp + wt`.

```{r}
#| label: mtcars-Q
summary(lm(mpg ~ hp + wt, data = mtcars))$coef
```

If we were to instead fit `mpg ~ hp + wt + disp`, what would the coefficient of `wt` be?

\pspace

a. Approximately -3.87
b. Lower than -3.87
c. Higher than -3.87
d. We cannot know without fitting the model!

<!--- D --->

:::

## Examples

### mtcars

::: {.columns}
::: {.column}
- `disp`: Engine size (displacement)
- `hp`: Horse power
- `drat`: Rear axel ratio
    - How much the axel must turn
- `wt`: Weight
- `qsec`: Quarter mile time (seconds)
:::
::: {.column}

```{r}
#| label: mtcars-1
M1 <- lm(mpg ~ disp + hp + drat + wt + qsec,
    data = mtcars)
```
:::
:::

Some potential questions:

1. Is the car's "power" important for the regression?
2. Is the car's "size" important?
3. Are `hp` and `wt` enough to model the mpg of a car?

::: {.content-visible when-profile="book"}
Solutions:

```{r}
#| label: mtcar-a1

anova(M1, update(M1, ~ . - hp - qsec - drat)) # At least one is important!
anova(M1, update(M1, ~ . - wt - disp)) # At least one is important!
anova(M1, lm(mpg ~ hp + wt, data = mtcars)) # None are important!
```

Of course, this raises more questions: If "at least one is important", then which is it? 

We're slowly getting into the "art" territory of regression. The choice of which predictors to include is never going to have a right answer. If someone is trying to decide how heavy of a car to make, they probably want `wt` in the model no matter what. If the Ministry of Transportation wants to predict the mpg based on things that are easy to measure, we might choose the predictors that are easiest to measure.

We should *not* just check a whole bunch of p-values! A recurring and important theme in this class is that Type 1 error is a monster! The whole point of ESS (including overall F-tests) is that we can check a bunch of things all at once *without* having to calculate a bunch of p-values!
:::

### Next time

- When to check ESS\lspace
- How to check all ESS\lspace
- What is R's `anova()` function even doing???

::: {.content-visible when-profile="book"}
## Exercises

Suggested textbook exercises: **A**, D.

1. Explain why $SS_{Reg}$ is always larger when predictors are added. (That is, if $M1$ is nested within $M2$, then $SS_{Reg}$ for $M2$ will be larger).
2. Simulate a data set with 10 predictors, where only 5 of them have a non-zero coefficient. Show that an ESS test can detect this, even if some of the parameters with a 0 coefficient have a significant p-value (Type 1 error).
3. Demonstrate that $\hat\beta_1$ in the model $y_i = \beta_0 + \beta_1$ is different $y_i = \beta_0 + \beta_1 + \beta_2$. Demonstrate this across any data we've seen before, new data, simulated data, or whatever you want!
4. Why does the estimate of one parameter change when you add other estimates? (*Hint: see the bill length and bill depth example here in the course notes.*)

:::

