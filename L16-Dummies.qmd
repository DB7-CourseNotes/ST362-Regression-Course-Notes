---
title: "L15: Dummy Variables"
institute: "Jam: ** by "
---


## Preamble

### Announcements


## 0/1 Predictors

### Dummy Coding

"Dummy" variables are just predictors that only take the values 0 and 1.

\pspace

- 0 pairs of glasses versus 1 pair of glasses
    - This is a count that can only be 0 or 1\lspace
- 0 means automatic, 1 means manual
    - Arbitrary choice of 0/1\lspace
- 0 means off, 1 means on
    - Natural choice of 0/1, but still arbitrary

### Slopes with a Dummy Variable

Usual interpretation: as $x$ increases by 1, $y$ increases by $\beta$. 

\pspace

This doesn't go away, but we get a new interpretation!

- $x$ can only increase by one (from 0 to 1).
    - $\beta$ is the *difference in groups*.

\pspace

Note that we assume constant variance; this means the variance is the same in both groups

- *Exact* same assumptions as a t-test.
    - (Different from a "Welch" t-test)

### Categorical Variables

Consider the `cyl` column in mtcars. We *could* code three dummy variables:

- $I(cyl == 4)$
- $I(cyl == 6)$
- $I(cyl == 8)$

This would lead to the model:
$$
y = \beta_0 + \beta_1I(cyl == 4) + \beta_2I(cyl == 6) + \beta_3I(cyl == 8)
$$

What is the intercept here? 

### Categorical Variable Dummy Coding

Instead, we set one as a **reference** variable and let the intercept "absorb" it:

- $I(cyl == 6)$
- $I(cyl == 8)$

This would lead to the model:
$$
y = \beta_0 + \beta_1I(cyl == 6) + \beta_2I(cyl == 8)
$$
where

- $\beta_0$ is the mean of mpg when cyl = 4.
- $\beta_1$ is the difference in mean of mpg between 4 and 6 cylinder cars.
- $\beta_2$ is the difference in means for 4 versus 8.
    - Difference btwn 6 and 8 can be found with some cleverness.

### Models with Categorical Variables

The model $y = \beta_0 + \beta_1I(cyl == 6) + \beta_2I(cyl == 8)$ is equivalent to:
$$
y_i = \begin{cases}\beta_0 & \text{if }\; cyl == 4\\ \beta_0 + \beta_1 & \text{if }\; cyl == 6\\\beta_0  + \beta_2 & \text{if }\; cyl == 8\end{cases}
$$

This is equivalent to fitting an intercept-only model $y = \beta_0$ for subsets of the data.

\pspace

By putting them in the same model, we can easily test for significance. 

## Interactions

### Different Intercepts, Same Slope

If we have `cyl` and `disp` in the model, we get the following:

$$
y = \beta_0 + \beta_1I(cyl == 6) + \beta_2I(cyl == 8) + \beta_3 disp
$$
which is equivalent to:
$$
y_i = \begin{cases}\beta_0  + \beta_3 disp& \text{if }\; cyl == 4\\ \beta_0 + \beta_1  + \beta_3 disp& \text{if }\; cyl == 6\\\beta_0  + \beta_2  + \beta_3 disp& \text{if }\; cyl == 8\end{cases}
$$
This is three different models of mpg versus disp, but with a different intercept depending on the value of `cyl`.

### Interaction Terms: Different Intercepts, Different Slopes

We can expand the model above with an **interaction term**.
$$
y = \beta_0 + \beta_1I(6) + \beta_2I(8) + \beta_3 disp + \beta_4I(6)disp + \beta_5I(8)disp
$$
where $I(6)$ is just shorthand for $I(cyl == 6)$.

This is the same as:
$$
y_i = \begin{cases}\beta_0  + \beta_3 disp& \text{if }\; cyl == 4\\ (\beta_0 + \beta_1)  + (\beta_3 + \beta_4) disp& \text{if }\; cyl == 6\\(\beta_0  + \beta_2)  + (\beta_3 + \beta_5) disp& \text{if }\; cyl == 8\end{cases}
$$
In this case, we might as well fit 3 completely different models!

(Except we can test for significance!)

### ANCOVA

If `g` is a categorical variable, then:

- `lm(y ~ g)` is a t-test if `g` is binary
- `lm(y ~ g)` is an ANOVA if `g` has more than 2 categories
- `lm(y ~ x * g)` is an **ANCOVA** model
    - Analysis of **Covariance**.

\pspace

Main idea: Is the covariance (or correlation) between $x$ and $y$ different for different categories of $g$?

- Only a small extension to ANOVA
    - ANOVA: means
    - ANCOVA: covariances

### Beyond $x$ and $g$

- In the simple cases, we're doing t-test, ANOVA, or ANCOVA.\lspace
- Beyond this, we're just doing regression, no special names.
    - "Controlling for" is a term we'll use later.\lspace
- Choosing interaction terms is *hard*.
    - `ggplot2` makes parts of it a lot easier.

## Participation Questions

### Q1

1.
2.

## Significance of a Group

### Output of `summary.lm()`

The output compares each slope to 0.

\pspace

- For a categorical predictor, this tests if a category is different *from the reference*. 

