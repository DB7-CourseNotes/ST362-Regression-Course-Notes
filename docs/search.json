[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "docs",
    "section": "",
    "text": "Introduction\nThis repo contains the course notes for the Spring 2023 offering of ST362: Regression Analysis.\nThis is very much a work in progress."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "L01-Introduction.html",
    "href": "L01-Introduction.html",
    "title": "1  L01: Introduction and Distributions",
    "section": "",
    "text": "2 Preamble"
  },
  {
    "objectID": "L01-Introduction.html#announcements",
    "href": "L01-Introduction.html#announcements",
    "title": "1  L01: Introduction and Distributions",
    "section": "2.1 Announcements",
    "text": "2.1 Announcements\n\n“That’s my Jam!”\nNo Lab this week\nA1 released on Weds"
  },
  {
    "objectID": "L01-Introduction.html#agenda",
    "href": "L01-Introduction.html#agenda",
    "title": "1  L01: Introduction and Distributions",
    "section": "2.2 Agenda",
    "text": "2.2 Agenda\n\nIntroducing the course\nIntroduction to the course"
  },
  {
    "objectID": "L01-Introduction.html#read-the-syllabus",
    "href": "L01-Introduction.html#read-the-syllabus",
    "title": "1  L01: Introduction and Distributions",
    "section": "3.1 Read the Syllabus",
    "text": "3.1 Read the Syllabus\n\n{Source: Ph.D. Comics by Jorge Cham}"
  },
  {
    "objectID": "L01-Introduction.html#the-midterm",
    "href": "L01-Introduction.html#the-midterm",
    "title": "1  L01: Introduction and Distributions",
    "section": "3.2 The Midterm",
    "text": "3.2 The Midterm\n\nIn-Class\nOpen Book? (As in, you’re allowed one book of handwritten notes.)\n\nYes: Harder, will require good notes.\nNo: Easier, will require memory."
  },
  {
    "objectID": "L01-Introduction.html#todays-learning-outcomes",
    "href": "L01-Introduction.html#todays-learning-outcomes",
    "title": "1  L01: Introduction and Distributions",
    "section": "4.1 Today’s Learning Outcomes",
    "text": "4.1 Today’s Learning Outcomes\n\nImportant facts about distributions.\nCIs and t-tests."
  },
  {
    "objectID": "L01-Introduction.html#normal",
    "href": "L01-Introduction.html#normal",
    "title": "1  L01: Introduction and Distributions",
    "section": "5.1 Normal",
    "text": "5.1 Normal\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{(x-\\mu)^2}{-2\\sigma^2}\\right);\\;-\\infty&lt;x&lt;\\infty\n\\]\nShiny: https://shiny.abdn.ac.uk/Stats/apps/app_normal/#section-theory\n\nCompletely determined by \\(\\mu\\) and \\(\\sigma\\).\nIf \\(X \\sim N(\\mu,\\sigma^2)\\), then \\(Z=\\frac{X-\\mu}{\\sigma} \\sim N(0,1)\\).\n\nOften called “standardizing”\n\nYou won’t need to memorize the pdf, but it’s useful!\n\n\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pnorm\")"
  },
  {
    "objectID": "L01-Introduction.html#t",
    "href": "L01-Introduction.html#t",
    "title": "1  L01: Introduction and Distributions",
    "section": "5.2 \\(t\\)",
    "text": "5.2 \\(t\\)"
  },
  {
    "objectID": "L01-Introduction.html#but-first-gamma",
    "href": "L01-Introduction.html#but-first-gamma",
    "title": "1  L01: Introduction and Distributions",
    "section": "5.3 But first, Gamma!",
    "text": "5.3 But first, Gamma!\n\n\n\\(t\\) is based on the Gamma (\\(\\Gamma\\)) function:\n\\[\n\\Gamma(q) = \\int_0^\\infty e^{-t}t^{q-1}dt\n\\]\n\nInteresting property: \\(\\Gamma(k+1) = k!\\) for integer \\(k\\).\n\nIn general, \\(\\Gamma(q) = (q-1)\\Gamma(q-1)\\)\n\nAlso, \\(\\Gamma(1/2) = \\pi^{1/2}\\)\n\n\\(\\Gamma(3/2) = (3/2-1)\\gamma(1/2) = \\sqrt{\\pi}/2\\)"
  },
  {
    "objectID": "L01-Introduction.html#the-t-distribution",
    "href": "L01-Introduction.html#the-t-distribution",
    "title": "1  L01: Introduction and Distributions",
    "section": "5.4 The \\(t\\) Distribution",
    "text": "5.4 The \\(t\\) Distribution\n\\[\nf(x; \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu + 1}{2}\\right)}{\\sqrt{\\nu\\pi}\\Gamma(\\nu/2)}\\left(1 + \\frac{x^2}{\\nu}\\right)^{-(\\nu + 1)/2}\n\\]\n\nThe notation \\(f(x; \\nu)\\) means “function of \\(x\\), given a value of \\(\\nu\\).”\nCompletely determined by \\(\\nu\\)\nAs \\(\\nu\\rightarrow\\infty\\), this becomes \\(N(0,1)\\).\n\n\\(\\nu&gt;30\\) is pretty much normal already.\nFor \\(\\nu&lt;\\infty\\), \\(t\\) has wider tails than normal."
  },
  {
    "objectID": "L01-Introduction.html#the-chi2-distribution---variances",
    "href": "L01-Introduction.html#the-chi2-distribution---variances",
    "title": "1  L01: Introduction and Distributions",
    "section": "5.5 The \\(\\chi^2\\) distribution - variances",
    "text": "5.5 The \\(\\chi^2\\) distribution - variances\nIf \\(Z_1, Z_2, ..., Z_k\\) are iid \\(N(0,1)\\), then \\(\\chi^2_k = \\sum_{i=1}^kZ_i^2\\) has a chi-square distribution on \\(k\\) degrees of freedom.\n\\[\nf(x; k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2}\n\\]\n\nIf \\(X_i\\sim N(\\mu_i\\sigma_i)\\), then we can just standardize each first.\nAs \\(k\\rightarrow\\infty\\), \\((\\chi^2_k-k)/\\sqrt{2k}\\stackrel{d}{\\rightarrow} N(0,1)\\).\n\nThat is, for large \\(k\\) this can be approximated by a normal distribution.\n\nVery related to the variance\n\n\\((n-1)s^2/\\sigma^2\\) follows a \\(\\chi^2_n\\) distribution."
  },
  {
    "objectID": "L01-Introduction.html#the-f-distribution---ratio-of-variances",
    "href": "L01-Introduction.html#the-f-distribution---ratio-of-variances",
    "title": "1  L01: Introduction and Distributions",
    "section": "5.6 The \\(F\\) distribution - ratio of variances",
    "text": "5.6 The \\(F\\) distribution - ratio of variances\nIf \\(S_1\\) and \\(S_2\\) are independent \\(\\chi^2\\) distributions with degrees of freedom \\(\\nu_1\\) and \\(\\nu_2\\), then \\(\\frac{S_1/\\nu_1}{S_2/\\nu_2}\\) follows an \\(F_{\\nu_1,\\nu_2}\\) distribution.\n\\[\nf(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}\n\\]\n\nIf \\(s_1^2\\) and \\(s_2^2\\) are the sample-based estimates of the true values \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\), then \\(\\frac{s_1^2/\\sigma_1^2}{s_2^2/\\sigma_2^2}\\) follows an \\(F_{\\nu_1, \\nu_2}\\) distribution.\n\nUnder \\(H_0\\), \\(\\sigma_1=\\sigma_2\\), so we don’t need the true values.\nNote: \\(s_1^2\\) is calculated from \\(S_1^2/\\nu_1\\)."
  },
  {
    "objectID": "L01-Introduction.html#general-idea-terminology",
    "href": "L01-Introduction.html#general-idea-terminology",
    "title": "1  L01: Introduction and Distributions",
    "section": "6.1 General Idea: Terminology",
    "text": "6.1 General Idea: Terminology\nConsider the statistic \\(t = \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)}\\) (a statistic is any number that can be calculated from data alone).\n\n\\(\\theta\\) is the “““true”“” value of the parameter.\n\nUnknown, unknowable, not used in formula - hypothesize \\(\\theta = \\theta_0\\) instead.\n\n\\(\\hat\\theta\\) is our estimator of \\(\\theta\\).\n\nEstimator is a function, like \\(\\hat\\mu = \\bar X =(1/n)\\sum_{i=1}^nX_i\\).\n\n\\(X\\) is a random variable.\n\nEstimate is a number, like the calculated mean of a sample.\n\n\\(se(\\hat\\theta)\\) is the standard error of \\(\\hat\\theta\\).\n\nIf we had a different sample, the value of \\(\\hat\\theta\\) would be different. It has variance.\nStandard error: the standard deviation of the sampling distribution."
  },
  {
    "objectID": "L01-Introduction.html#general-idea-distributional-assumptions",
    "href": "L01-Introduction.html#general-idea-distributional-assumptions",
    "title": "1  L01: Introduction and Distributions",
    "section": "6.2 General Idea: Distributional Assumptions",
    "text": "6.2 General Idea: Distributional Assumptions\nConsider the quantity (not statistic) \\(t = \\frac{\\hat\\theta - \\theta}{se(\\hat\\theta)}\\).\nIf we assume \\(X\\sim N(\\mu, \\sigma^2)\\) and \\(\\hat\\theta = \\bar X\\) is an unbiased estimate of \\(\\theta\\) on \\(\\nu=n-1\\) degrees of freedom, then \\(t \\sim t_\\nu\\)\nFrom this, we can find the lower and upper \\(\\alpha/2\\)% of the \\(t\\)$ curve.\n\n\\(t_\\nu(\\alpha/2)\\) is the lower \\(\\alpha/2\\)%.\n\ni.e., if \\(\\alpha = 0.11\\), then it’s \\(t(\\nu, 0.055)\\), the lower 5.5% area.\n\n\\(t_\\nu(1 - \\alpha/2)\\) is the upper \\(\\alpha/2\\)%.\nSince \\(t\\) is symmetric, \\(t_\\nu(1 - \\alpha/2) = -t_\\nu(\\alpha/2)\\).\n\n\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pvalues\")"
  },
  {
    "objectID": "L01-Introduction.html#general-idea-distribution-to-quantiles",
    "href": "L01-Introduction.html#general-idea-distribution-to-quantiles",
    "title": "1  L01: Introduction and Distributions",
    "section": "6.3 General Idea: Distribution to Quantiles",
    "text": "6.3 General Idea: Distribution to Quantiles\nUnder the null hypothesis, \\(\\theta = \\theta_0\\), so \\[\nt = \\frac{\\hat\\theta - \\theta}{se(\\hat\\theta)} = \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)} \\sim t_\\nu\n\\]\nWe know that \\(\\bar X\\) is a random variable, and we want everything in between its 5.5% and 94.5% quantiles. \n\nThis is a confidence interval!"
  },
  {
    "objectID": "L01-Introduction.html#general-idea-distribution-to-ci",
    "href": "L01-Introduction.html#general-idea-distribution-to-ci",
    "title": "1  L01: Introduction and Distributions",
    "section": "6.4 General Idea: Distribution to CI",
    "text": "6.4 General Idea: Distribution to CI\nWe can do this easily for the \\(t_\\nu\\) distribution: we want all values \\(t_0\\) such that\n\\[\\begin{align*}\nt_\\nu(5.5) &\\le t_0 \\le t_\\nu(94.5)\\\\\nt_\\nu(5.5) &\\le \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)} \\le t_\\nu(94.5)\\\\\nse(\\hat\\theta)t_\\nu(5.5) &\\le \\hat\\theta - \\theta_0 \\le se(\\hat\\theta)t_\\nu(94.5)\\\\\n\\hat\\theta + se(\\hat\\theta)t_\\nu(5.5) &\\le \\theta_0 \\le \\hat\\theta + se(\\hat\\theta)t_\\nu(94.5)\n\\end{align*}\\]\nThe CI is all values \\(\\theta_0\\) that would not be rejected by the null hypothesis \\(\\theta = \\theta_0\\) at the \\(\\alpha\\)% level.\nSince \\(t_\\nu(1 - \\alpha/2) = -t_\\nu(\\alpha/2)\\), our 89% CI is \\(\\hat\\theta \\pm se(\\hat\\theta)t_\\nu(5.5)\\)."
  },
  {
    "objectID": "L01-Introduction.html#what-is-the-standard-error",
    "href": "L01-Introduction.html#what-is-the-standard-error",
    "title": "1  L01: Introduction and Distributions",
    "section": "6.5 What is the Standard Error?",
    "text": "6.5 What is the Standard Error?\nIf we’re estimating the mean, \\(\\hat\\theta = (1/n)\\sum_{i=1}^nX_i\\), where we assume \\(X_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma)\\). \\[\nE(\\hat\\theta) = E\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n}\\sum_{i=1}^nE(X_i) = \\frac{n\\mu}{n} = \\mu\n\\] From this, we see that the mean is an unbiased estimator! (This is nice, but not required.)\n\\[\nV(\\hat\\theta) = V\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n^2}V\\left(\\sum_{i=1}^nX_i\\right) \\stackrel{indep}{=} \\frac{1}{n^2}\\sum_{i=1}^nV(X_i) = \\sigma^2/n \\stackrel{plug-in}{=} s^2/n\n\\] where \\(s^2\\) is the estimate variance since we cannot know the true mean. Note that \\(s^2\\) is a biased estimator for \\(\\sigma\\)."
  },
  {
    "objectID": "L01-Introduction.html#ci-for-variance",
    "href": "L01-Introduction.html#ci-for-variance",
    "title": "1  L01: Introduction and Distributions",
    "section": "6.6 CI for Variance",
    "text": "6.6 CI for Variance\nFrom before: \\(\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_n\\).\nLet \\(\\chi^2_n(0.055)\\) be the lower 5.5% quantile, \\(\\chi^2_n(0.945)\\) be the upper.\nFor homework, find the CI from: \\[\n\\chi^2_n(0.055) \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi^2_n(0.945)\n\\] Note that \\(\\chi^2_n(0.055)\\ne-\\chi^2_n(0.945)\\)."
  },
  {
    "objectID": "L01-Introduction.html#summary",
    "href": "L01-Introduction.html#summary",
    "title": "1  L01: Introduction and Distributions",
    "section": "6.7 Summary",
    "text": "6.7 Summary\n\nDistributions exist and are important\n\nMost things will be normal, which leads to \\(t\\), \\(\\chi^2\\), and \\(F\\).\n\nCIs are all values that would not be rejected by a hypothesis test.\n\nThe null hypothesis determines the distribution of the test statistic, which allows us to find the CI.\n\n\nFor Next Class:\n\nRead through Ch01, especially procedure for least squares estimation.\n\n&lt;—"
  },
  {
    "objectID": "L01-Introduction.html#q1",
    "href": "L01-Introduction.html#q1",
    "title": "1  L01: Introduction and Distributions",
    "section": "7.1 Q1",
    "text": "7.1 Q1\nWhich of the following is a Normal distribution?\n\n\\(f(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}\\)\n\\(f(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-x^2/2\\right)\\)\n\\(f(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}\\)\n\\(f(x; k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2}\\)"
  },
  {
    "objectID": "L01-Introduction.html#q2",
    "href": "L01-Introduction.html#q2",
    "title": "1  L01: Introduction and Distributions",
    "section": "7.2 Q2",
    "text": "7.2 Q2\nIf you know \\(\\mu\\) and \\(\\sigma\\), then you know the exact shape of the normal distribution.\n\nTrue\nFalse"
  },
  {
    "objectID": "L01-Introduction.html#q3",
    "href": "L01-Introduction.html#q3",
    "title": "1  L01: Introduction and Distributions",
    "section": "7.3 Q3",
    "text": "7.3 Q3\nA confidence interval for \\(\\theta\\) contains all values \\(\\theta_0\\) that would not be rejected by a hypothesis test (assume that both are at the same significance level).\n\nTrue\nFalse"
  },
  {
    "objectID": "L01-Introduction.html#q4",
    "href": "L01-Introduction.html#q4",
    "title": "1  L01: Introduction and Distributions",
    "section": "7.4 Q4",
    "text": "7.4 Q4\nWhich of the following is the correct value for \\(E(\\hat\\theta)\\), where \\(\\hat\\theta = \\sum_{i=1}^n\\left(a + bX_i\\right)\\) and \\(E(X_i)=\\mu\\) for all \\(i\\)?\n\n\n\\(a + b\\mu\\)\n\\(b\\mu\\)\n\\(na + nb\\mu\\)"
  },
  {
    "objectID": "L01-Introduction.html#q5",
    "href": "L01-Introduction.html#q5",
    "title": "1  L01: Introduction and Distributions",
    "section": "7.5 Q5",
    "text": "7.5 Q5\nWhich of the following is the definition of an estimator?\n\nA value calculated from data.\nA function that returns the estimate for a parameter.\nAny function of the data.\nA person who estimates."
  },
  {
    "objectID": "L01-Introduction.html#q6",
    "href": "L01-Introduction.html#q6",
    "title": "1  L01: Introduction and Distributions",
    "section": "7.6 Q6",
    "text": "7.6 Q6\nThe general approach to finding confidence intervals is to find a function of the statistic and the parameter it’s estimating that follows a known distribution and then solve for the unknown parameter.\n\n\nFalse\nTrue\n\n\n—&gt;"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html",
    "href": "L02-Fitting_Straight_Lines.html",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "",
    "text": "3 Preamble"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#announcements",
    "href": "L02-Fitting_Straight_Lines.html#announcements",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "3.1 Announcements",
    "text": "3.1 Announcements\n\nDo the worksheet for the labs!\nA1 is out - get started soon!"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#agenda",
    "href": "L02-Fitting_Straight_Lines.html#agenda",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "3.2 Agenda",
    "text": "3.2 Agenda\n\nWhy fit models?\nWhat fit models?\nHow fit models?"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#the-quote.",
    "href": "L02-Fitting_Straight_Lines.html#the-quote.",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "4.1 The Quote.",
    "text": "4.1 The Quote.\n“All models are wrong, some are useful.” - George Box"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#all-models-are-wrong-but-some-are-useful",
    "href": "L02-Fitting_Straight_Lines.html#all-models-are-wrong-but-some-are-useful",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "4.2 All Models Are Wrong, But Some Are Useful",
    "text": "4.2 All Models Are Wrong, But Some Are Useful\nModel: A mathematical equation that explains something about the world.\n\nGravity: 9.8 \\(m/s^2\\) right?\n\nThis is a model - it’s a relationship that explains something in the world.\nVaries across the surface of the Earth.\nVaries according to air resistance.\n\nEvery additional cigarette decreases your lifespan by 11 minutes.\n\nVery, very much wrong.\nVery, very useful for communication."
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#all-linear-models-are-wrong",
    "href": "L02-Fitting_Straight_Lines.html#all-linear-models-are-wrong",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "4.3 All Linear Models are Wrong",
    "text": "4.3 All Linear Models are Wrong\n\n\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(palmerpenguins)\npenguins &lt;- penguins[complete.cases(penguins),]\n\nggplot(penguins) +\n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(title = \"A line fits reasonably well\",\n        x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\")\n\n\n\n\n\n\nggplot(mtcars) +\n    aes(x = disp, y = mpg) + \n    geom_point(size = 2) + \n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) + \n    geom_smooth(method = \"lm\", se = FALSE, formula = \"y~poly(x, 2)\", colour = 2) +\n    labs(title = \"A straight line misses the pattern\",\n        subtitle = \"A polynomial might fit better\",\n        x = \"Engine Displacement (1000 cu.in)\", \n        y = \"Fuel Efficiency (mpg)\")"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#some-linear-models-are-useful",
    "href": "L02-Fitting_Straight_Lines.html#some-linear-models-are-useful",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "4.4 Some Linear Models are Useful",
    "text": "4.4 Some Linear Models are Useful\n\nxpred &lt;- 200\nypred &lt;- predict(\n    lm(body_mass_g ~ flipper_length_mm, data = penguins),\n        newdata = data.frame(flipper_length_mm = xpred))\n\nggplot(penguins) +\n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(title = \"The height of the line is an okay prediction for Body Mass\",\n        x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\") +\n    annotate(geom = \"point\", shape = \"*\", size = 30, colour = 2,\n        x = xpred, \n        y = ypred) + \n    annotate(geom = \"segment\", colour = 2, size = 2, \n        linetype = \"dashed\",\n        x = xpred, xend = xpred, \n        yend = -Inf, y = ypred) + \n    annotate(geom = \"segment\", colour = 2, size = 2, \n        linetype = \"dashed\",\n        x = -Inf, xend = xpred, \n        yend = ypred, y = ypred)"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#a-model-is-an-equation",
    "href": "L02-Fitting_Straight_Lines.html#a-model-is-an-equation",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "4.5 A Model is an Equation",
    "text": "4.5 A Model is an Equation\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\Leftrightarrow Y = X\\underline\\beta + \\underline\\epsilon \\Leftrightarrow E(Y|X) = X\\underline \\beta;\\; V(Y|X)=\\sigma^2\n\\]\n\nFor a one unit increase in \\(x_i\\), \\(y_i\\) increases by \\(\\beta_1\\).\n\nIf our model fits well and this is statistically significant, we can apply this to the population.\n\nThe estimated value of \\(y_i\\) when \\(x_i=0\\) is \\(\\beta_0\\).\n\nNot always interesting, but usually\\(^*\\) necessary.\n\n\nOur prediction for \\(y_i\\) at any given value of \\(x_i\\) is calculated as: \\[\n\\hat y_i = \\hat\\beta_0 + \\hat \\beta_1x_i\n\\] where the “hats” mean we’ve found estimates for the \\(\\beta\\) values."
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#aside-my-notation-differs-from-the-text",
    "href": "L02-Fitting_Straight_Lines.html#aside-my-notation-differs-from-the-text",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "4.6 Aside: My notation differs from the text",
    "text": "4.6 Aside: My notation differs from the text\nI will make mistakes, but in general:\n\n\\(Y\\) is a random variable (usually a vector) representing the response.\n\n\\(Y_i\\) is also a random variable (not a vector)\n\n\\(\\underline y\\) is the response vector; the observed values of \\(Y\\)\n\\(\\underline x\\) is the vector of covariates in simple linear regression\n\\(X\\) is a matrix of covariates\n\nNot a random variable!!! Capital letters could be either random or matrix (or both). I will specify when necessary.\nI will avoid the notation \\(X_i\\).\nWhen necessary, the first column is all ones so that \\(X\\underline\\beta=\\beta_0 + \\beta_1\\underline x_1 + \\beta_1\\underline x_2 + ...\\).\n\nContext should make this clear."
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#the-mean-of-y-at-any-value-of-x",
    "href": "L02-Fitting_Straight_Lines.html#the-mean-of-y-at-any-value-of-x",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "4.7 The Mean of \\(Y\\) at any value of \\(X\\)",
    "text": "4.7 The Mean of \\(Y\\) at any value of \\(X\\)\n\\[\nE(Y|X) = X\\underline\\beta\n\\]"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#a-linear-model-is-linear-in-the-parameters",
    "href": "L02-Fitting_Straight_Lines.html#a-linear-model-is-linear-in-the-parameters",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "4.8 A Linear Model is Linear in the Parameters",
    "text": "4.8 A Linear Model is Linear in the Parameters\nThe following are linear:\n\n\\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1\\sin\\left(x_i^{x_i}\\right) + \\beta_2x_i^{\\sin(x_i)} + \\epsilon_i\\)\n\nThe following are not linear:\n\n\\(y_i = \\beta_0 + \\beta_1^2x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_0\\beta_1x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\sin(\\beta_1)x_i +\\epsilon_i\\)"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#goal-find-beta-values-which-minimize-the-error",
    "href": "L02-Fitting_Straight_Lines.html#goal-find-beta-values-which-minimize-the-error",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "5.1 Goal: Find \\(\\beta\\) values which minimize the error",
    "text": "5.1 Goal: Find \\(\\beta\\) values which minimize the error\nModel: \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\)\nError: \\(\\hat\\epsilon_i = y_i - \\hat y_i\\)\nWhy is this a bad way to do it?"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#least-squares",
    "href": "L02-Fitting_Straight_Lines.html#least-squares",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "5.2 Least Squares",
    "text": "5.2 Least Squares\nGoal: Minimize \\(\\sum_{i=1}^n\\hat\\epsilon_i^2=\\sum_{i=1}^n(y_i - \\hat y_i)^2\\), the sum of squared errors.\nThis ensures errors don’t cancel out. It also penalizes large errors more.\nCould we have used \\(|\\epsilon_i|\\), or some other function? \\(|ly|\\)!"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#least-squares-estimates",
    "href": "L02-Fitting_Straight_Lines.html#least-squares-estimates",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "5.3 Least Squares Estimates",
    "text": "5.3 Least Squares Estimates\nI assume that you can do the Least Squares estimate in your sleep (be ready for tests).\nThe final results are: \\[\\begin{align*}\n\\hat\\beta_0 &= \\bar y - \\hat\\beta_1\\bar x\\\\\n\\hat\\beta_1 &= \\frac{\\sum_{i-1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i-1}^n(x_i - \\bar x)^2}\\stackrel{def}{=}\\frac{S_{XY}}{S_{XX}}\n\\end{align*}\\]\nThe textbook was written in 1998 and gives formulas to make the calculation easier to do on pocket calculators. You will not need a pocket calculator for this course."
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#lets-add-assumptions",
    "href": "L02-Fitting_Straight_Lines.html#lets-add-assumptions",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "5.4 Let’s Add Assumptions",
    "text": "5.4 Let’s Add Assumptions\nAssumptions allow great things, but only when they’re correct!\n\n\\(E(\\epsilon_i) = 0\\), \\(V(\\epsilon_i) = \\sigma^2\\).\n\\(cov(\\epsilon_i, \\epsilon_j) =0\\) when \\(i\\ne j\\).\n\nImplies that \\(E(y_i) = \\beta_0 + \\beta_1x_i\\) and \\(V(y_i) = \\sigma^2\\).\n\n\\(\\epsilon_i\\stackrel{iid}{\\sim} N(0,\\sigma^2)\\)\n\nThis is a strong assumption, but often works!\n\n\nInterpretation: the model looks like a line with completely random errors. (“Completely random” doesn’t mean “without structure”!)"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#mean-of-hatbeta_1",
    "href": "L02-Fitting_Straight_Lines.html#mean-of-hatbeta_1",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "5.5 Mean of \\(\\hat\\beta_1\\)",
    "text": "5.5 Mean of \\(\\hat\\beta_1\\)\nIt is easy to show that \\[\n\\hat\\beta_1 = \\frac{\\sum_{i-1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i-1}^n(x_i - \\bar x)^2} = \\frac{\\sum_{i-1}^n(x_i - \\bar x)y_i}{\\sum_{i-1}^n(x_i - \\bar x)^2}\n\\] since \\(\\sum(x_i - \\bar x) = 0\\).\nHomework: show that \\(E(\\hat\\beta_1) = \\beta_1\\) (Hint: use the fact that \\(E(y_i) = \\beta_0 + \\beta_1x_i\\) and the “pocket calculator” expansions)."
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#variance-of-hatbeta_1",
    "href": "L02-Fitting_Straight_Lines.html#variance-of-hatbeta_1",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "5.6 Variance of \\(\\hat\\beta_1\\)",
    "text": "5.6 Variance of \\(\\hat\\beta_1\\)\n\\[\n\\hat\\beta_1 = \\frac{\\sum_{i-1}^n(x_i - \\bar x)y_i}{\\sum_{i-1}^n(x_i - \\bar x)^2}\n\\] can be re-written as \\[\n\\hat\\beta_1 = \\sum_{i=1}^na_iy_i\\text{, where }a_i = \\frac{x_i - \\bar x}{\\sum_{i=1}^n(x_i - \\bar x)^2}\n\\]\nThus the variance of \\(\\hat\\beta_1\\) is \\[\nV(\\hat\\beta_1) = \\sum_{i=1}^na_i^2V(y_i) = ... = \\frac{\\sigma^2}{S_{XX}} \\stackrel{plug-in}{=} = \\frac{s^2}{S_{XX}}\n\\]"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#confidence-interval-and-test-statistic-for-hatbeta_1",
    "href": "L02-Fitting_Straight_Lines.html#confidence-interval-and-test-statistic-for-hatbeta_1",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "5.7 Confidence Interval and Test Statistic for \\(\\hat\\beta_1\\)",
    "text": "5.7 Confidence Interval and Test Statistic for \\(\\hat\\beta_1\\)\nThe test statistic can be found as: \\[\nt = \\frac{\\hat\\beta_1 - \\beta_1}{se(\\hat\\beta_1)} = \\frac{(\\hat\\beta_1 - \\beta_1)\\sqrt{S_{XX}}}{s} \\sim t_\\nu\n\\]\nSince this follows a \\(t\\) distribution, we can get the CI: \\[\n\\hat\\beta_1 \\pm t_\\nu(\\alpha/2)\\sqrt{\\frac{s^2}{S_{XX}}}\n\\]"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#q1",
    "href": "L02-Fitting_Straight_Lines.html#q1",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "6.1 Q1",
    "text": "6.1 Q1\nAll models are useful, but some are wrong.\n\nTrue\nFalse"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#q2",
    "href": "L02-Fitting_Straight_Lines.html#q2",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "6.2 Q2",
    "text": "6.2 Q2\nFor a one unit increase in \\(x\\), \\(y\\) increases by\n\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(\\beta_1x\\)\n\\(\\beta_0 + \\beta_1x\\)"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#q3",
    "href": "L02-Fitting_Straight_Lines.html#q3",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "6.3 Q3",
    "text": "6.3 Q3\nThe standard error of \\(\\beta_1\\) refers to:\n\nThe variance of the population slope\nThe amount by which the slope might be off\nThe variance of the estimated slope across different samples\nA big chicken. Not, like, worryingly big, but big enough that you’d be like, “Wow, that’s a big chicken!”"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#q4",
    "href": "L02-Fitting_Straight_Lines.html#q4",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "6.4 Q4",
    "text": "6.4 Q4\nWhich is not an assumption that we usually make in linear regression?\n\n\\(V(\\epsilon_i) = \\sigma^2\\)\n\\(E(\\epsilon_i) = 0\\)\n\\(E(X) = 0\\)\n\\(\\epsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\)"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#q5",
    "href": "L02-Fitting_Straight_Lines.html#q5",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "6.5 Q5",
    "text": "6.5 Q5\nWhich of the following is not a linear model?\n\n\\(y_i = \\beta_0 + \\beta_1^2x_i + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_0x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1\\sin\\left(x_i^{x_i}\\right) + \\beta_2x_i^{\\sin(x_i)} + \\epsilon_i\\)"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#q6",
    "href": "L02-Fitting_Straight_Lines.html#q6",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "6.6 Q6",
    "text": "6.6 Q6\nThe sum of squared errors is the best way to estimate the model parameters.\n\nTrue\nFalse"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#statistics-is-the-study-of-variance",
    "href": "L02-Fitting_Straight_Lines.html#statistics-is-the-study-of-variance",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "7.1 Statistics is the Study of Variance",
    "text": "7.1 Statistics is the Study of Variance\n\nGiven a data set with variable \\(\\underline y\\), \\(V(\\underline y) = \\sigma^2_y\\).\n\nThis is just the variance of a single variable.\n\nOnce we’ve incorporated the linear relationship with \\(\\underline x\\), \\(V(\\hat\\beta \\underline x + \\underline\\epsilon)=0+V(\\underline\\epsilon) = \\sigma^2\\)\n\nMathematically, \\(\\sigma^2 \\le \\sigma_y^2\\).\n\n\nThe variance in \\(Y\\) is explained by \\(X\\)!"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#a-useful-identity-and-its-interpretations",
    "href": "L02-Fitting_Straight_Lines.html#a-useful-identity-and-its-interpretations",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "7.2 A Useful Identity, and its Interpretations",
    "text": "7.2 A Useful Identity, and its Interpretations\n\\[\\begin{align*}\n\\hat\\epsilon_i &= y_i - \\hat y_i \\\\&= y_i - \\bar y - (\\hat y_i - \\bar y)\\\\\n\\implies \\sum_{i=1}^n\\hat\\epsilon_i^2 &= \\sum_{i=1}^n(y_i-\\bar y)^2 - \\sum_{i=1}^n(\\hat y_i - \\bar y)^2\n\\end{align*}\\] where we’ve simply added and subtracted \\(\\bar y\\). The final line skips a few steps (try them yourself)!\nThe last line is often written as: \\(SS_E = SS_T - SS_{Reg}\\)"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#sums-of-squares",
    "href": "L02-Fitting_Straight_Lines.html#sums-of-squares",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "7.3 Sums of Squares",
    "text": "7.3 Sums of Squares\n\\[\nSS_E = SS_T - SS_{Reg}\n\\]\n\n\\(SS_E\\): Sum of Squared Errors\n\\(SS_T\\): Sum of Squares Total (i.e., without \\(\\underline x\\))\n\\(SS_{Reg}\\): Sum of Squares due to the regression.\n\nIt’s the variance of the line (calculated at observed \\(\\underline x\\) values) around the mean of \\(\\underline y\\)???\n\nThis is incredibly useful, but weird.\n\nI use \\(SS_{Reg}\\) instead of \\(SS_R\\) because some textbooks use \\(SS_R\\) as the Sume of Squared Residuals, which is confusing."
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#aside-degrees-of-freedom",
    "href": "L02-Fitting_Straight_Lines.html#aside-degrees-of-freedom",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "7.4 Aside: Degrees of Freedom",
    "text": "7.4 Aside: Degrees of Freedom\nDef: The number of “pieces of information” from \\(y_1, y_2, ..., y_n\\) to construct a new number.\n\nIf I have \\(x = (1,3,2,1,3,???)\\) and I know that \\(\\bar x = 2\\), I can recover the missing piece.\n\nThe mean “uses” (accounts for) one degree of freedom\n\nIf I have \\(x = (1,2,3,1,???,???)\\) and I know \\(\\bar x = 2\\) and \\(s_x^2=1\\), I can recover the two missing pieces.\n\nThe variance accounts for two degrees of freedom.\n\nOne \\(df\\) is required to compute it.\n\n\n\nEstimating one parameter takes away one degree of freedom for the rest!\n\nCan find \\(\\bar x\\) when \\(x = (1)\\), but can’t find \\(s_x^2\\) because there aren’t enough \\(df\\)!"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#sums-of-squares-1",
    "href": "L02-Fitting_Straight_Lines.html#sums-of-squares-1",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "7.5 Sums of Squares",
    "text": "7.5 Sums of Squares\n\n\n\n\n\n\n\n\n\nSource of Variation\nDegress of Freedom \\(df\\)\nSum of Squares (\\(SS\\))\nMean Square (\\(MS\\))\n\n\n\n\nRegression\n1\n\\(\\sum_{i=1}^n(\\hat y_i - \\bar y)^2\\)\n\\(MS_{Reg}\\)\n\n\nError\n\\(n-2\\)\n\\(\\sum_{i=1}^n(y_i - \\hat y_i)^2\\)\n\\(MS_E=s^2\\)\n\n\nTotal\n\\(n-1\\)\n\\(\\sum_{i=1}^n(y_i - \\bar y)^2\\)\n\\(s_y^2\\)\n\n\n\n\nNotice that \\(SS_T = SS_{Reg} + SS_E\\), which is also true for the \\(df\\) (but not \\(MS\\)). \nWhy is \\(df_E = n-2\\)? What two parameters have we estimated?\n\n\\(df_{Reg}\\) is trickier to explain. It suffices to know that \\(df_{Reg} = df_T-df_E\\)."
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#using-sumsmeans-of-squares",
    "href": "L02-Fitting_Straight_Lines.html#using-sumsmeans-of-squares",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "7.6 Using Sums/Means of Squares",
    "text": "7.6 Using Sums/Means of Squares\n\nIf \\(\\hat y_i = \\bar y\\) for all \\(i\\), then we have a horizontal line!\n\nThat is, there is no relationship between \\(\\underline x\\) and $\\underline \\(y\\).\nIn this case, \\(SS_{reg} = \\sum_{i=1}^n(\\hat y_i - \\bar y)^2 = 0\\).\n\n\nOkay, so, just test for \\(SS_{Reg} = 0\\)?\nBut how??? We need some measure of how far from 0 is statistically significant!!!\nRecall that \\(SS_E = \\sum_{i=1}^n(y_i - \\hat y_i)^2\\).\n\nWe can compare the variation around the line to the variation of the line.\n\nThis is \\(MS_{Reg}/MS_E\\), and it follows an \\(F\\) distribution!!"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#the-f-test-for-significance-of-regression",
    "href": "L02-Fitting_Straight_Lines.html#the-f-test-for-significance-of-regression",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "7.7 The F-test for Significance of Regression",
    "text": "7.7 The F-test for Significance of Regression\n\\[\nF_{df_{Reg}, df_E} = \\dfrac{MS_{Reg}}{MS_E} = \\dfrac{MS_{Reg}}{s^2}\n\\]\n\nRecall that \\(SS_{Reg} = SS_E + SS_T &gt; SS_E\\), but the \\(df\\) make a difference.\nFor homework, show that \\(E(MS_{Reg}) = \\sigma^2 + \\beta_1^2\\sum_{i=1}^n(x_i-\\bar x)^2\\)\nThis implies that \\(E(MS_{Reg}) &gt; E(MS_E) = \\sigma^2\\)."
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#exercise-a-from-chapter-3-ch03-exercises-cover-ch1-3",
    "href": "L02-Fitting_Straight_Lines.html#exercise-a-from-chapter-3-ch03-exercises-cover-ch1-3",
    "title": "2  Ch1: Straight Line Fitting",
    "section": "7.8 Exercise A from Chapter 3 (Ch03 Exercises cover Ch1-3)",
    "text": "7.8 Exercise A from Chapter 3 (Ch03 Exercises cover Ch1-3)\n\n\nA study was made on the effect of temperature on the yield of a chemical process. The data are shown to the right.\n\nAssuming \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\), what are the least squares estimates of \\(\\beta_0\\) and \\(\\beta_1\\)?\nConstruct the analysis of variance table and test the hypothesis \\(H_0: \\beta_1=0\\) at the 0.05 level.\nWhat are the confidence limits (at = 0.05) for \\(\\beta_1\\)?\n\n\n\nlibrary(aprean3) # Data from textbook\nhead(dse03a) # Data Set Exercise Ch03A\n\n   x  y\n1 -5  1\n2 -4  5\n3 -3  4\n4 -2  7\n5 -1 10\n6  0  8\n\nnrow(dse03a)\n\n[1] 11\n\n\n\n\n\nGood textbook questions: A, K, O, P, T, U, X (using data(anscombe) in R), Z, AA, CC. Note that all data sets in the textbook are available in an R package found here."
  },
  {
    "objectID": "L02-OLS_Estimates.html",
    "href": "L02-OLS_Estimates.html",
    "title": "3  Lecture 2: OLS Estimates",
    "section": "",
    "text": "The code in this notebook demonstrates what we’ve seen in class.\n\nset.seed(2112)\nn &lt;- 30\nsigma &lt;- 1\nbeta0 &lt;- 1\nbeta1 &lt;- 5\nx &lt;- runif(n, 0, 10)\n\ndgp &lt;- function(x, beta0 = 1, beta1 = 5, \n                sigma = 1) {\n    epsilon &lt;- rnorm(length(x), mean = 0, sd = sigma)\n    y &lt;- beta0 + beta1 * x\n    return(data.frame(x = x, y = y + epsilon))\n}\n\nset.seed(2112)\nmydata &lt;- dgp(x = x, beta0 = beta0, beta1 = beta1,\n    sigma = sigma)\n\nplot(mydata)\n\n\n\n\nLet’s find the estimate for \\(\\beta\\) as well as the variance.\n\nSdotdot &lt;- function(x, y) sum( (x - mean(x)) * (y - mean(y)) )\n\nSxx &lt;- Sdotdot(mydata$x, mydata$x)\nSxy &lt;- Sdotdot(mydata$x, mydata$y)\nSyy &lt;- Sdotdot(mydata$y, mydata$y)\n\nb1 &lt;- Sxy / Sxx\nb1\n\n[1] 4.925759\n\nb0 &lt;- mean(mydata$y) - b1 * mean(mydata$x)\nb0\n\n[1] 1.528286\n\nerrors &lt;- mydata$y - (b0 + b1 * mydata$x)\ns &lt;- sd(errors)\nb1_var &lt;- s^2 / Sxx\nb1_var\n\n[1] 0.005382233\n\n\nOur estimates of \\(\\beta_0\\) and \\(\\beta_1\\) are very close to the truth!\nNow, let’s generate new data a bunch of times and see if our estimate of the variance of \\(\\hat\\beta_1\\) is accurate. This is not a proof, just a demonstration.\nIn the code below, I’m also keeping track of \\(\\hat\\beta_0\\) from each sample. This will be useful later.\n\nR &lt;- 1000\nbeta1s &lt;- rep(NA, R)\nbeta0s &lt;- rep(NA, R)\nfor (i in 1:R) {\n    new_data &lt;- dgp(x, beta0 = beta0, beta1 = beta1, sigma = sigma)\n    Sxx &lt;- Sdotdot(new_data$x, new_data$x)\n    Sxy &lt;- Sdotdot(new_data$x, new_data$y)\n    beta1s[i] &lt;- Sxy / Sxx\n    beta0s[i] &lt;- mean(new_data$y) - b1 * mean(new_data$x)\n}\n\nvar(beta1s)\n\n[1] 0.005407588\n\n\nIn this example, we know the data generating process (dgp), so these randomly generated values are samples from the population.\nAs we know, the standard error is the variance of the estimator over all possible samples from the population. We only took 1000, but that’s probably close enough to infinity to draw the sampling distribution.\n\nhist(beta1s)\nabline(v = 5, col = 2, lwd = 10) # true value of beta\n\n\n\n\nAs you can see, \\(\\hat\\beta_1\\) is unbiased and has some variance (at this stage, there’s nothing to compare this variance to so we can’t really call it “large” or “small”).\nLet’s use this to calculate an 89% Confidence Interval!\n\n# Empirical Ci\nquantile(beta1s, c(0.055, 0.945))\n\n    5.5%    94.5% \n4.883143 5.120293 \n\n# Theoretical CI\nSxx &lt;- Sdotdot(mydata$x, mydata$x)\n5 + c(-1, 1) * qt(0.945, df = nrow(mydata) - 2) * 1/sqrt(Sxx)\n\n[1] 4.882763 5.117237\n\n# Sample CI\nb1 + c(-1, 1) * qt(0.945, nrow(mydata) - 2) * s/sqrt(Sxx)\n\n[1] 4.804666 5.046851\n\n\nThe empirical and the theoretical CIs line up pretty well! However, the sample CI is fundamentally different. The sample CI is centered at the estimated value, and we expect it to contain the true population value 89% of the time!\nWe practically never know the actual DGP, so this is just a demonstration that the math works.\nNote that we treated \\(\\underline x\\) as if it were fixed. The value \\(S_{XX}\\) will be different for different \\(\\underline x\\), and we don’t make any assumptions about what distribution \\(\\underline x\\) follows.\n\n4 Analysis of Variance\nThe code below demonstrates how the ANOVA tables are calculated.\n\ny &lt;- mydata$y\nyhat &lt;- b0 + b1*mydata$x\nybar &lt;- mean(y)\n\nc(sum((yhat - ybar)^2), sum((y - yhat)^2), sum((y - ybar)^2))\n\n[1] 4809.33809   30.93852 4840.27661\n\nANOVA &lt;- data.frame(\n    Source = c(\"Regression\", \"Error\", \"Total (cor.)\"),\n    df = c(1, nrow(mydata) - 2, nrow(mydata)),\n    SS = c(sum((yhat - ybar)^2), sum((y - yhat)^2), sum((y - ybar)^2))\n)\nANOVA$MS &lt;- ANOVA$SS / ANOVA$df\nANOVA\n\n        Source df         SS          MS\n1   Regression  1 4809.33809 4809.338089\n2        Error 28   30.93852    1.104947\n3 Total (cor.) 30 4840.27661  161.342554\n\n\nThis is equivalent to what R’s built-in functions do!\n\nanova(lm(y ~ x, data = mydata))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 4809.3  4809.3  4352.5 &lt; 2.2e-16 ***\nResiduals 28   30.9     1.1                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n5 Dependence and Centering\nSomething not touched on in class is that \\(cov(\\hat\\beta_0, \\hat\\beta_1)\\) is not 0! This should be clear from the formula got \\(\\hat\\beta_0\\), which is \\(\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\\).\nThe code below repeats what we did before, but with higher variance to better demonstrate the problem.\nIt also records the estimates based on centering \\(\\underline x\\). Notice how the formula for \\(\\hat\\beta_0\\) is no longer dependent on \\(\\hat\\beta_1\\) if the mean of \\(\\underline x\\) is 0!\n\nb1s &lt;- double(R)\nb0s &lt;- double(R)\nb1cs &lt;- double(R)\nb0cs &lt;- double(R)\nn &lt;- 25\nx &lt;- runif(n, 0, 10)\nxc &lt;- x - mean(x) # centered\n\nfor (i in 1:R) {\n    y &lt;- 2 - 2 * x + rnorm(25, 0, 4)\n    b1 &lt;- Sdotdot(x, y) / Sdotdot(x, x)\n    b0 &lt;- mean(y) - b1 * mean(x)\n    b1s[i] &lt;- b1\n    b0s[i] &lt;- b0\n\n    # Centered\n    y &lt;- 2 - 2 * xc + rnorm(25, 0, 4)\n    b1c &lt;- Sdotdot(xc, y) / Sdotdot(xc, xc)\n    b1cs[i] &lt;- b1c\n    b0c &lt;- mean(y) - b1 * mean(xc)\n    b0cs[i] &lt;- b0c\n}\n\npar(mfrow = c(1,2))\nplot(b1s, b0s)\nplot(b1cs, b0cs)"
  },
  {
    "objectID": "L01-Introduction.html#introduction",
    "href": "L01-Introduction.html#introduction",
    "title": "1  L01: Distributions",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\n\n1.1.1 Today’s Learning Outcomes\n\nImportant facts about distributions.\nCIs and t-tests."
  },
  {
    "objectID": "L01-Introduction.html#introduction-1",
    "href": "L01-Introduction.html#introduction-1",
    "title": "1  L01: Distributions",
    "section": "1.2 Introduction",
    "text": "1.2 Introduction\n\n1.2.1 Today’s Learning Outcomes\n\nImportant facts about distributions.\nCIs and t-tests."
  },
  {
    "objectID": "L01-Introduction.html#distributions",
    "href": "L01-Introduction.html#distributions",
    "title": "1  L01: Distributions",
    "section": "1.2 Distributions",
    "text": "1.2 Distributions\n\n1.2.1 Normal\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{(x-\\mu)^2}{-2\\sigma^2}\\right);\\;-\\infty&lt;x&lt;\\infty\n\\]\nShiny: https://shiny.abdn.ac.uk/Stats/apps/app_normal/#section-theory\n\nCompletely determined by \\(\\mu\\) and \\(\\sigma\\).\nIf \\(X \\sim N(\\mu,\\sigma^2)\\), then \\(Z=\\frac{X-\\mu}{\\sigma} \\sim N(0,1)\\).\n\nOften called “standardizing”\n\nYou won’t need to memorize the pdf, but it’s useful!\n\n\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pnorm\")\n\n\n\n\n1.2.2 \\(t\\)\n\n\n1.2.3 But first, Gamma!\n\n\n\\(t\\) is based on the Gamma (\\(\\Gamma\\)) function:\n\\[\n\\Gamma(q) = \\int_0^\\infty e^{-t}t^{q-1}dt\n\\]\n\nInteresting property: \\(\\Gamma(k+1) = k!\\) for integer \\(k\\).\n\nIn general, \\(\\Gamma(q) = (q-1)\\Gamma(q-1)\\)\n\nAlso, \\(\\Gamma(1/2) = \\pi^{1/2}\\)\n\n\\(\\Gamma(3/2) = (3/2-1)\\gamma(1/2) = \\sqrt{\\pi}/2\\)\n\n\n\n\n\n\n\n\n1.2.4 The \\(t\\) Distribution\n\\[\nf(x; \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu + 1}{2}\\right)}{\\sqrt{\\nu\\pi}\\Gamma(\\nu/2)}\\left(1 + \\frac{x^2}{\\nu}\\right)^{-(\\nu + 1)/2}\n\\]\n\nThe notation \\(f(x; \\nu)\\) means “function of \\(x\\), given a value of \\(\\nu\\).”\nCompletely determined by \\(\\nu\\)\nAs \\(\\nu\\rightarrow\\infty\\), this becomes \\(N(0,1)\\).\n\n\\(\\nu&gt;30\\) is pretty much normal already.\nFor \\(\\nu&lt;\\infty\\), \\(t\\) has wider tails than normal.\n\n\n\n\n1.2.5 The \\(\\chi^2\\) distribution - variances\nIf \\(Z_1, Z_2, ..., Z_k\\) are iid \\(N(0,1)\\), then \\(\\chi^2_k = \\sum_{i=1}^kZ_i^2\\) has a chi-square distribution on \\(k\\) degrees of freedom.\n\\[\nf(x; k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2}\n\\]\n\nIf \\(X_i\\sim N(\\mu_i\\sigma_i)\\), then we can just standardize each first.\nAs \\(k\\rightarrow\\infty\\), \\((\\chi^2_k-k)/\\sqrt{2k}\\stackrel{d}{\\rightarrow} N(0,1)\\).\n\nThat is, for large \\(k\\) this can be approximated by a normal distribution.\n\nVery related to the variance\n\n\\((n-1)s^2/\\sigma^2\\) follows a \\(\\chi^2_n\\) distribution.\n\n\n\n\n1.2.6 The \\(F\\) distribution - ratio of variances\nIf \\(S_1\\) and \\(S_2\\) are independent \\(\\chi^2\\) distributions with degrees of freedom \\(\\nu_1\\) and \\(\\nu_2\\), then \\(\\frac{S_1/\\nu_1}{S_2/\\nu_2}\\) follows an \\(F_{\\nu_1,\\nu_2}\\) distribution.\n\\[\nf(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}\n\\]\n\nIf \\(s_1^2\\) and \\(s_2^2\\) are the sample-based estimates of the true values \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\), then \\(\\frac{s_1^2/\\sigma_1^2}{s_2^2/\\sigma_2^2}\\) follows an \\(F_{\\nu_1, \\nu_2}\\) distribution.\n\nUnder \\(H_0\\), \\(\\sigma_1=\\sigma_2\\), so we don’t need the true values.\nNote: \\(s_1^2\\) is calculated from \\(S_1^2/\\nu_1\\)."
  },
  {
    "objectID": "L01-Introduction.html#confidence-intervals",
    "href": "L01-Introduction.html#confidence-intervals",
    "title": "1  L01: Distributions",
    "section": "1.3 Confidence Intervals",
    "text": "1.3 Confidence Intervals\n\n1.3.1 General Idea: Terminology\nConsider the statistic \\(t = \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)}\\) (a statistic is any number that can be calculated from data alone).\n\n\\(\\theta\\) is the “““true”“” value of the parameter.\n\nUnknown, unknowable, not used in formula - hypothesize \\(\\theta = \\theta_0\\) instead.\n\n\\(\\hat\\theta\\) is our estimator of \\(\\theta\\).\n\nEstimator is a function, like \\(\\hat\\mu = \\bar X =(1/n)\\sum_{i=1}^nX_i\\).\n\n\\(X\\) is a random variable.\n\nEstimate is a number, like the calculated mean of a sample.\n\n\\(se(\\hat\\theta)\\) is the standard error of \\(\\hat\\theta\\).\n\nIf we had a different sample, the value of \\(\\hat\\theta\\) would be different. It has variance.\nStandard error: the standard deviation of the sampling distribution.\n\n\n\n\n1.3.2 General Idea: Distributional Assumptions\nConsider the quantity (not statistic) \\(t = \\frac{\\hat\\theta - \\theta}{se(\\hat\\theta)}\\).\nIf we assume \\(X\\sim N(\\mu, \\sigma^2)\\) and \\(\\hat\\theta = \\bar X\\) is an unbiased estimate of \\(\\theta\\) on \\(\\nu=n-1\\) degrees of freedom, then \\(t \\sim t_\\nu\\)\nFrom this, we can find the lower and upper \\(\\alpha/2\\)% of the \\(t\\)$ curve.\n\n\\(t_\\nu(\\alpha/2)\\) is the lower \\(\\alpha/2\\)%.\n\ni.e., if \\(\\alpha = 0.11\\), then it’s \\(t(\\nu, 0.055)\\), the lower 5.5% area.\n\n\\(t_\\nu(1 - \\alpha/2)\\) is the upper \\(\\alpha/2\\)%.\nSince \\(t\\) is symmetric, \\(t_\\nu(1 - \\alpha/2) = -t_\\nu(\\alpha/2)\\).\n\n\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pvalues\")\n\n\n\n\n1.3.3 General Idea: Distribution to Quantiles\nUnder the null hypothesis, \\(\\theta = \\theta_0\\), so \\[\nt = \\frac{\\hat\\theta - \\theta}{se(\\hat\\theta)} = \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)} \\sim t_\\nu\n\\]\nWe know that \\(\\bar X\\) is a random variable, and we want everything in between its 5.5% and 94.5% quantiles. \n\nThis is a confidence interval!\n\n\n\n1.3.4 General Idea: Distribution to CI\nWe can do this easily for the \\(t_\\nu\\) distribution: we want all values \\(t_0\\) such that\n\\[\\begin{align*}\nt_\\nu(5.5) &\\le t_0 \\le t_\\nu(94.5)\\\\\nt_\\nu(5.5) &\\le \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)} \\le t_\\nu(94.5)\\\\\nse(\\hat\\theta)t_\\nu(5.5) &\\le \\hat\\theta - \\theta_0 \\le se(\\hat\\theta)t_\\nu(94.5)\\\\\n\\hat\\theta + se(\\hat\\theta)t_\\nu(5.5) &\\le \\theta_0 \\le \\hat\\theta + se(\\hat\\theta)t_\\nu(94.5)\n\\end{align*}\\]\nThe CI is all values \\(\\theta_0\\) that would not be rejected by the null hypothesis \\(\\theta = \\theta_0\\) at the \\(\\alpha\\)% level.\nSince \\(t_\\nu(1 - \\alpha/2) = -t_\\nu(\\alpha/2)\\), our 89% CI is \\(\\hat\\theta \\pm se(\\hat\\theta)t_\\nu(5.5)\\).\n\n\n1.3.5 What is the Standard Error?\nIf we’re estimating the mean, \\(\\hat\\theta = (1/n)\\sum_{i=1}^nX_i\\), where we assume \\(X_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma)\\). \\[\nE(\\hat\\theta) = E\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n}\\sum_{i=1}^nE(X_i) = \\frac{n\\mu}{n} = \\mu\n\\] From this, we see that the mean is an unbiased estimator! (This is nice, but not required.)\n\\[\nV(\\hat\\theta) = V\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n^2}V\\left(\\sum_{i=1}^nX_i\\right) \\stackrel{indep}{=} \\frac{1}{n^2}\\sum_{i=1}^nV(X_i) = \\sigma^2/n \\stackrel{plug-in}{=} s^2/n\n\\] where \\(s^2\\) is the estimate variance since we cannot know the true mean. Note that \\(s^2\\) is a biased estimator for \\(\\sigma\\).\n\n\n1.3.6 CI for Variance\nFrom before: \\(\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_n\\).\nLet \\(\\chi^2_n(0.055)\\) be the lower 5.5% quantile, \\(\\chi^2_n(0.945)\\) be the upper.\nFor homework, find the CI from: \\[\n\\chi^2_n(0.055) \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi^2_n(0.945)\n\\] Note that \\(\\chi^2_n(0.055)\\ne-\\chi^2_n(0.945)\\).\n\n\n1.3.7 Summary\n\nDistributions exist and are important\n\nMost things will be normal, which leads to \\(t\\), \\(\\chi^2\\), and \\(F\\).\n\nCIs are all values that would not be rejected by a hypothesis test.\n\nThe null hypothesis determines the distribution of the test statistic, which allows us to find the CI.\n\n\nFor Next Class:\n\nRead through Ch01, especially procedure for least squares estimation.\n\n&lt;—"
  },
  {
    "objectID": "L01-Introduction.html#participation-questions",
    "href": "L01-Introduction.html#participation-questions",
    "title": "1  L01: Distributions",
    "section": "1.4 Participation Questions",
    "text": "1.4 Participation Questions\n\n1.4.1 Q1\nWhich of the following is a Normal distribution?\n\n\\(f(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}\\)\n\\(f(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-x^2/2\\right)\\)\n\\(f(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}\\)\n\\(f(x; k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2}\\)\n\n\n\n1.4.2 Q2\nIf you know \\(\\mu\\) and \\(\\sigma\\), then you know the exact shape of the normal distribution.\n\nTrue\nFalse\n\n\n\n1.4.3 Q3\nA confidence interval for \\(\\theta\\) contains all values \\(\\theta_0\\) that would not be rejected by a hypothesis test (assume that both are at the same significance level).\n\nTrue\nFalse\n\n\n\n1.4.4 Q4\nWhich of the following is the correct value for \\(E(\\hat\\theta)\\), where \\(\\hat\\theta = \\sum_{i=1}^n\\left(a + bX_i\\right)\\) and \\(E(X_i)=\\mu\\) for all \\(i\\)?\n\n\n\\(a + b\\mu\\)\n\\(b\\mu\\)\n\\(na + nb\\mu\\)\n\n\n\n1.4.5 Q5\nWhich of the following is the definition of an estimator?\n\nA value calculated from data.\nA function that returns the estimate for a parameter.\nAny function of the data.\nA person who estimates.\n\n\n\n1.4.6 Q6\nThe general approach to finding confidence intervals is to find a function of the statistic and the parameter it’s estimating that follows a known distribution and then solve for the unknown parameter.\n\n\nFalse\nTrue\n\n\n—&gt;"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#preamble",
    "href": "L02-Fitting_Straight_Lines.html#preamble",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.1 Preamble",
    "text": "2.1 Preamble\n\n2.1.1 Announcements\n\nDo the worksheet for the labs!\nA1 is out - get started soon!\n\n\n\n2.1.2 Agenda\n\nWhy fit models?\nWhat fit models?\nHow fit models?"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#why-fit-models",
    "href": "L02-Fitting_Straight_Lines.html#why-fit-models",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.1 Why Fit Models?",
    "text": "2.1 Why Fit Models?\n\nThe Quote.\n“All models are wrong, some are useful.” - George Box\n\n\nAll Models Are Wrong, But Some Are Useful\nModel: A mathematical equation that explains something about the world.\n\nGravity: 9.8 \\(m/s^2\\) right?\n\nThis is a model - it’s a relationship that explains something in the world.\nVaries across the surface of the Earth.\nVaries according to air resistance.\n\nEvery additional cigarette decreases your lifespan by 11 minutes.\n\nVery, very much wrong.\nVery, very useful for communication.\n\n\n\n\nAll Linear Models are Wrong\n\n\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(palmerpenguins)\npenguins &lt;- penguins[complete.cases(penguins),]\n\nggplot(penguins) +\n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(title = \"A line fits reasonably well\",\n        x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\")\n\n\n\n\n\n\nggplot(mtcars) +\n    aes(x = disp, y = mpg) + \n    geom_point(size = 2) + \n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) + \n    geom_smooth(method = \"lm\", se = FALSE, formula = \"y~poly(x, 2)\", colour = 2) +\n    labs(title = \"A straight line misses the pattern\",\n        subtitle = \"A polynomial might fit better\",\n        x = \"Engine Displacement (1000 cu.in)\", \n        y = \"Fuel Efficiency (mpg)\")\n\n\n\n\n\n\n\n\nSome Linear Models are Useful\n\nxpred &lt;- 200\nypred &lt;- predict(\n    lm(body_mass_g ~ flipper_length_mm, data = penguins),\n        newdata = data.frame(flipper_length_mm = xpred))\n\nggplot(penguins) +\n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(title = \"The height of the line is an okay prediction for Body Mass\",\n        x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\") +\n    annotate(geom = \"point\", shape = \"*\", size = 30, colour = 2,\n        x = xpred, \n        y = ypred) + \n    annotate(geom = \"segment\", colour = 2, size = 2, \n        linetype = \"dashed\",\n        x = xpred, xend = xpred, \n        yend = -Inf, y = ypred) + \n    annotate(geom = \"segment\", colour = 2, size = 2, \n        linetype = \"dashed\",\n        x = -Inf, xend = xpred, \n        yend = ypred, y = ypred)\n\n\n\n\n\n\nA Model is an Equation\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\Leftrightarrow Y = X\\underline\\beta + \\underline\\epsilon \\Leftrightarrow E(Y|X) = X\\underline \\beta;\\; V(Y|X)=\\sigma^2\n\\]\n\nFor a one unit increase in \\(x_i\\), \\(y_i\\) increases by \\(\\beta_1\\).\n\nIf our model fits well and this is statistically significant, we can apply this to the population.\n\nThe estimated value of \\(y_i\\) when \\(x_i=0\\) is \\(\\beta_0\\).\n\nNot always interesting, but usually\\(^*\\) necessary.\n\n\nOur prediction for \\(y_i\\) at any given value of \\(x_i\\) is calculated as: \\[\n\\hat y_i = \\hat\\beta_0 + \\hat \\beta_1x_i\n\\] where the “hats” mean we’ve found estimates for the \\(\\beta\\) values.\n\n\nAside: My notation differs from the text\nI will make mistakes, but in general:\n\n\\(Y\\) is a random variable (usually a vector) representing the response.\n\n\\(Y_i\\) is also a random variable (not a vector)\n\n\\(\\underline y\\) is the response vector; the observed values of \\(Y\\)\n\\(\\underline x\\) is the vector of covariates in simple linear regression\n\\(X\\) is a matrix of covariates\n\nNot a random variable!!! Capital letters could be either random or matrix (or both). I will specify when necessary.\nI will avoid the notation \\(X_i\\).\nWhen necessary, the first column is all ones so that \\(X\\underline\\beta=\\beta_0 + \\beta_1\\underline x_1 + \\beta_1\\underline x_2 + ...\\).\n\nContext should make this clear.\n\n\n\n\n\nThe Mean of \\(Y\\) at any value of \\(X\\)\n\\[\nE(Y|X) = X\\underline\\beta\n\\]\n\n\n\nA Linear Model is Linear in the Parameters\nThe following are linear:\n\n\\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1\\sin\\left(x_i^{x_i}\\right) + \\beta_2x_i^{\\sin(x_i)} + \\epsilon_i\\)\n\nThe following are not linear:\n\n\\(y_i = \\beta_0 + \\beta_1^2x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_0\\beta_1x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\sin(\\beta_1)x_i +\\epsilon_i\\)"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#estimation",
    "href": "L02-Fitting_Straight_Lines.html#estimation",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.2 Estimation",
    "text": "2.2 Estimation\n\nGoal: Find \\(\\beta\\) values which minimize the error\nModel: \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\)\nError: \\(\\hat\\epsilon_i = y_i - \\hat y_i\\)\nWhy is this a bad way to do it?\n\n\nLeast Squares\nGoal: Minimize \\(\\sum_{i=1}^n\\hat\\epsilon_i^2=\\sum_{i=1}^n(y_i - \\hat y_i)^2\\), the sum of squared errors.\nThis ensures errors don’t cancel out. It also penalizes large errors more.\nCould we have used \\(|\\epsilon_i|\\), or some other function? \\(|ly|\\)!\n\n\nLeast Squares Estimates\nI assume that you can do the Least Squares estimate in your sleep (be ready for tests).\nThe final results are: \\[\\begin{align*}\n\\hat\\beta_0 &= \\bar y - \\hat\\beta_1\\bar x\\\\\n\\hat\\beta_1 &= \\frac{\\sum_{i-1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i-1}^n(x_i - \\bar x)^2}\\stackrel{def}{=}\\frac{S_{XY}}{S_{XX}}\n\\end{align*}\\]\nThe textbook was written in 1998 and gives formulas to make the calculation easier to do on pocket calculators. You will not need a pocket calculator for this course.\n\n\nLet’s Add Assumptions\nAssumptions allow great things, but only when they’re correct!\n\n\\(E(\\epsilon_i) = 0\\), \\(V(\\epsilon_i) = \\sigma^2\\).\n\\(cov(\\epsilon_i, \\epsilon_j) =0\\) when \\(i\\ne j\\).\n\nImplies that \\(E(y_i) = \\beta_0 + \\beta_1x_i\\) and \\(V(y_i) = \\sigma^2\\).\n\n\\(\\epsilon_i\\stackrel{iid}{\\sim} N(0,\\sigma^2)\\)\n\nThis is a strong assumption, but often works!\n\n\nInterpretation: the model looks like a line with completely random errors. (“Completely random” doesn’t mean “without structure”!)\n\n\nMean of \\(\\hat\\beta_1\\)\nIt is easy to show that \\[\n\\hat\\beta_1 = \\frac{\\sum_{i-1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i-1}^n(x_i - \\bar x)^2} = \\frac{\\sum_{i-1}^n(x_i - \\bar x)y_i}{\\sum_{i-1}^n(x_i - \\bar x)^2}\n\\] since \\(\\sum(x_i - \\bar x) = 0\\).\nHomework: show that \\(E(\\hat\\beta_1) = \\beta_1\\) (Hint: use the fact that \\(E(y_i) = \\beta_0 + \\beta_1x_i\\) and the “pocket calculator” expansions).\n\n\nVariance of \\(\\hat\\beta_1\\)\n\\[\n\\hat\\beta_1 = \\frac{\\sum_{i-1}^n(x_i - \\bar x)y_i}{\\sum_{i-1}^n(x_i - \\bar x)^2}\n\\] can be re-written as \\[\n\\hat\\beta_1 = \\sum_{i=1}^na_iy_i\\text{, where }a_i = \\frac{x_i - \\bar x}{\\sum_{i=1}^n(x_i - \\bar x)^2}\n\\]\nThus the variance of \\(\\hat\\beta_1\\) is \\[\nV(\\hat\\beta_1) = \\sum_{i=1}^na_i^2V(y_i) = ... = \\frac{\\sigma^2}{S_{XX}} \\stackrel{plug-in}{=} = \\frac{s^2}{S_{XX}}\n\\]\n\n\nConfidence Interval and Test Statistic for \\(\\hat\\beta_1\\)\nThe test statistic can be found as: \\[\nt = \\frac{\\hat\\beta_1 - \\beta_1}{se(\\hat\\beta_1)} = \\frac{(\\hat\\beta_1 - \\beta_1)\\sqrt{S_{XX}}}{s} \\sim t_\\nu\n\\]\nSince this follows a \\(t\\) distribution, we can get the CI: \\[\n\\hat\\beta_1 \\pm t_\\nu(\\alpha/2)\\sqrt{\\frac{s^2}{S_{XX}}}\n\\]"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#participation-questions",
    "href": "L02-Fitting_Straight_Lines.html#participation-questions",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.3 Participation Questions",
    "text": "2.3 Participation Questions\n\nQ1\nAll models are useful, but some are wrong.\n\nTrue\nFalse\n\n\n\nQ2\nFor a one unit increase in \\(x\\), \\(y\\) increases by\n\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(\\beta_1x\\)\n\\(\\beta_0 + \\beta_1x\\)\n\n\n\nQ3\nThe standard error of \\(\\beta_1\\) refers to:\n\nThe variance of the population slope\nThe amount by which the slope might be off\nThe variance of the estimated slope across different samples\nA big chicken. Not, like, worryingly big, but big enough that you’d be like, “Wow, that’s a big chicken!”\n\n\n\nQ4\nWhich is not an assumption that we usually make in linear regression?\n\n\\(V(\\epsilon_i) = \\sigma^2\\)\n\\(E(\\epsilon_i) = 0\\)\n\\(E(X) = 0\\)\n\\(\\epsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\)\n\n\n\nQ5\nWhich of the following is not a linear model?\n\n\\(y_i = \\beta_0 + \\beta_1^2x_i + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_0x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1\\sin\\left(x_i^{x_i}\\right) + \\beta_2x_i^{\\sin(x_i)} + \\epsilon_i\\)\n\n\n\nQ6\nThe sum of squared errors is the best way to estimate the model parameters.\n\nTrue\nFalse"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#analysis-of-variance",
    "href": "L02-Fitting_Straight_Lines.html#analysis-of-variance",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.4 Analysis of Variance",
    "text": "2.4 Analysis of Variance\n\nStatistics is the Study of Variance\n\nGiven a data set with variable \\(\\underline y\\), \\(V(\\underline y) = \\sigma^2_y\\).\n\nThis is just the variance of a single variable.\n\nOnce we’ve incorporated the linear relationship with \\(\\underline x\\), \\(V(\\hat\\beta \\underline x + \\underline\\epsilon)=0+V(\\underline\\epsilon) = \\sigma^2\\)\n\nMathematically, \\(\\sigma^2 \\le \\sigma_y^2\\).\n\n\nThe variance in \\(Y\\) is explained by \\(X\\)!\n\n\nA Useful Identity, and its Interpretations\n\\[\\begin{align*}\n\\hat\\epsilon_i &= y_i - \\hat y_i \\\\&= y_i - \\bar y - (\\hat y_i - \\bar y)\\\\\n\\implies \\sum_{i=1}^n\\hat\\epsilon_i^2 &= \\sum_{i=1}^n(y_i-\\bar y)^2 - \\sum_{i=1}^n(\\hat y_i - \\bar y)^2\n\\end{align*}\\] where we’ve simply added and subtracted \\(\\bar y\\). The final line skips a few steps (try them yourself)!\nThe last line is often written as: \\(SS_E = SS_T - SS_{Reg}\\)\n\n\nSums of Squares\n\\[\nSS_E = SS_T - SS_{Reg}\n\\]\n\n\\(SS_E\\): Sum of Squared Errors\n\\(SS_T\\): Sum of Squares Total (i.e., without \\(\\underline x\\))\n\\(SS_{Reg}\\): Sum of Squares due to the regression.\n\nIt’s the variance of the line (calculated at observed \\(\\underline x\\) values) around the mean of \\(\\underline y\\)???\n\nThis is incredibly useful, but weird.\n\nI use \\(SS_{Reg}\\) instead of \\(SS_R\\) because some textbooks use \\(SS_R\\) as the Sume of Squared Residuals, which is confusing.\n\n\n\n\nAside: Degrees of Freedom\nDef: The number of “pieces of information” from \\(y_1, y_2, ..., y_n\\) to construct a new number.\n\nIf I have \\(x = (1,3,2,1,3,???)\\) and I know that \\(\\bar x = 2\\), I can recover the missing piece.\n\nThe mean “uses” (accounts for) one degree of freedom\n\nIf I have \\(x = (1,2,3,1,???,???)\\) and I know \\(\\bar x = 2\\) and \\(s_x^2=1\\), I can recover the two missing pieces.\n\nThe variance accounts for two degrees of freedom.\n\nOne \\(df\\) is required to compute it.\n\n\n\nEstimating one parameter takes away one degree of freedom for the rest!\n\nCan find \\(\\bar x\\) when \\(x = (1)\\), but can’t find \\(s_x^2\\) because there aren’t enough \\(df\\)!\n\n\n\nSums of Squares\n\n\n\n\n\n\n\n\n\nSource of Variation\nDegress of Freedom \\(df\\)\nSum of Squares (\\(SS\\))\nMean Square (\\(MS\\))\n\n\n\n\nRegression\n1\n\\(\\sum_{i=1}^n(\\hat y_i - \\bar y)^2\\)\n\\(MS_{Reg}\\)\n\n\nError\n\\(n-2\\)\n\\(\\sum_{i=1}^n(y_i - \\hat y_i)^2\\)\n\\(MS_E=s^2\\)\n\n\nTotal\n\\(n-1\\)\n\\(\\sum_{i=1}^n(y_i - \\bar y)^2\\)\n\\(s_y^2\\)\n\n\n\n\nNotice that \\(SS_T = SS_{Reg} + SS_E\\), which is also true for the \\(df\\) (but not \\(MS\\)). \nWhy is \\(df_E = n-2\\)? What two parameters have we estimated?\n\n\\(df_{Reg}\\) is trickier to explain. It suffices to know that \\(df_{Reg} = df_T-df_E\\).\n\n\n\n\nUsing Sums/Means of Squares\n\nIf \\(\\hat y_i = \\bar y\\) for all \\(i\\), then we have a horizontal line!\n\nThat is, there is no relationship between \\(\\underline x\\) and $\\underline \\(y\\).\nIn this case, \\(SS_{reg} = \\sum_{i=1}^n(\\hat y_i - \\bar y)^2 = 0\\).\n\n\nOkay, so, just test for \\(SS_{Reg} = 0\\)?\nBut how??? We need some measure of how far from 0 is statistically significant!!!\nRecall that \\(SS_E = \\sum_{i=1}^n(y_i - \\hat y_i)^2\\).\n\nWe can compare the variation around the line to the variation of the line.\n\nThis is \\(MS_{Reg}/MS_E\\), and it follows an \\(F\\) distribution!!\n\n\n\n\nThe F-test for Significance of Regression\n\\[\nF_{df_{Reg}, df_E} = \\dfrac{MS_{Reg}}{MS_E} = \\dfrac{MS_{Reg}}{s^2}\n\\]\n\nRecall that \\(SS_{Reg} = SS_E + SS_T &gt; SS_E\\), but the \\(df\\) make a difference.\nFor homework, show that \\(E(MS_{Reg}) = \\sigma^2 + \\beta_1^2\\sum_{i=1}^n(x_i-\\bar x)^2\\)\nThis implies that \\(E(MS_{Reg}) &gt; E(MS_E) = \\sigma^2\\).\n\n\n\nExercise A from Chapter 3 (Ch03 Exercises cover Ch1-3)\n\n\nA study was made on the effect of temperature on the yield of a chemical process. The data are shown to the right.\n\nAssuming \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\), what are the least squares estimates of \\(\\beta_0\\) and \\(\\beta_1\\)?\nConstruct the analysis of variance table and test the hypothesis \\(H_0: \\beta_1=0\\) at the 0.05 level.\nWhat are the confidence limits (at = 0.05) for \\(\\beta_1\\)?\n\n\n\nlibrary(aprean3) # Data from textbook\nhead(dse03a) # Data Set Exercise Ch03A\n\n   x  y\n1 -5  1\n2 -4  5\n3 -3  4\n4 -2  7\n5 -1 10\n6  0  8\n\nnrow(dse03a)\n\n[1] 11\n\n\n\n\n\nGood textbook questions: A, K, O, P, T, U, X (using data(anscombe) in R), Z, AA, CC. Note that all data sets in the textbook are available in an R package found here."
  },
  {
    "objectID": "L02-OLS_Estimates.html#analysis-of-variance",
    "href": "L02-OLS_Estimates.html#analysis-of-variance",
    "title": "3  Lab 1: OLS Estimates",
    "section": "3.1 Analysis of Variance",
    "text": "3.1 Analysis of Variance\nThe code below demonstrates how the ANOVA tables are calculated.\n\ny &lt;- mydata$y\nyhat &lt;- b0 + b1*mydata$x\nybar &lt;- mean(y)\n\nc(sum((yhat - ybar)^2), sum((y - yhat)^2), sum((y - ybar)^2))\n\n[1] 4809.33809   30.93852 4840.27661\n\nANOVA &lt;- data.frame(\n    Source = c(\"Regression\", \"Error\", \"Total (cor.)\"),\n    df = c(1, nrow(mydata) - 2, nrow(mydata)),\n    SS = c(sum((yhat - ybar)^2), sum((y - yhat)^2), sum((y - ybar)^2))\n)\nANOVA$MS &lt;- ANOVA$SS / ANOVA$df\nANOVA\n\n        Source df         SS          MS\n1   Regression  1 4809.33809 4809.338089\n2        Error 28   30.93852    1.104947\n3 Total (cor.) 30 4840.27661  161.342554\n\n\nThis is equivalent to what R’s built-in functions do!\n\nanova(lm(y ~ x, data = mydata))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 4809.3  4809.3  4352.5 &lt; 2.2e-16 ***\nResiduals 28   30.9     1.1                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "L02-OLS_Estimates.html#dependence-and-centering",
    "href": "L02-OLS_Estimates.html#dependence-and-centering",
    "title": "3  Lab 1: OLS Estimates",
    "section": "3.2 Dependence and Centering",
    "text": "3.2 Dependence and Centering\nSomething not touched on in class is that \\(cov(\\hat\\beta_0, \\hat\\beta_1)\\) is not 0! This should be clear from the formula got \\(\\hat\\beta_0\\), which is \\(\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\\).\nThe code below repeats what we did before, but with higher variance to better demonstrate the problem.\nIt also records the estimates based on centering \\(\\underline x\\). Notice how the formula for \\(\\hat\\beta_0\\) is no longer dependent on \\(\\hat\\beta_1\\) if the mean of \\(\\underline x\\) is 0!\n\nb1s &lt;- double(R)\nb0s &lt;- double(R)\nb1cs &lt;- double(R)\nb0cs &lt;- double(R)\nn &lt;- 25\nx &lt;- runif(n, 0, 10)\nxc &lt;- x - mean(x) # centered\n\nfor (i in 1:R) {\n    y &lt;- 2 - 2 * x + rnorm(25, 0, 4)\n    b1 &lt;- Sdotdot(x, y) / Sdotdot(x, x)\n    b0 &lt;- mean(y) - b1 * mean(x)\n    b1s[i] &lt;- b1\n    b0s[i] &lt;- b0\n\n    # Centered\n    y &lt;- 2 - 2 * xc + rnorm(25, 0, 4)\n    b1c &lt;- Sdotdot(xc, y) / Sdotdot(xc, xc)\n    b1cs[i] &lt;- b1c\n    b0c &lt;- mean(y) - b1 * mean(xc)\n    b0cs[i] &lt;- b0c\n}\n\npar(mfrow = c(1,2))\nplot(b1s, b0s)\nplot(b1cs, b0cs)"
  },
  {
    "objectID": "Lb02-OLS_Estimates.html#analysis-of-variance",
    "href": "Lb02-OLS_Estimates.html#analysis-of-variance",
    "title": "3  Lab 2: OLS Estimates",
    "section": "3.1 Analysis of Variance",
    "text": "3.1 Analysis of Variance\nThe code below demonstrates how the ANOVA tables are calculated.\n\ny &lt;- mydata$y\nyhat &lt;- b0 + b1*mydata$x\nybar &lt;- mean(y)\n\nc(sum((yhat - ybar)^2), sum((y - yhat)^2), sum((y - ybar)^2))\n\n[1] 4809.33809   30.93852 4840.27661\n\nANOVA &lt;- data.frame(\n    Source = c(\"Regression\", \"Error\", \"Total (cor.)\"),\n    df = c(1, nrow(mydata) - 2, nrow(mydata)),\n    SS = c(sum((yhat - ybar)^2), sum((y - yhat)^2), sum((y - ybar)^2))\n)\nANOVA$MS &lt;- ANOVA$SS / ANOVA$df\nANOVA\n\n        Source df         SS          MS\n1   Regression  1 4809.33809 4809.338089\n2        Error 28   30.93852    1.104947\n3 Total (cor.) 30 4840.27661  161.342554\n\n\nThis is equivalent to what R’s built-in functions do!\n\nanova(lm(y ~ x, data = mydata))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 4809.3  4809.3  4352.5 &lt; 2.2e-16 ***\nResiduals 28   30.9     1.1                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Lb02-OLS_Estimates.html#dependence-and-centering",
    "href": "Lb02-OLS_Estimates.html#dependence-and-centering",
    "title": "3  Lab 2: OLS Estimates",
    "section": "3.2 Dependence and Centering",
    "text": "3.2 Dependence and Centering\nSomething not touched on in class is that \\(cov(\\hat\\beta_0, \\hat\\beta_1)\\) is not 0! This should be clear from the formula got \\(\\hat\\beta_0\\), which is \\(\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\\).\nThe code below repeats what we did before, but with higher variance to better demonstrate the problem.\nIt also records the estimates based on centering \\(\\underline x\\). Notice how the formula for \\(\\hat\\beta_0\\) is no longer dependent on \\(\\hat\\beta_1\\) if the mean of \\(\\underline x\\) is 0!\n\nb1s &lt;- double(R)\nb0s &lt;- double(R)\nb1cs &lt;- double(R)\nb0cs &lt;- double(R)\nn &lt;- 25\nx &lt;- runif(n, 0, 10)\nxc &lt;- x - mean(x) # centered\n\nfor (i in 1:R) {\n    y &lt;- 2 - 2 * x + rnorm(25, 0, 4)\n    b1 &lt;- Sdotdot(x, y) / Sdotdot(x, x)\n    b0 &lt;- mean(y) - b1 * mean(x)\n    b1s[i] &lt;- b1\n    b0s[i] &lt;- b0\n\n    # Centered\n    y &lt;- 2 - 2 * xc + rnorm(25, 0, 4)\n    b1c &lt;- Sdotdot(xc, y) / Sdotdot(xc, xc)\n    b1cs[i] &lt;- b1c\n    b0c &lt;- mean(y) - b1 * mean(xc)\n    b0cs[i] &lt;- b0c\n}\n\npar(mfrow = c(1,2))\nplot(b1s, b0s)\nplot(b1cs, b0cs)"
  },
  {
    "objectID": "Lb03-MSE.html#best-estimator-of-sigma2",
    "href": "Lb03-MSE.html#best-estimator-of-sigma2",
    "title": "5  Lab 3: Bias/variance of \\(\\sigma^2\\)",
    "section": "5.1 Best estimator of \\(\\sigma^2\\)",
    "text": "5.1 Best estimator of \\(\\sigma^2\\)\nWe saw in the notes that \\(E(s^2) = \\sigma^2\\). Let’s explore why this might not be the best estimator.\nWe define the MSE of an estimator \\(\\theta\\) as \\(E((\\theta - \\hat\\theta^2)^2)\\). For the variance, this is \\(E((\\sigma^2 - s^2)^2)\\).\nLet’s simulate a bunch of linear models, calculate the standard deviations, and then calculate this quantity.\nWe’re going to focus on multiplicative bias, and you’ll see why at the end. This means that we’ll focus on estimators of the form \\(as^2\\).\n\nset.seed(2112)\n\nn &lt;- 30\nx &lt;- runif(n, 0, 10)\nbeta0 &lt;- -3\nbeta1 &lt;- -4\nsigma &lt;- 3\n\nreps &lt;- 1000\nesst &lt;- double(reps)\n\nfor (i in 1:reps) {\n    y &lt;- beta0 + beta1*x + rnorm(n, 0, sigma)\n    beta1 &lt;- sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)\n    beta0 &lt;- mean(y) - beta1 * mean(x)\n    yhat &lt;- beta0 + beta1 * x\n    e &lt;- y - yhat\n    esst[i] &lt;- sum(e^2)\n}\n\na &lt;- (n-4):(n+4)\nmse &lt;- double(length(a))\nbias2 &lt;- double(length(a))\nfor (i in seq_along(a)) {\n    mse[i] &lt;- mean((sigma^2 - esst / a[i])^2)\n    bias2[i] &lt;- (sigma^2 - mean(esst / a[i]))^2\n}\n\npar(mfrow = c(1,2))\nplot(a - n, mse, main = \"MSE = Bias^2 + Variance\")\nplot(a - n, bias2, main = \"Bias^2\")\nabline(h = 0)\n\n\n\n\nFrom these plots, we see that the lowest MSE, i.e. the lowest value of \\(E((\\sigma^2 - as^2)^2)\\), is at \\(a = 1/n\\). Note that this corresponds to the MLE of \\(\\sigma^2\\). However, this is a biased estimate, and the unbiased estimate occurs at \\(a=1/(n-2)\\).\nWhat’s happening here? Shouldn’t unbiased be best? Well, yes, if our criteria is minimizing bias! If we want to minimize \\(E((\\sigma^2 - \\hat\\sigma^2)^2)\\), we have to account for the variance of the estimator across all possible samples as well!\nHWK: Modify the code to show that the bias of the constant model (\\(\\beta_1 = 0\\)) is minimized at \\(n-1\\), with the MSE being minimized at \\(n+1\\). It’s bizarre, but that’s how it works!"
  },
  {
    "objectID": "Lb03-MSE.html#residuals",
    "href": "Lb03-MSE.html#residuals",
    "title": "5  Lab 3: Bias/variance of \\(\\sigma^2\\)",
    "section": "5.2 Residuals",
    "text": "5.2 Residuals\nThe plot.lm() function makes most of the plots you’ll need.\n\nmylm &lt;- lm(mpg ~ disp, data = mtcars)\nplot(mylm, which=1:6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe broom package will be very useful in the future. In particular, the augment() function results in a tidy data frame with columns that are very relevant to our analyses.\n\nlibrary(broom)\n\nhead(augment(mylm))\n\n# A tibble: 6 × 9\n  .rownames           mpg  disp .fitted .resid   .hat .sigma .cooksd .std.resid\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1 Mazda RX4          21     160    23.0  -2.01 0.0418   3.29 0.00865     -0.630\n2 Mazda RX4 Wag      21     160    23.0  -2.01 0.0418   3.29 0.00865     -0.630\n3 Datsun 710         22.8   108    25.1  -2.35 0.0629   3.28 0.0187      -0.746\n4 Hornet 4 Drive     21.4   258    19.0   2.43 0.0328   3.27 0.00983      0.761\n5 Hornet Sportabout  18.7   360    14.8   3.94 0.0663   3.22 0.0558       1.25 \n6 Valiant            18.1   225    20.3  -2.23 0.0313   3.28 0.00782     -0.696\n\n\n\nglance(mylm)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1     0.718        0.709  3.25    76.5 9.38e-10     1  -82.1  170.  175.    317.\n# … with 2 more variables: df.residual &lt;int&gt;, nobs &lt;int&gt;, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\n\nanova(mylm)\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndisp       1 808.89  808.89  76.513 9.38e-10 ***\nResiduals 30 317.16   10.57                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nx &lt;- rnorm(1000); qqnorm(x); qqline(x)"
  },
  {
    "objectID": "Lb04-R_Matrix_Form.html#verifying-matrix-results",
    "href": "Lb04-R_Matrix_Form.html#verifying-matrix-results",
    "title": "7  Lab 4: Verifying Matrix Identities",
    "section": "7.1 Verifying Matrix Results",
    "text": "7.1 Verifying Matrix Results\nWe’ll use the mtcars data for this. Here’s what it looks like:\n\nx &lt;- mtcars$disp\ny &lt;- mtcars$mpg\n\nplot(y ~ x)\nabline(lm(y ~ x))\n\n\n\n\nIt looks like the slope is negative, and the intercept will be somewhere between 25 and 35.\nLet’s use the formulae from the previous course: \\(\\hat\\beta_1 = S_{XY}/S_{XX}\\) and \\(\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\\).\n\nb1 &lt;- sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)\nb0 &lt;- mean(y) - mean(x) * b1\n\nmatrix(c(b0, b1))\n\n            [,1]\n[1,] 29.59985476\n[2,] -0.04121512\n\n\nTo make the matrix multiplication to work, we need \\(X\\) to be a column of 1s and a column representing our covariate.\n\nX &lt;- cbind(1, x)\nhead(X)\n\n         x\n[1,] 1 160\n[2,] 1 160\n[3,] 1 108\n[4,] 1 258\n[5,] 1 360\n[6,] 1 225\n\n\nThe estimates should be \\((X^TX)^{-1}X^T\\underline y\\). In R, we find the transpose with the t() function and we find inverse with the solve() function.\n\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nbeta_hat\n\n         [,1]\n  29.59985476\nx -0.04121512\n\n\nIt works!\nNow let’s check the ANOVA table!\n\nn &lt;- length(x)\ndata.frame(source = c(\"Regression\", \"Error\", \"Total\"),\n    df = c(2, n-2, n),\n    SS = c(t(beta_hat) %*% t(X) %*% y, \n        t(y) %*% y - t(beta_hat) %*% t(X) %*% y,\n        t(y) %*% y)\n)\n\n      source df         SS\n1 Regression  2 13725.1513\n2      Error 30   317.1587\n3      Total 32 14042.3100\n\n\n\nanova(lm(y ~ x))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nx          1 808.89  808.89  76.513 9.38e-10 ***\nResiduals 30 317.16   10.57                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncolSums(anova(lm(y ~ x)))\n\n       Df    Sum Sq   Mean Sq   F value    Pr(&gt;F) \n  31.0000 1126.0472  819.4605        NA        NA \n\n\nThey’re slightly different? Why?\nBecause the equation in the textbook is for the uncorrected sum of squares, which basically means we’re looking at estimating both \\(\\beta_0\\) and \\(\\beta_1\\) at the same time (hence the df of \\(n-2\\)). The usual ANOVA table is the corrected sum of squares, which the textbook labels \\(SS(\\hat\\beta_1|\\hat\\beta_1)\\) to make it clear that it’s estimating \\(\\beta_1\\) only; \\(\\beta_0\\) has already been estimated.\n\nn &lt;- length(x)\ndata.frame(source = c(\"Regression\", \"Error\", \"Total\"),\n    df = c(1, n-1, n),\n    SS = c(t(beta_hat) %*% t(X) %*% y - n * mean(y)^2, \n        t(y) %*% y - t(beta_hat) %*% t(X) %*% y,\n        t(y) %*% y - n * mean(y)^2)\n)\n\n      source df        SS\n1 Regression  1  808.8885\n2      Error 31  317.1587\n3      Total 32 1126.0472\n\n\nThe matrix form for \\(R^2\\) is a little different from what you might expect. It uses this idea of “corrected” sum-of-squares as well. For homework, verify that the corrected sum-of-squares works out to the same formula.\nHere’s how to extract the \\(R^2\\) value from R (note that the programming language R has nothing to do with the \\(R^2\\); R is named after S, which was the programming language that came before it (both chronologically and alphabetically); you’ll still find references to S and S-Plus).\n\nsummary(lm(y ~ x))$r.squared\n\n[1] 0.7183433\n\n\nIn the textbook, the formula is given as: \\[\nR^2 = \\frac{\\hat{\\underline\\beta}^TX^T\\underline y - n\\bar y^2}{\\underline y^t\\underline y - n\\bar y^2}\n\\]\n\nnumerator &lt;- t(beta_hat) %*% t(X) %*% y - n * mean(y)^2\ndenominator &lt;- t(y) %*% y - n * mean(y)^2\nnumerator / denominator\n\n          [,1]\n[1,] 0.7183433"
  },
  {
    "objectID": "Lb04-R_Matrix_Form.html#section",
    "href": "Lb04-R_Matrix_Form.html#section",
    "title": "7  Lab 4: Verifying Matrix Identities",
    "section": "7.2 ",
    "text": "7.2"
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html#basic-anova",
    "href": "Lb05-ANOVA-R2-F_test-CI.html#basic-anova",
    "title": "9  Lab 5: ANOVA",
    "section": "9.1 Basic ANOVA",
    "text": "9.1 Basic ANOVA\n\nplot(mpg ~ disp, data = mtcars)\nabline(lm(mpg ~ disp, data = mtcars))\n\n\n\n\n\nanova(lm(mpg ~ qsec, data = mtcars))\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nqsec       1 197.39 197.392  6.3767 0.01708 *\nResiduals 30 928.66  30.955                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsummary(lm(mpg ~ qsec, data = mtcars))$coef\n\n             Estimate Std. Error    t value   Pr(&gt;|t|)\n(Intercept) -5.114038 10.0295433 -0.5098974 0.61385436\nqsec         1.412125  0.5592101  2.5252133 0.01708199\n\n\nNotice the p-values! Also notice that the \\(F\\)-value is the square of the \\(t\\)-value! It’s like magic! Math is cool."
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html#r2-always-increases-with-new-predictors",
    "href": "Lb05-ANOVA-R2-F_test-CI.html#r2-always-increases-with-new-predictors",
    "title": "9  Lab 5: ANOVA",
    "section": "9.2 \\(R^2\\) always increases with new predictors",
    "text": "9.2 \\(R^2\\) always increases with new predictors\n\nnx &lt;- 10 # Number of uncorrelated predictors\nuncorr &lt;- matrix(rnorm(50*(nx + 1)), \n    nrow = 50, ncol = nx + 1)\n## First column is y, rest are x\ncolnames(uncorr) &lt;- c(\"y\", paste0(\"x\", 1:nx))\nuncorr &lt;- as.data.frame(uncorr)\n\nrsquares &lt;- NA\nfor (i in 2:(nx + 1)) {\n    rsquares &lt;- c(rsquares,\n        summary(lm(y ~ ., data = uncorr[,1:i]))$r.squared)\n}\nplot(1:10, rsquares[-1], type = \"b\",\n    xlab = \"Number of Uncorrelated Predictors\",\n    ylab = \"R^2 Value\",\n    main = \"R^2 ALWAYS increases\")\n\n\n\n\nThe one exception is when one of the predictors is a linear combination of the previous predictors. In this case, \\(R^2\\) will not change!\n\nuncorr[, nx + 2] &lt;- uncorr[,2] + 3*uncorr[,3]\nrsquares &lt;- c(rsquares, summary(lm(y ~ ., data = uncorr))$r.squared)\nrsquares\n\n [1]         NA 0.01829891 0.02387831 0.04419122 0.10169165 0.11606637\n [7] 0.12880519 0.15445357 0.20303605 0.21277197 0.23403062 0.23403062\n\nplot(rsquares, type = \"b\")\n\n\n\n\n\nx &lt;- runif(20, 0, 10)\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nb0s &lt;- b1s &lt;- double(1000)\nplot(NA, pch = 0, \n    xlim = c(-2, 12), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nabline(h = b0 + b1*mean(x))\nabline(v = mean(x))\nfor (i in 1:1000) {\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    abline(lm(y ~ x), col = rgb(0,0,0,0.1))\n}\n\n\n\n\nLet’s do that again, but record the values and only show the 89% quantiles!\n\nx &lt;- runif(20, 0, 10)\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nall_lines &lt;- replicate(1000, {\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    predict(lm(y ~ x), newdata = list(x = seq(0, 10, 0.1)))\n})\n\neightnine &lt;- apply(all_lines, 1, quantile, probs = c(0.045, 0.945))\n\n\nplot(NA, pch = 0, \n    xlim = c(0, 10), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nlines(seq(0, 10, 0.1), eightnine[1, ])\nlines(seq(0, 10, 0.1), eightnine[2, ])\n\n\n\n\nNote that the theoretical calculation of these bounds is built into R:\n\nplot(NA, pch = 0, \n    xlim = c(0, 10), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nlines(seq(0, 10, 0.1), eightnine[1, ])\nlines(seq(0, 10, 0.1), eightnine[2, ])\n\n## Add the TRUE relationship\nxseq &lt;- seq(0, 10, 0.1)\nlines(xseq, b0 + b1*xseq, col = 3)\n\n## New sample from the data generating process\nx &lt;- runif(20, 0, 10)\ny &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n\n## Extract the CI\nmylm &lt;- lm(y ~ x)\nxbeta &lt;- predict(mylm, interval = \"confidence\",\n    newdata = list(x = xseq))\n#lines(xseq, xbeta[,\"fit\"], col = 4)\nlines(xseq, xbeta[,\"upr\"], col = 4)\nlines(xseq, xbeta[,\"lwr\"], col = 4)\n\n\n\n\nNote that the intervals won’t exactly align - the samples are going to be different each time! In 95% of the samples we collect from this data generating process, the CI we construct from the sample will contain the true (green) line. This is a basic definition for confidence intervals, but it’s neat to see it around a line.\nNotice how the CI is curved. This is completely, 100% expected. Recall that \\((\\bar x, \\bar y)\\) is always a point on the line. If \\(x\\) is the same for all samples, then the variance in the height at \\(\\bar y\\) is just the variance in \\(y\\). However, we can rotate the line around this point and still fit most of the data “pretty well”, which is where the curved nature of the line comes from!\n\n9.2.1 Aside\nWhy did I use the same \\(x\\) values for all of the simulations? Because that’s part of the assumptions (this isn’t an important point to make). Again, notice how the point \\((\\bar x, \\bar y)\\) is always on the line, and how the variance at the point \\(\\bar x\\) is minimized. If \\(\\bar x\\) is randomly moved, then there’s extra variance in the line.\n\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nall_lines2 &lt;- replicate(1000, {\n    x &lt;- runif(20, 0, 10)\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    predict(lm(y ~ x), newdata = list(x = seq(0, 10, 0.1)))\n})\n\neightnine2 &lt;- apply(all_lines2, 1, quantile, probs = c(0.045, 0.945))\n\n\nplot(NA, pch = 0, \n    xlim = c(0, 10), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nlines(seq(0, 10, 0.1), eightnine[1, ])\nlines(seq(0, 10, 0.1), eightnine[2, ])\nlines(seq(0, 10, 0.1), eightnine2[1, ], col = 2)\nlines(seq(0, 10, 0.1), eightnine2[2, ], col = 2)\nlegend(\"topleft\", legend = c(\"non-random x\", \"random x\"), col = 1:2, lty = 1)\n\n\n\n\nThe textbook for this course also covers models that incorporate randomness in \\(X\\), but this is not covered in this course."
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html#covariance-of-the-parameters",
    "href": "Lb05-ANOVA-R2-F_test-CI.html#covariance-of-the-parameters",
    "title": "9  Lab 5: ANOVA",
    "section": "9.3 Covariance of the parameters",
    "text": "9.3 Covariance of the parameters\n\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nall_params &lt;- replicate(1000, {\n    x &lt;- runif(20, 0, 10)\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    coef(lm(y ~ x))\n})\n\n\nplot(t(all_params))\n\n\n\n\nIntuition check: were you expecting a negative slope? Does this make sense? If you increase \\(\\beta_0\\), why would \\(\\beta_1\\) decrease?\nFor homework, try a negative intercept and see what happens! What about a negative slope?"
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html#joint-normality-of-the-underlinehatbetas",
    "href": "Lb05-ANOVA-R2-F_test-CI.html#joint-normality-of-the-underlinehatbetas",
    "title": "9  Lab 5: ANOVA",
    "section": "9.4 Joint Normality of the \\(\\underline{\\hat\\beta}\\)s",
    "text": "9.4 Joint Normality of the \\(\\underline{\\hat\\beta}\\)s\nJoint normality leads to marginal normality! This means we can create a confidence interval based on the marginal. However, if the joint distribution has a strong correlation, the marginal confidence intervals might contain unlikely points!\n\npar(mfrow = c(1, 3))\n\n## Marginal distribution of beta_0\nplot(density(all_params[1,]),\n    main = \"Distribution of b_0\",\n    xlab = \"b_0\")\nabline(v = quantile(all_params[1,], c(0.055, 0.945)), col = 2)\nabline(v = 8, col = 3) # Hypothesized beta_0\n\n## Marginal distribution of beta_1\nplot(density(all_params[2,]),\n    main = \"Distribution of b_1\",\n    xlab = \"b_1\")\nabline(v = quantile(all_params[2,], c(0.055, 0.945)), col = 2)\nabline(v = 6, col = 3) # Hypothesized beta_1\n\n## Joint distribution\nplot(t(all_params), main = \"Joint Distribution\",\n    xlab = \"b_0\", ylab = \"b_1\")\nabline(v = quantile(all_params[1,], c(0.055, 0.945)), col = 2)\nabline(h = quantile(all_params[2,], c(0.055, 0.945)), col = 2)\npoints(x = 8, y = 6, col = 3, pch = 16, cex = 1.5)\n\n\n\n\nNotice how the rectangular confidence region in the joint distribution contains regions where there are no points! For example, a hypothesis test for whether \\(\\beta_0=8\\) and \\(\\beta_1 = 6\\) (the green lines/points) would not be rejected if we checked the two confidence intervals separately, but likely should be rejected given the joint distribution! This is exactly what happens when the F-test is significant but none of the t-tests for individual predictors is significant.\nIn general, the CIs for each individual \\(\\hat\\beta\\) are missing something - especially if there’s correlation in the predictors!\nIn these examples, we repeatedly sampled from the true relationship to obtain simulation-based confidence intervals. The normality assumption allows us to make inferences about the distribution of the parameters - including the joint distribution - from a single sample! Inference is powerful!"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#the-hat-matrix",
    "href": "Lb11-R_hat_resids_cook.html#the-hat-matrix",
    "title": "15  Hat Matrix and Residuals in R",
    "section": "15.1 The Hat Matrix",
    "text": "15.1 The Hat Matrix\n\\[\nH = X(X^TX)^{-1}X^T\n\\]\nIn R, we can calculate the diagonal of the hat matrix as follows:\n\nmylm &lt;- lm(mpg ~ disp + wt, data = mtcars)\nhatvalues(mylm) |&gt; unname()\n\n [1] 0.04339369 0.04550894 0.06309504 0.03877647 0.14078260 0.04406584\n [7] 0.11516157 0.09635365 0.09875274 0.11012510 0.11012510 0.08141444\n[13] 0.04168379 0.04521644 0.17206264 0.19889125 0.19275897 0.08015728\n[19] 0.12405357 0.09579747 0.05703451 0.06246825 0.05648077 0.06838477\n[25] 0.14119998 0.08720679 0.07149742 0.16032953 0.18794989 0.05044456\n[31] 0.04474121 0.07408572"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#extracting-the-diagonals-from-h",
    "href": "Lb11-R_hat_resids_cook.html#extracting-the-diagonals-from-h",
    "title": "15  Hat Matrix and Residuals in R",
    "section": "15.2 Extracting the Diagonals from H",
    "text": "15.2 Extracting the Diagonals from H\nThere isn’t a built-in function for the full hat matrix (the diagonals are usually all you’ll need). For demonstration, here are some demonstrations of the features of the hat matrix.\n\nX &lt;- model.matrix(mylm)\nH &lt;- X %*% solve(t(X) %*% X) %*% t(X)\nall.equal(diag(H), hatvalues(mylm))\n\n[1] TRUE"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#features-of-h",
    "href": "Lb11-R_hat_resids_cook.html#features-of-h",
    "title": "15  Hat Matrix and Residuals in R",
    "section": "15.3 Features of H",
    "text": "15.3 Features of H\nIn the code below, I use the unname() function because mtcars has rownames which make the output harder to see (this used to be the norm in R, but it’s fallen out of fashion).\n\ncolSums(H) |&gt; unname()\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\nrowSums(H) |&gt; unname()\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\nrange(H) # -1 &lt;= h_{ij} &lt;= 1\n\n[1] -0.1076609  0.1988913\n\nrange(diag(H)) # 0 &lt;= h_{ii} &lt;= 1\n\n[1] 0.03877647 0.19889125\n\nH %*% rep(1, ncol(H)) # H1 = 1\n\n                    [,1]\nMazda RX4              1\nMazda RX4 Wag          1\nDatsun 710             1\nHornet 4 Drive         1\nHornet Sportabout      1\nValiant                1\nDuster 360             1\nMerc 240D              1\nMerc 230               1\nMerc 280               1\nMerc 280C              1\nMerc 450SE             1\nMerc 450SL             1\nMerc 450SLC            1\nCadillac Fleetwood     1\nLincoln Continental    1\nChrysler Imperial      1\nFiat 128               1\nHonda Civic            1\nToyota Corolla         1\nToyota Corona          1\nDodge Challenger       1\nAMC Javelin            1\nCamaro Z28             1\nPontiac Firebird       1\nFiat X1-9              1\nPorsche 914-2          1\nLotus Europa           1\nFord Pantera L         1\nFerrari Dino           1\nMaserati Bora          1\nVolvo 142E             1"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#extracting-the-residuals",
    "href": "Lb11-R_hat_resids_cook.html#extracting-the-residuals",
    "title": "15  Hat Matrix and Residuals in R",
    "section": "15.4 Extracting the residuals",
    "text": "15.4 Extracting the residuals\nSee ?influence.measures.\n\n?rstandard\ncooks.distance(mylm)\n\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n       1.022220e-02        4.351414e-03        1.721743e-02        5.241995e-03 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n       2.027544e-02        3.089406e-03        3.094888e-02        3.443123e-02 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n       3.775416e-03        8.693734e-03        3.864780e-02        4.425052e-06 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n       1.330376e-04        9.458368e-03        1.920638e-02        3.794816e-02 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n       3.441118e-01        1.429900e-01        3.046404e-02        1.850525e-01 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n       2.372110e-02        1.146745e-02        2.036673e-02        2.070769e-02 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n       1.331763e-01        2.049673e-04        3.812246e-04        4.292901e-02 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n       5.996344e-02        2.547326e-02        1.362489e-02        1.494183e-02"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#the-broom-package",
    "href": "Lb11-R_hat_resids_cook.html#the-broom-package",
    "title": "15  Hat Matrix and Residuals in R",
    "section": "15.5 The broom package",
    "text": "15.5 The broom package\nThe broom package has a wonderful function called augment(). This function sets up our data so that it’s super easy to see what we need.\n\nlibrary(broom)\naugment(mylm)\n\n# A tibble: 32 × 10\n   .rownames        mpg  disp    wt .fitted .resid   .hat .sigma .cooksd .std.…¹\n   &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Mazda RX4       21    160   2.62    23.3 -2.35  0.0434   2.93 0.0102   -0.822\n 2 Mazda RX4 Wag   21    160   2.88    22.5 -1.49  0.0455   2.95 0.00435  -0.523\n 3 Datsun 710      22.8  108   2.32    25.3 -2.47  0.0631   2.93 0.0172   -0.876\n 4 Hornet 4 Drive  21.4  258   3.22    19.6  1.79  0.0388   2.95 0.00524   0.624\n 5 Hornet Sporta…  18.7  360   3.44    17.1  1.65  0.141    2.95 0.0203    0.609\n 6 Valiant         18.1  225   3.46    19.4 -1.28  0.0441   2.96 0.00309  -0.448\n 7 Duster 360      14.3  360   3.57    16.6 -2.32  0.115    2.93 0.0309   -0.845\n 8 Merc 240D       24.4  147.  3.19    21.7  2.73  0.0964   2.92 0.0344    0.984\n 9 Merc 230        22.8  141.  3.15    21.9  0.890 0.0988   2.96 0.00378   0.322\n10 Merc 280        19.2  168.  3.44    20.5 -1.26  0.110    2.96 0.00869  -0.459\n# … with 22 more rows, and abbreviated variable name ¹​.std.resid\n\n\nNotice that it includes:\n\n.fitted = \\(X\\underline{\\hat\\beta}\\)\n.resid = \\(\\hat{\\underline\\epsilon}\\)\n.hat = \\(diag(H)\\)\n.sigma = \\(s_{(i)}\\)\n.cooksd = D\n.std.resid = standardized or studentized residuals?"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#plotting-the-residuals",
    "href": "Lb11-R_hat_resids_cook.html#plotting-the-residuals",
    "title": "15  Hat Matrix and Residuals in R",
    "section": "15.6 Plotting the residuals",
    "text": "15.6 Plotting the residuals\nIf you’ve ever accidentally typed plot(mylm), you’ve seen some plots of the residuals.\n\npar(mfrow = c(2, 2))\nplot(mylm) # ?plot.lm\n\n\n\n\nYou can access individual plots with the which argument.\n\npar(mfrow = c(2, 3))\nplot(mylm, which = 1:6)\n\n\n\n\n99% of the time, the default plots are the ones you’ll want to look at. For teaching purposes, we’ll look at a few extra."
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#demonstrations",
    "href": "Lb11-R_hat_resids_cook.html#demonstrations",
    "title": "15  Hat Matrix and Residuals in R",
    "section": "15.7 Demonstrations",
    "text": "15.7 Demonstrations\nWe’ll use the Ozone data from the mlbench package.\n\nV4 is response (measurement of Ozone)\nV5 is atmospheric pressure\nV6 is wind speed\nV7 is humidity\nWe’ll ignore the rest.\n\n\nlibrary(mlbench)\ndata(Ozone)\nstr(Ozone) # V4 is response (measurement of Ozone)\n\n'data.frame':   366 obs. of  13 variables:\n $ V1 : Factor w/ 12 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ V2 : Factor w/ 31 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ V3 : Factor w/ 7 levels \"1\",\"2\",\"3\",\"4\",..: 4 5 6 7 1 2 3 4 5 6 ...\n $ V4 : num  3 3 3 5 5 6 4 4 6 7 ...\n $ V5 : num  5480 5660 5710 5700 5760 5720 5790 5790 5700 5700 ...\n $ V6 : num  8 6 4 3 3 4 6 3 3 3 ...\n $ V7 : num  20 NA 28 37 51 69 19 25 73 59 ...\n $ V8 : num  NA 38 40 45 54 35 45 55 41 44 ...\n $ V9 : num  NA NA NA NA 45.3 ...\n $ V10: num  5000 NA 2693 590 1450 ...\n $ V11: num  -15 -14 -25 -24 25 15 -33 -28 23 -2 ...\n $ V12: num  30.6 NA 47.7 55 57 ...\n $ V13: num  200 300 250 100 60 60 100 250 120 120 ...\n\nOzone &lt;- Ozone[complete.cases(Ozone), ]\n\nA small amount of exploration first:\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(patchwork)\ng5 &lt;- ggplot(Ozone) +\n    aes(x = V5, y = V4) +\n    geom_point()\ng6 &lt;- ggplot(Ozone) +\n    aes(x = V6, y = V4) +\n    geom_jitter() # deal with overlapping points\ng7 &lt;- ggplot(Ozone) +\n    aes(x = V7, y = V4) +\n    geom_point()\ng5 + g6 + g7\n\n\n\n\nNow let’s check some residuals!\n\nChange .resid to .std.resid.\n\nTry rstudent(olm) as well.\n\nChange .hat to .cooksd.\n\n\nlibrary(dplyr)\nolm &lt;- lm(V4 ~ V5 + V6 + V7, data = Ozone)\naugment(olm) %&gt;%\n    ggplot() +\n        aes(x = .fitted, y = .std.resid, col = .cooksd) +\n        scale_colour_viridis_c(option = 2, end = 0.7) +\n        theme(legend.position = \"bottom\") +\n        geom_point(size = 2) +\n        geom_hline(yintercept = 0, colour = \"grey\")\n\n\n\n\nWhich ones have a large hat value?\nThe plots below are the same as the ones above, but coloured according to the hat values.\n\ng5 &lt;- ggplot(Ozone) +\n    aes(x = V5, y = V4, col = hatvalues(olm)) +\n    geom_point(size = 2)\ng6 &lt;- ggplot(Ozone) +\n    aes(x = V6, y = V4, col = hatvalues(olm)) +\n    geom_jitter(size = 2) # deal with overlapping points\ng7 &lt;- ggplot(Ozone) +\n    aes(x = V7, y = V4, col = hatvalues(olm)) +\n    geom_point(size = 2)\n(g5 + g6 + g7) +\n    plot_layout(guides = \"collect\") &\n    scale_colour_viridis_c(option = 2, end = 0.8) &\n    theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#adding-an-outlier",
    "href": "Lb11-R_hat_resids_cook.html#adding-an-outlier",
    "title": "15  Hat Matrix and Residuals in R",
    "section": "15.8 Adding an Outlier",
    "text": "15.8 Adding an Outlier\nLet’s add an outlier to see what happens with these data.\n\nnewzone &lt;- Ozone[, c(4:7)]\nnewzone &lt;- rbind(newzone,\n    data.frame(V4 = 30, V5 = 5300, V6 = 5, V7 = 40))\nnewlm &lt;- augment(lm(V4 ~ ., data = newzone))\n\ng5 &lt;- ggplot(newlm) +\n    aes(x = V5, y = V4, col = .hat) +\n    geom_point(size = 2)\ng6 &lt;- ggplot(newlm) +\n    aes(x = V6, y = V4, col = .hat) +\n    geom_jitter(size = 2) # deal with overlapping points\ng7 &lt;- ggplot(newlm) +\n    aes(x = V7, y = V4, col = .hat) +\n    geom_point(size = 2)\n(g5 + g6 + g7) +\n    plot_layout(guides = \"collect\") &\n    scale_colour_viridis_c(option = 2, end = 0.8) &\n    theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#using-rs-built-in-diagnostics",
    "href": "Lb11-R_hat_resids_cook.html#using-rs-built-in-diagnostics",
    "title": "15  Hat Matrix and Residuals in R",
    "section": "15.9 Using R’s Built-In Diagnostics",
    "text": "15.9 Using R’s Built-In Diagnostics\n\npar(mfrow = c(2, 2), mar = rep(2, 4))\nplot(lm(V4 ~ ., data = newzone))\n\n\n\nnewzone[row.names(newzone) %in% c(1, 58, 243), ]\n\n    V4   V5 V6 V7\n58  23 5740  3 47\n243 38 5950  5 62\n1   30 5300  5 40\n\n\n\nHuge residual!\n\nThis plot also just has a bad pattern\n\nDeviates from normality!\n\nOtherwise this looks pretty good.\n\nLarge standardized residual\n\nClear pattern without the outlier\n\nCook’s distance is massive compared to the others\n\nPotentially some large \\(D_i\\)’s\n\nIn the corner\n\nOtherwise this looks okay-ish\n\nLast plot also shows it as something different (harder to interpret)"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#what-to-do-with-a-large-residual",
    "href": "Lb11-R_hat_resids_cook.html#what-to-do-with-a-large-residual",
    "title": "15  Hat Matrix and Residuals in R",
    "section": "15.10 What to do with a large residual?",
    "text": "15.10 What to do with a large residual?\n\nMisrecorded: remove or fix, if possible\n\nFires with negative lengths (MDY versus DMY)\nCO2 measured as -99 (code for NA in a system with no NA option)\nHeights measured in the wrong units\n\nReal, but large residual: Consider whether it’s actually part of the population of interest\n\nStudying heights and got a basketball player in your sample? That’s a real data point and your model should allow for it!\nStudying fish and a shark was included? That’s real, but maybe you should narrow your scope!\n\nMany large outliers: you may need to try more predictors or a non-linear model.\n\nDo NOT remove a point simply because it’s an outlier!"
  },
  {
    "objectID": "Lb12-Corr_in_Betas.html",
    "href": "Lb12-Corr_in_Betas.html",
    "title": "17  Lab 12: Extra Topics",
    "section": "",
    "text": "n_sim &lt;- 1000\nn &lt;- 30\nbetas &lt;- matrix(ncol = 3, nrow = n_sim)\nbetacs &lt;- betas\n\nfor (i in 1:n_sim) {\n    x1 &lt;- runif(n, 0, 10)\n    x2 &lt;- runif(n, 0, 10) + 2*x1\n    y &lt;- 3 - 8*x1 + 4*x2 + rnorm(n, 0, 4)\n    betas[i, ] &lt;- coef(lm(y ~ x1 + x2))\n\n    x1c &lt;- scale(x1)\n    x2c &lt;- scale(x2)\n    betacs[i, ] &lt;- coef(lm(y ~ x1c + x2c))\n}\n\npar(mfrow = c(2, 3))\nplot(betas[, c(1,3)])\nplot(betas[, c(1,3)])\nplot(betas[, c(2,3)])\nplot(betacs[, c(1,2)])\nplot(betacs[, c(1,3)])\nplot(betacs[, c(2,3)])"
  },
  {
    "objectID": "Lb13-Wrong_Model.html#missing-predictors",
    "href": "Lb13-Wrong_Model.html#missing-predictors",
    "title": "19  Lab 13: Missing/Extra Predictors",
    "section": "19.1 Missing Predictors",
    "text": "19.1 Missing Predictors\n\nTrue model: \\(Y = X\\underline\\beta + X_2\\underline\\beta_2 + \\underline\\epsilon\\)\nEstimated model: \\(Y = X\\underline\\beta + \\underline\\epsilon\\)\n\nWe’re going to do this a little differently than other days. Let’s look at the penguins data again:\n\nlibrary(palmerpenguins)\n## Remove NAs and take continuous variables\npenguins &lt;- subset(penguins, species == \"Chinstrap\")\npeng &lt;- penguins[complete.cases(penguins), c(3, 4, 5, 6)]\n## Standardize the x values\n#peng[, 1:3] &lt;- apply(peng[, 1:3], 2, scale)\nhead(peng)\n\n# A tibble: 6 × 4\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1           46.5          17.9               192        3500\n2           50            19.5               196        3900\n3           51.3          19.2               193        3650\n4           45.4          18.7               188        3525\n5           52.7          19.8               197        3725\n6           45.2          17.8               198        3950\n\n\nLet’s “““make”“” a true model:\n\npenglm &lt;- lm(body_mass_g ~ ., data = peng)\nbeta &lt;- coef(penglm)\nsigma &lt;- summary(penglm)$sigma\n\nbeta\n\n      (Intercept)    bill_length_mm     bill_depth_mm flipper_length_mm \n      -3157.53005          16.03916          91.51275          22.57975 \n\nsigma\n\n[1] 276.9998\n\n\nWe’ll use these values as if they are population values and forget that they were calculated from a sample.\n\nThe x-values will stay the same, we’ll simulate new \\(y\\) values according to this model.\nThe advantage of this approach is that the predictors retain any correlation that they had."
  },
  {
    "objectID": "Lb13-Wrong_Model.html#simulating-from-the-right-model",
    "href": "Lb13-Wrong_Model.html#simulating-from-the-right-model",
    "title": "19  Lab 13: Missing/Extra Predictors",
    "section": "19.2 Simulating from the “Right” model",
    "text": "19.2 Simulating from the “Right” model\nLet’s forget the actual values of body_mass_g, and pretend that this is the true relationship: \\[\nbodymass = 4207 + 18*billlength + 35*billdepth + 711.5*flipperlength + \\epsilon\n\\] where \\(\\epsilon_i \\sim N(0, 393)\\).\nWe can simulate from this as follows:\n\nX &lt;- cbind(1, as.matrix(peng[, 1:3]))\nn &lt;- nrow(X)\nbody_mass_g &lt;- X %*% beta + rnorm(n, 0, sigma)\n\nunname(beta)\n\n[1] -3157.53005    16.03916    91.51275    22.57975\n\nunname(coef(lm(body_mass_g ~ -1 + X)))\n\n[1] -2973.01734    26.23170   134.33248    14.94195\n\nunname(coef(lm(body_mass_g ~ X[, -1])))\n\n[1] -2973.01734    26.23170   134.33248    14.94195\n\n\nNow let’s do this 1000s of times!\n\nres &lt;- matrix(ncol = 4, nrow = 0)\nfor (i in 1:10000) {\n    body_mass_g &lt;- X %*% beta + rnorm(n, 0, sigma)\n    right_lm &lt;- lm(body_mass_g ~ -1 + X)\n    res &lt;- rbind(res, unname(coef(right_lm)))\n}\n\ndim(res)\n\n[1] 10000     4\n\n\n\npar(mfrow = c(2, 2))\nfor(i in 1:4) {\n    hist(beta[i] - res[, i], \n        main =paste0(\"Bias = \",round(beta[i], 2), \" - \", \n            round(mean(res[, i]), 2), \" = \", \n            round(beta[i] - mean(res[, i]), 2)))\n    abline(v = 0, col = 2, lwd = 2)\n}\n\n\n\n\nThis looks good- we simulated according to the values in beta, and we were able to recover them. We’ve also shown that the linear model is unbiased!"
  },
  {
    "objectID": "Lb13-Wrong_Model.html#estimating-the-wrong-model---too-few-predictors",
    "href": "Lb13-Wrong_Model.html#estimating-the-wrong-model---too-few-predictors",
    "title": "19  Lab 13: Missing/Extra Predictors",
    "section": "19.3 Estimating the Wrong Model - Too few predictors",
    "text": "19.3 Estimating the Wrong Model - Too few predictors\nIn the following code, I remove the “flipper_length_mm” (the third predictor) by only taking the first three columns of X, which includes the column of 1s.\nI then fit the model without flipper length, which we’ve seen before is an important predictor!\n\nres_reduced &lt;- matrix(ncol = 3, nrow = 0)\nX_reduced &lt;- X[, 1:3] # Still includes column of 1s\nbeta_reduced &lt;- beta[1:3]\nfor (i in 1:10000) {\n    # Simulate from the correct model\n    body_mass_g &lt;- X %*% beta + rnorm(n, 0, sigma)\n    # Only estimate beta 0-3 (not beta4)\n    wrong_lm &lt;- lm(body_mass_g ~ -1 + X_reduced)\n    res_reduced &lt;- rbind(res_reduced, unname(coef(wrong_lm)))\n}\n\ndim(res_reduced)\n\n[1] 10000     3\n\n\n\npar(mfrow = c(2, 2))\nfor(i in 1:3) {\n    bias &lt;- beta[i] - res_reduced[, i]\n    hist(bias, \n        main =paste0(\"Bias = \",round(beta[i], 2), \" - \", \n            round(mean(res_reduced[, i]), 2), \" = \", \n            round(mean(bias), 2)),\n        xlim = range(0, bias))\n    abline(v = 0, col = 2, lwd = 2)\n}\n\n\n\n\nEverything is biased! Since flipper_length_mm was an important predictor, the estimates from the other predictors are biased!\nHere’s how I like to think of this: the machine is trying to learn a pattern using the predictors we give it. These other predictors are trying to pick up on as much pattern as possible. Without the true pattern, they have to adjust.\nA big part of this comes from the fact that there’s correlation in the predictors. Since they’re correlated, if one is missing then the others can find the pattern through their correlation. Instead of flipper_length_mm causing a change in body mass, flipper_length_mm is correlated with bill_length_mm and bill_depth_mm, which then affect body_mass_g in place of flipper_length_m’s affect. In other words, they’re trying to make up for missing patterns through the correlation, like a game of telephone where information has been lost along the way."
  },
  {
    "objectID": "Lb13-Wrong_Model.html#too-many-predictors",
    "href": "Lb13-Wrong_Model.html#too-many-predictors",
    "title": "19  Lab 13: Missing/Extra Predictors",
    "section": "19.4 Too Many Predictors",
    "text": "19.4 Too Many Predictors\nWhat happens if we include predictors that aren’t correlated with the response?\nBefore we run this code, what do you expect?\nRecall our results when \\(Y = X\\underline\\beta + X_2\\underline\\beta_2 + \\underline\\epsilon\\):\n\\[\n\\begin{align*}\nE(\\hat{\\underline\\beta}) &= E((X^TX)^{-1}X^TY)\\\\\n& = (X^TX)^{-1}X^T(X\\underline\\beta + X_2\\underline\\beta_2) \\\\\n& = (X^TX)^{-1}X^TX\\underline\\beta + (X^TX)^{-1}X^TX_2\\underline\\beta_2 \\\\\n&= \\underline\\beta+ (X^TX)^{-1}X^TX_2\\underline\\beta_2\\\\\n&= \\underline\\beta + A\\underline\\beta_2\n\\end{align*}\n\\]\nThis isn’t directly applicable, but might help you think about what happens when \\(\\underline\\beta\\) is too big.\nSince we already have the objects created, let’s pretend that X_reduced is correct.\n\nres &lt;- matrix(ncol = 4, nrow = 0)\nX_reduced &lt;- X[, 1:3] # Still includes column of 1s\nbeta_reduced &lt;- beta[1:3]\nfor (i in 1:10000) {\n    # Simulate from the correct (smaller) model\n    body_mass_g &lt;- X_reduced %*% beta_reduced + rnorm(n, 0, sigma)\n    # Estimate the wrong model\n    wrong_lm &lt;- lm(body_mass_g ~ -1 + X)\n    res &lt;- rbind(res, unname(coef(wrong_lm)))\n\n}\n\npar(mfrow = c(2, 2))\nfor(i in 1:3) {\n    bias &lt;- beta_reduced[i] - res[, i]\n    hist(bias, \n        main =paste0(\"Bias = \",round(beta[i], 2), \" - \", \n            round(mean(res[, i]), 2), \" = \", \n            round(mean(bias), 2)),\n        xlim = range(0, bias))\n    abline(v = 0, col = 2, lwd = 2)\n}\n\n\n\n\nIt’s unbiased! In this case, the estimate of \\(\\beta\\) for flipper_length_mm is 0, and it’s successfully estimating this:\n\nhist(res[, 4])"
  },
  {
    "objectID": "Lb13-Wrong_Model.html#too-many-and-too-few",
    "href": "Lb13-Wrong_Model.html#too-many-and-too-few",
    "title": "19  Lab 13: Missing/Extra Predictors",
    "section": "19.5 Too many and too few",
    "text": "19.5 Too many and too few\nSo let’s get to the final case. As we know, bill length and bill depth are correlated:\n\ncor(X[, -1]) # correlation matrix, without column of 1s\n\n                  bill_length_mm bill_depth_mm flipper_length_mm\nbill_length_mm         1.0000000     0.6535362         0.4716073\nbill_depth_mm          0.6535362     1.0000000         0.5801429\nflipper_length_mm      0.4716073     0.5801429         1.0000000\n\n\nLet’s simulate with the coefficient for bill depth as 0, but include it in the model.\n\nprint(beta)\n\n      (Intercept)    bill_length_mm     bill_depth_mm flipper_length_mm \n      -3157.53005          16.03916          91.51275          22.57975 \n\nprint(head(X))\n\n       bill_length_mm bill_depth_mm flipper_length_mm\n[1,] 1           46.5          17.9               192\n[2,] 1           50.0          19.5               196\n[3,] 1           51.3          19.2               193\n[4,] 1           45.4          18.7               188\n[5,] 1           52.7          19.8               197\n[6,] 1           45.2          17.8               198\n\n\nTo be clear:\n\nData Generating Process: body mass = \\(\\beta_0\\) + \\(\\beta_1\\) bill length + \\(\\beta_3\\) flipper length\nEstimating: body mass = \\(\\beta_0\\) + \\(\\beta_2\\) bill depth + \\(\\beta_3\\) flipper length\n\n\nres &lt;- matrix(ncol = 3, nrow = 0)\nbeta_fewmany &lt;- beta\nbeta_fewmany[3] &lt;- 0 # True coefficient for depth is 0, length != 0\nX_fewmany &lt;- X[, c(1, 3, 4)] # estimating depth, not length\n\nfor (i in 1:10000) {\n    # Simulate from the correct (smaller) model\n    body_mass_g &lt;- X %*% beta_fewmany + rnorm(n, 0, sigma)\n    # Estimate the wrong model\n    wrong_lm &lt;- lm(body_mass_g ~ -1 + X_fewmany)\n    res &lt;- rbind(res, unname(coef(wrong_lm)))\n\n}\n\npar(mfrow = c(2, 2))\nhist(res[, 1],\n    main = paste0(\"bias=\", round(beta[1], 2),\n        \"-\", round(mean(res[, 1]), 2),\n        \"=\", round(beta[1] - mean(res[, 1]), 2)))\nabline(v = beta[1], col = 2, lwd = 2)\n\nhist(res[, 2])\nabline(v = 0, col = 2, lwd = 2)\n\nhist(res[, 3],\n    main = paste0(\"bias=\", round(beta[4], 2),\n        \"-\", round(mean(res[, 3]), 2),\n        \"=\", round(beta[4] - mean(res[, 3]), 2)))\nabline(v = beta[4], col = 2, lwd = 2)\n\n\n\n\n\nIt looks like flipper length is unbiased\n\nTechnically, it isn’t, but in this case it’s a small bias.\nIf we were primarily interested in flipper length, misspecifying bill length/depth isn’t so bad.\n\nThe estimate of bill_depth isn’t 0, but also doesn’t correspond to anything in the DGP!\n\nIt’s called a “proxy measure”, and the coefficient must be interpreted carefully."
  },
  {
    "objectID": "Lb13-Wrong_Model.html#summary",
    "href": "Lb13-Wrong_Model.html#summary",
    "title": "19  Lab 13: Missing/Extra Predictors",
    "section": "19.6 Summary",
    "text": "19.6 Summary\nChoosing the right subset of predictors can be HARD!\n\nMissing predictors means your estimates are biased\nToo many predictors isn’t as bad of an issue\n\nOverfitting!\n\nThe wrong subset means no relation to DGP\n\nCan still give (nearly) unbiased estimates for predictors of interest."
  }
]