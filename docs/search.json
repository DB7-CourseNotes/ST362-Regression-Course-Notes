[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Analysis",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "Regression Analysis",
    "section": "About This Course",
    "text": "About This Course\nThis book contains the course notes for the Spring 2024 offering of ST362 Regression Analysis, based on the following sources:\n\nApplied Regression Analysis, 3rd edition, by Draper and Smith\n\nA PDF of this textbook is available through the WLU library\n\nIntroduction to Linear Regression Anaysis, 2nd edition, by Montgomery and Peck\n\nThis textbook is excellent but expensive, and I am striving to use free and OER materials.\n\nThe online course notes from Stat 501 at Penn State."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Regression Analysis",
    "section": "About This Book",
    "text": "About This Book\nThis book is a living document. Expect changes throughout each semester that I teach!\nSome features:\n\nThe GitHub logo takes you to the repo for this book. Feel free to fork and adapt as you please (under the CC BY-SA 4.0 license).\nThe little toggle next to the logo puts this into night mode. Try it out!\nEach lecture had a “Jam”, where I played music at the start of class that related to a particular slide. When that slide showed up, a student would say “That’s my Jam!” and I would throw chocolate at them.\n\nThe jams are still there, and you may wish to listen to them while reading!\n\n\nThis book is very much a work in progress. There are missing sections and typos. I am working on adding speaker notes to the slides, which will show up as text in this book.\nI am also working on a major re-write of the first few chapters to walk through the concepts in a better order. In particular, I would like to stay in simple linear regression for a lot longer, demonstrating correlation, Cook’s distance, correlation between \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\), etc., then moving into binary and categorical predictors as a first step into multiple regression, polynomial as a second step, then a lecture demonstrating that all of these concepts generalize into multiple dimensions.\nThis is a quarto book based on my lecture slides. The “Lectures” are quarto files that were rendered into beamer PDF slides. I have included the configs to render the slides. To re-create my slides, you can use the code:\nquarto render L01-Introduction.qmd\nAlternatively, you can hit Cmd-Shift-K (Ctrl-Shift-K on Linux and other operating systems) inside VSCode or Rstudio to render the slides into a presentation.\nTo render the whole book (as html), use:\nquarto render --profile book\nThe --profile argument tells Quarto to use the configuration in the file _quarto-slides.yml. I have added speaker notes in a notes environment, which means that the notes will appear in the book but not the pdf slides.\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 Unported License."
  },
  {
    "objectID": "L01-Introduction.html#introduction",
    "href": "L01-Introduction.html#introduction",
    "title": "1  Stats Review",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\n\nToday’s Learning Outcomes\n\nImportant facts about distributions.\nCIs and t-tests."
  },
  {
    "objectID": "L01-Introduction.html#distributions",
    "href": "L01-Introduction.html#distributions",
    "title": "1  Stats Review",
    "section": "1.2 Distributions",
    "text": "1.2 Distributions\n\nNormal\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{(x-\\mu)^2}{-2\\sigma^2}\\right);\\;-\\infty&lt;x&lt;\\infty\n\\]\nShiny: https://shiny.abdn.ac.uk/Stats/apps/app_normal/#section-theory\n\nCompletely determined by \\(\\mu\\) and \\(\\sigma\\).\nIf \\(X \\sim N(\\mu,\\sigma^2)\\), then \\(Z=\\frac{X-\\mu}{\\sigma} \\sim N(0,1)\\).\n\nOften called “standardizing”\n\nYou won’t need to memorize the pdf, but it’s useful!\n\n\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pnorm\")\n\nThe normal distribution is the foundation for pretty much everything that we’re going to do in this class. In general, things aren’t normally distributed, but the assumption is very robust and works out in a lot of cases.\nIn this lecture we’re going to build up an important result that we’ll use frequently. In particular, we want some background into why the F and \\(t\\) distributions show up so often!\n\n\n\nBut first, Gamma!\n\n\n\\(t\\) is based on the Gamma (\\(\\Gamma\\)) function:\n\\[\n\\Gamma(q) = \\int_0^\\infty e^{-t}t^{q-1}dt\n\\]\n\nInteresting property: \\(\\Gamma(k+1) = k!\\) for integer \\(k\\).\n\nIn general, \\(\\Gamma(q) = (q-1)\\Gamma(q-1)\\)\n\nAlso, \\(\\Gamma(1/2) = \\pi^{1/2}\\)\n\n\\(\\Gamma(3/2) = (3/2-1)\\gamma(1/2) = \\sqrt{\\pi}/2\\)\n\n\n\n\n\n\n\n\nThe \\(t\\) Distribution\n\\[\nf(x; \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu + 1}{2}\\right)}{\\sqrt{\\nu\\pi}\\Gamma(\\nu/2)}\\left(1 + \\frac{x^2}{\\nu}\\right)^{-(\\nu + 1)/2}\n\\]\n\nThe notation \\(f(x; \\nu)\\) means “function of \\(x\\), given a value of \\(\\nu\\).”\nCompletely determined by \\(\\nu\\)\nAs \\(\\nu\\rightarrow\\infty\\), this becomes \\(N(0,1)\\).\n\n\\(\\nu&gt;30\\) is pretty much normal already.\nFor \\(\\nu&lt;\\infty\\), \\(t\\) has wider tails than normal.\n\n\n\n\nThe \\(\\chi^2\\) distribution - variances\nIf \\(Z_1, Z_2, ..., Z_k\\) are iid \\(N(0,1)\\), then \\(\\chi^2_k = \\sum_{i=1}^kZ_i^2\\) has a chi-square distribution on \\(k\\) degrees of freedom.\n\\[\nf(x; k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2}\n\\]\n\nIf \\(X_i\\sim N(\\mu_i\\sigma_i)\\), then we can just standardize each first.\nAs \\(k\\rightarrow\\infty\\), \\((\\chi^2_k-k)/\\sqrt{2k}\\stackrel{d}{\\rightarrow} N(0,1)\\).\n\nThat is, for large \\(k\\) this can be approximated by a normal distribution.\n\nVery related to the variance\n\n\\((n-1)s^2/\\sigma^2\\) follows a \\(\\chi^2_n\\) distribution.\n\n\n\n\nThe \\(F\\) distribution - ratio of variances\nIf \\(S_1\\) and \\(S_2\\) are independent \\(\\chi^2\\) distributions with degrees of freedom \\(\\nu_1\\) and \\(\\nu_2\\), then \\(\\frac{S_1/\\nu_1}{S_2/\\nu_2}\\) follows an \\(F_{\\nu_1,\\nu_2}\\) distribution.\n\\[\nf(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}\n\\]\n\nIf \\(s_1^2\\) and \\(s_2^2\\) are the sample-based estimates of the true values \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\), then \\(\\frac{s_1^2/\\sigma_1^2}{s_2^2/\\sigma_2^2}\\) follows an \\(F_{\\nu_1, \\nu_2}\\) distribution.\n\nUnder \\(H_0\\), \\(\\sigma_1=\\sigma_2\\), so we don’t need the true values.\nNote: \\(s_1^2\\) is calculated from \\(S_1^2/\\nu_1\\)."
  },
  {
    "objectID": "L01-Introduction.html#confidence-intervals",
    "href": "L01-Introduction.html#confidence-intervals",
    "title": "1  Stats Review",
    "section": "1.3 Confidence Intervals",
    "text": "1.3 Confidence Intervals\n\nGeneral Idea: Terminology\nConsider the statistic \\(t = \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)}\\) (a statistic is any number that can be calculated from data alone).\n\n\\(\\theta\\) is the “““true”“” value of the parameter.\n\nUnknown, unknowable, not used in formula - hypothesize \\(\\theta = \\theta_0\\) instead.\n\n\\(\\hat\\theta\\) is our estimator of \\(\\theta\\).\n\nEstimator is a function, like \\(\\hat\\mu = \\bar X =(1/n)\\sum_{i=1}^nX_i\\).\n\n\\(X\\) is a random variable.\n\nEstimate is a number, like the calculated mean of a sample.\n\n\\(se(\\hat\\theta)\\) is the standard error of \\(\\hat\\theta\\).\n\nIf we had a different sample, the value of \\(\\hat\\theta\\) would be different. It has variance.\nStandard error: the standard deviation of the sampling distribution.\n\n\n\n\nGeneral Idea: Distributional Assumptions\nConsider the quantity (not statistic) \\(t = \\frac{\\hat\\theta - \\theta}{se(\\hat\\theta)}\\).\nIf we assume \\(X\\sim N(\\mu, \\sigma^2)\\) and \\(\\hat\\theta = \\bar X\\) is an unbiased estimate of \\(\\theta\\) on \\(\\nu=n-1\\) degrees of freedom, then \\(t \\sim t_\\nu\\)\nFrom this, we can find the lower and upper \\(\\alpha/2\\)% of the \\(t\\)$ curve.\n\n\\(t_\\nu(\\alpha/2)\\) is the lower \\(\\alpha/2\\)%.\n\ni.e., if \\(\\alpha = 0.11\\), then it’s \\(t(\\nu, 0.055)\\), the lower 5.5% area.\n\n\\(t_\\nu(1 - \\alpha/2)\\) is the upper \\(\\alpha/2\\)%.\nSince \\(t\\) is symmetric, \\(t_\\nu(1 - \\alpha/2) = -t_\\nu(\\alpha/2)\\).\n\n\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pvalues\")\n\n\n\n\nGeneral Idea: Distribution to Quantiles\nUnder the null hypothesis, \\(\\theta = \\theta_0\\), so \\[\nt = \\frac{\\hat\\theta - \\theta}{se(\\hat\\theta)} = \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)} \\sim t_\\nu\n\\]\nWe know that \\(\\bar X\\) is a random variable, and we want everything in between its 5.5% and 94.5% quantiles. \n\nThis is a confidence interval!\n\n\n\nGeneral Idea: Distribution to CI\nWe can do this easily for the \\(t_\\nu\\) distribution: we want all values \\(t_0\\) such that\n\\[\\begin{align*}\nt_\\nu(5.5) &\\le t_0 \\le t_\\nu(94.5)\\\\\nt_\\nu(5.5) &\\le \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)} \\le t_\\nu(94.5)\\\\\nse(\\hat\\theta)t_\\nu(5.5) &\\le \\hat\\theta - \\theta_0 \\le se(\\hat\\theta)t_\\nu(94.5)\\\\\n\\hat\\theta + se(\\hat\\theta)t_\\nu(5.5) &\\le \\theta_0 \\le \\hat\\theta + se(\\hat\\theta)t_\\nu(94.5)\n\\end{align*}\\]\nThe CI is all values \\(\\theta_0\\) that would not be rejected by the null hypothesis \\(\\theta = \\theta_0\\) at the \\(\\alpha\\)% level.\nSince \\(t_\\nu(1 - \\alpha/2) = -t_\\nu(\\alpha/2)\\), our 89% CI is \\(\\hat\\theta \\pm se(\\hat\\theta)t_\\nu(5.5)\\).\n\n\nWhat is the Standard Error?\nIf we’re estimating the mean, \\(\\hat\\theta = (1/n)\\sum_{i=1}^nX_i\\), where we assume \\(X_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma)\\). \\[\nE(\\hat\\theta) = E\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n}\\sum_{i=1}^nE(X_i) = \\frac{n\\mu}{n} = \\mu\n\\] From this, we see that the mean is an unbiased estimator! (This is nice, but not required.)\n\\[\nV(\\hat\\theta) = V\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n^2}V\\left(\\sum_{i=1}^nX_i\\right) \\stackrel{indep}{=} \\frac{1}{n^2}\\sum_{i=1}^nV(X_i) = \\sigma^2/n \\stackrel{plug-in}{=} s^2/n\n\\] where \\(s^2\\) is the estimate variance since we cannot know the true mean. Note that \\(s^2\\) is a biased estimator for \\(\\sigma\\).\n\n\nCI for Variance\nFrom before: \\(\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_n\\).\nLet \\(\\chi^2_n(0.055)\\) be the lower 5.5% quantile, \\(\\chi^2_n(0.945)\\) be the upper.\nFor homework, find the CI from: \\[\n\\chi^2_n(0.055) \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi^2_n(0.945)\n\\] Note that \\(\\chi^2_n(0.055)\\ne-\\chi^2_n(0.945)\\).\n\n\nSummary\n\nDistributions exist and are important\n\nMost things will be normal, which leads to \\(t\\), \\(\\chi^2\\), and \\(F\\).\n\nCIs are all values that would not be rejected by a hypothesis test.\n\nThe null hypothesis determines the distribution of the test statistic, which allows us to find the CI."
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#why-fit-models",
    "href": "L02-Fitting_Straight_Lines.html#why-fit-models",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.1 Why Fit Models?",
    "text": "2.1 Why Fit Models?\n\nThe Quote.\n“All models are wrong, some are useful.” - George Box\n\n\nAll Models Are Wrong, But Some Are Useful\nModel: A mathematical equation that explains something about the world.\n\nGravity: 9.8 \\(m/s^2\\) right?\n\nThis is a model - it’s a relationship that explains something in the world.\nVaries across the surface of the Earth.\nVaries according to air resistance.\n\nEvery additional cigarette decreases your lifespan by 11 minutes.\n\nVery, very much wrong.\nVery, very useful for communication.\n\n\n\n\nAll Linear Models are Wrong\n\n\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(palmerpenguins)\npenguins &lt;- penguins[complete.cases(penguins),]\n\nggplot(penguins) +\n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(title = \"A line fits reasonably well\",\n        x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\")\n\n\n\n\n\n\nggplot(mtcars) +\n    aes(x = disp, y = mpg) + \n    geom_point(size = 2) + \n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) + \n    geom_smooth(method = \"lm\", se = FALSE, formula = \"y~poly(x, 2)\", colour = 2) +\n    labs(title = \"A straight line misses the pattern\",\n        subtitle = \"A polynomial might fit better\",\n        x = \"Engine Displacement (1000 cu.in)\", \n        y = \"Fuel Efficiency (mpg)\")\n\n\n\n\n\n\n\n\nSome Linear Models are Useful\n\nxpred &lt;- 200\nypred &lt;- predict(\n    lm(body_mass_g ~ flipper_length_mm, data = penguins),\n        newdata = data.frame(flipper_length_mm = xpred))\n\nggplot(penguins) +\n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(title = \"The height of the line is an okay prediction for Body Mass\",\n        x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\") +\n    annotate(geom = \"point\", shape = \"*\", size = 30, colour = 2,\n        x = xpred, \n        y = ypred) + \n    annotate(geom = \"segment\", colour = 2, size = 2, \n        linetype = \"dashed\",\n        x = xpred, xend = xpred, \n        yend = -Inf, y = ypred) + \n    annotate(geom = \"segment\", colour = 2, size = 2, \n        linetype = \"dashed\",\n        x = -Inf, xend = xpred, \n        yend = ypred, y = ypred)\n\n\n\n\n\n\nA Model is an Equation\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\Leftrightarrow Y = X\\underline\\beta + \\underline\\epsilon \\Leftrightarrow E(Y|X) = X\\underline \\beta;\\; V(Y|X)=\\sigma^2\n\\]\n\nFor a one unit increase in \\(x_i\\), \\(y_i\\) increases by \\(\\beta_1\\).\n\nIf our model fits well and this is statistically significant, we can apply this to the population.\n\nThe estimated value of \\(y_i\\) when \\(x_i=0\\) is \\(\\beta_0\\).\n\nNot always interesting, but usually\\(^*\\) necessary.\n\n\nOur prediction for \\(y_i\\) at any given value of \\(x_i\\) is calculated as: \\[\n\\hat y_i = \\hat\\beta_0 + \\hat \\beta_1x_i\n\\] where the “hats” mean we’ve found estimates for the \\(\\beta\\) values.\n\n\nAside: My notation differs from the text\nI will make mistakes, but in general:\n\n\\(Y\\) is a random variable (usually a vector) representing the response.\n\n\\(Y_i\\) is also a random variable (not a vector)\n\n\\(\\underline y\\) is the response vector; the observed values of \\(Y\\)\n\\(\\underline x\\) is the vector of covariates in simple linear regression\n\\(X\\) is a matrix of covariates\n\nNot a random variable!!! Capital letters could be either random or matrix (or both). I will specify when necessary.\nI will avoid the notation \\(X_i\\).\nWhen necessary, the first column is all ones so that \\(X\\underline\\beta=\\beta_0 + \\beta_1\\underline x_1 + \\beta_1\\underline x_2 + ...\\).\n\nContext should make this clear.\n\n\n\n\n\nThe Mean of \\(Y\\) at any value of \\(X\\)\n\\[\nE(Y|X) = X\\underline\\beta\n\\]\n\n\n\nA Linear Model is Linear in the Parameters\nThe following are linear:\n\n\\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1\\sin\\left(x_i^{x_i}\\right) + \\beta_2x_i^{\\sin(x_i)} + \\epsilon_i\\)\n\nThe following are not linear:\n\n\\(y_i = \\beta_0 + \\beta_1^2x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_0\\beta_1x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\sin(\\beta_1)x_i +\\epsilon_i\\)"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#estimation",
    "href": "L02-Fitting_Straight_Lines.html#estimation",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.2 Estimation",
    "text": "2.2 Estimation\n\nGoal: Find \\(\\beta\\) values which minimize the error\nModel: \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\)\nError: \\(\\hat\\epsilon_i = y_i - \\hat y_i\\)\nWhy is this a bad way to do it?\n\n\nLeast Squares\nGoal: Minimize \\(\\sum_{i=1}^n\\hat\\epsilon_i^2=\\sum_{i=1}^n(y_i - \\hat y_i)^2\\), the sum of squared errors.\nThis ensures errors don’t cancel out. It also penalizes large errors more.\nCould we have used \\(|\\epsilon_i|\\), or some other function? \\(|ly|\\)!\n\n\nLeast Squares Estimates\nI assume that you can do the Least Squares estimate in your sleep (be ready for tests).\nThe final results are: \\[\\begin{align*}\n\\hat\\beta_0 &= \\bar y - \\hat\\beta_1\\bar x\\\\\n\\hat\\beta_1 &= \\frac{\\sum_{i-1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i-1}^n(x_i - \\bar x)^2}\\stackrel{def}{=}\\frac{S_{XY}}{S_{XX}}\n\\end{align*}\\]\nThe textbook was written in 1998 and gives formulas to make the calculation easier to do on pocket calculators. You will not need a pocket calculator for this course.\n\n\nLet’s Add Assumptions\nAssumptions allow great things, but only when they’re correct!\n\n\\(E(\\epsilon_i) = 0\\), \\(V(\\epsilon_i) = \\sigma^2\\).\n\\(cov(\\epsilon_i, \\epsilon_j) =0\\) when \\(i\\ne j\\).\n\nImplies that \\(E(y_i) = \\beta_0 + \\beta_1x_i\\) and \\(V(y_i) = \\sigma^2\\).\n\n\\(\\epsilon_i\\stackrel{iid}{\\sim} N(0,\\sigma^2)\\)\n\nThis is a strong assumption, but often works!\n\n\nInterpretation: the model looks like a line with completely random errors. (“Completely random” doesn’t mean “without structure”!)\n\n\nMean of \\(\\hat\\beta_1\\)\nIt is easy to show that \\[\n\\hat\\beta_1 = \\frac{\\sum_{i-1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i-1}^n(x_i - \\bar x)^2} = \\frac{\\sum_{i-1}^n(x_i - \\bar x)y_i}{\\sum_{i-1}^n(x_i - \\bar x)^2}\n\\] since \\(\\sum(x_i - \\bar x) = 0\\).\nHomework: show that \\(E(\\hat\\beta_1) = \\beta_1\\) (Hint: use the fact that \\(E(y_i) = \\beta_0 + \\beta_1x_i\\) and the “pocket calculator” expansions).\n\n\nVariance of \\(\\hat\\beta_1\\)\n\\[\n\\hat\\beta_1 = \\frac{\\sum_{i-1}^n(x_i - \\bar x)y_i}{\\sum_{i-1}^n(x_i - \\bar x)^2}\n\\] can be re-written as \\[\n\\hat\\beta_1 = \\sum_{i=1}^na_iy_i\\text{, where }a_i = \\frac{x_i - \\bar x}{\\sum_{i=1}^n(x_i - \\bar x)^2}\n\\]\nThus the variance of \\(\\hat\\beta_1\\) is \\[\nV(\\hat\\beta_1) = \\sum_{i=1}^na_i^2V(y_i) = ... = \\frac{\\sigma^2}{S_{XX}} \\stackrel{plug-in}{=} = \\frac{s^2}{S_{XX}}\n\\]\n\n\nConfidence Interval and Test Statistic for \\(\\hat\\beta_1\\)\nThe test statistic can be found as: \\[\nt = \\frac{\\hat\\beta_1 - \\beta_1}{se(\\hat\\beta_1)} = \\frac{(\\hat\\beta_1 - \\beta_1)\\sqrt{S_{XX}}}{s} \\sim t_\\nu\n\\]\nSince this follows a \\(t\\) distribution, we can get the CI: \\[\n\\hat\\beta_1 \\pm t_\\nu(\\alpha/2)\\sqrt{\\frac{s^2}{S_{XX}}}\n\\]"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#participation-questions",
    "href": "L02-Fitting_Straight_Lines.html#participation-questions",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.3 Participation Questions",
    "text": "2.3 Participation Questions\n\nQ1\nAll models are useful, but some are wrong.\n\nTrue\nFalse\n\n\n\nQ2\nFor a one unit increase in \\(x\\), \\(y\\) increases by\n\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(\\beta_1x\\)\n\\(\\beta_0 + \\beta_1x\\)\n\n\n\nQ3\nThe standard error of \\(\\beta_1\\) refers to:\n\nThe variance of the population slope\nThe amount by which the slope might be off\nThe variance of the estimated slope across different samples\nA big chicken. Not, like, worryingly big, but big enough that you’d be like, “Wow, that’s a big chicken!”\n\n\n\nQ4\nWhich is not an assumption that we usually make in linear regression?\n\n\\(V(\\epsilon_i) = \\sigma^2\\)\n\\(E(\\epsilon_i) = 0\\)\n\\(E(X) = 0\\)\n\\(\\epsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\)\n\n\n\nQ5\nWhich of the following is not a linear model?\n\n\\(y_i = \\beta_0 + \\beta_1^2x_i + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_0x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1\\sin\\left(x_i^{x_i}\\right) + \\beta_2x_i^{\\sin(x_i)} + \\epsilon_i\\)\n\n\n\nQ6\nThe sum of squared errors is the best way to estimate the model parameters.\n\nTrue\nFalse"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#analysis-of-variance",
    "href": "L02-Fitting_Straight_Lines.html#analysis-of-variance",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.4 Analysis of Variance",
    "text": "2.4 Analysis of Variance\n\nStatistics is the Study of Variance\n\nGiven a data set with variable \\(\\underline y\\), \\(V(\\underline y) = \\sigma^2_y\\).\n\nThis is just the variance of a single variable.\n\nOnce we’ve incorporated the linear relationship with \\(\\underline x\\), \\(V(\\hat\\beta \\underline x + \\underline\\epsilon)=0+V(\\underline\\epsilon) = \\sigma^2\\)\n\nMathematically, \\(\\sigma^2 \\le \\sigma_y^2\\).\n\n\nThe variance in \\(Y\\) is explained by \\(X\\)!\n\n\nA Useful Identity, and its Interpretations\n\\[\\begin{align*}\n\\hat\\epsilon_i &= y_i - \\hat y_i \\\\&= y_i - \\bar y - (\\hat y_i - \\bar y)\\\\\n\\implies \\sum_{i=1}^n\\hat\\epsilon_i^2 &= \\sum_{i=1}^n(y_i-\\bar y)^2 - \\sum_{i=1}^n(\\hat y_i - \\bar y)^2\n\\end{align*}\\] where we’ve simply added and subtracted \\(\\bar y\\). The final line skips a few steps (try them yourself)!\nThe last line is often written as: \\(SS_E = SS_T - SS_{Reg}\\)\n\n\nSums of Squares\n\\[\nSS_E = SS_T - SS_{Reg}\n\\]\n\n\\(SS_E\\): Sum of Squared Errors\n\\(SS_T\\): Sum of Squares Total (i.e., without \\(\\underline x\\))\n\\(SS_{Reg}\\): Sum of Squares due to the regression.\n\nIt’s the variance of the line (calculated at observed \\(\\underline x\\) values) around the mean of \\(\\underline y\\)???\n\nThis is incredibly useful, but weird.\n\nI use \\(SS_{Reg}\\) instead of \\(SS_R\\) because some textbooks use \\(SS_R\\) as the Sume of Squared Residuals, which is confusing.\n\n\n\n\nAside: Degrees of Freedom\nDef: The number of “pieces of information” from \\(y_1, y_2, ..., y_n\\) to construct a new number.\n\nIf I have \\(x = (1,3,2,1,3,???)\\) and I know that \\(\\bar x = 2\\), I can recover the missing piece.\n\nThe mean “uses” (accounts for) one degree of freedom\n\nIf I have \\(x = (1,2,3,1,???,???)\\) and I know \\(\\bar x = 2\\) and \\(s_x^2=1\\), I can recover the two missing pieces.\n\nThe variance accounts for two degrees of freedom.\n\nOne \\(df\\) is required to compute it.\n\n\n\nEstimating one parameter takes away one degree of freedom for the rest!\n\nCan find \\(\\bar x\\) when \\(x = (1)\\), but can’t find \\(s_x^2\\) because there aren’t enough \\(df\\)!\n\n\n\nSums of Squares\n\n\n\n\n\n\n\n\n\nSource of Variation\nDegress of Freedom \\(df\\)\nSum of Squares (\\(SS\\))\nMean Square (\\(MS\\))\n\n\n\n\nRegression\n1\n\\(\\sum_{i=1}^n(\\hat y_i - \\bar y)^2\\)\n\\(MS_{Reg}\\)\n\n\nError\n\\(n-2\\)\n\\(\\sum_{i=1}^n(y_i - \\hat y_i)^2\\)\n\\(MS_E=s^2\\)\n\n\nTotal\n\\(n-1\\)\n\\(\\sum_{i=1}^n(y_i - \\bar y)^2\\)\n\\(s_y^2\\)\n\n\n\n\nNotice that \\(SS_T = SS_{Reg} + SS_E\\), which is also true for the \\(df\\) (but not \\(MS\\)). \nWhy is \\(df_E = n-2\\)? What two parameters have we estimated?\n\n\\(df_{Reg}\\) is trickier to explain. It suffices to know that \\(df_{Reg} = df_T-df_E\\).\n\n\n\n\nUsing Sums/Means of Squares\n\nIf \\(\\hat y_i = \\bar y\\) for all \\(i\\), then we have a horizontal line!\n\nThat is, there is no relationship between \\(\\underline x\\) and $\\underline \\(y\\).\nIn this case, \\(SS_{reg} = \\sum_{i=1}^n(\\hat y_i - \\bar y)^2 = 0\\).\n\n\nOkay, so, just test for \\(SS_{Reg} = 0\\)?\nBut how??? We need some measure of how far from 0 is statistically significant!!!\nRecall that \\(SS_E = \\sum_{i=1}^n(y_i - \\hat y_i)^2\\).\n\nWe can compare the variation around the line to the variation of the line.\n\nThis is \\(MS_{Reg}/MS_E\\), and it follows an \\(F\\) distribution!!\n\n\n\n\nThe F-test for Significance of Regression\n\\[\nF_{df_{Reg}, df_E} = \\dfrac{MS_{Reg}}{MS_E} = \\dfrac{MS_{Reg}}{s^2}\n\\]\n\nRecall that \\(SS_{Reg} = SS_E + SS_T &gt; SS_E\\), but the \\(df\\) make a difference.\nFor homework, show that \\(E(MS_{Reg}) = \\sigma^2 + \\beta_1^2\\sum_{i=1}^n(x_i-\\bar x)^2\\)\nThis implies that \\(E(MS_{Reg}) &gt; E(MS_E) = \\sigma^2\\).\n\n\n\nExercise A from Chapter 3 (Ch03 Exercises cover Ch1-3)\n\n\nA study was made on the effect of temperature on the yield of a chemical process. The data are shown to the right.\n\nAssuming \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\), what are the least squares estimates of \\(\\beta_0\\) and \\(\\beta_1\\)?\nConstruct the analysis of variance table and test the hypothesis \\(H_0: \\beta_1=0\\) at the 0.05 level.\nWhat are the confidence limits (at = 0.05) for \\(\\beta_1\\)?\n\n\n\nlibrary(aprean3) # Data from textbook\nhead(dse03a) # Data Set Exercise Ch03A\n\n   x  y\n1 -5  1\n2 -4  5\n3 -3  4\n4 -2  7\n5 -1 10\n6  0  8\n\nnrow(dse03a)\n\n[1] 11\n\n\n\n\n\nGood textbook questions: A, K, O, P, T, U, X (using data(anscombe) in R), Z, AA, CC. Note that all data sets in the textbook are available in an R package found here.\nPlease see Lab 2."
  },
  {
    "objectID": "L03-Residuals.html#the-residual-whats-left-over",
    "href": "L03-Residuals.html#the-residual-whats-left-over",
    "title": "3  L03: Assessing Fit",
    "section": "3.1 The residual: what’s left over",
    "text": "3.1 The residual: what’s left over\n\n\\(R^2\\): percent of variance explained by the regression model\n\n\n\\[\nR^2 = \\frac{SSReg}{SST} = \\frac{S_{XY}^2}{S_{XX}S_{YY}}\n\\]\n\n\nlayout(mat = matrix(c(1,2,3), nrow = 1), widths = c(0.5,1,1))\nset.seed(18)\nx &lt;- runif(25, 0, 10)\ny &lt;- rnorm(25, 2 + 5*x, 6)\n\nplot(rep(1, 25), y, xlab = \"y\", ylab = \"y has variance\", xaxt = \"n\")\nabline(h = mean(y))\naxis(2, mean(y), bquote(bar(y)), las = 1)\n\nplot(x, y, ylab = \"There's variance around the line\")\nabline(lm(y~x))\nabline(h = mean(y))\naxis(2, mean(y), bquote(bar(y)), las = 1)\n\nmids &lt;- predict(lm(y~x))\nfor(i in seq_along(mids)){\n    lines(x = rep(x[i], 2), y = c(y[i], mids[i]), col = 1)\n}\n\nmids &lt;- predict(lm(y~x))\nplot(mids ~ x, type = \"n\", ylab = \"The line varies around the mean of y!\")\nfor(i in seq_along(mids)){\n    lines(x = rep(x[i], 2), y = c(mean(y), mids[i]))\n}\naxis(2, at = mean(y), labels = bquote(bar(y)), las = 1)\nabline(h = mean(y))\n\n\n\n\n\n\n\n\nResidual Assumptions\n\nResidual: what’s left over\n\n\\(\\hat\\epsilon_i = y_i - \\hat y_i\\)\n\nAssumptions (from before):\n\n\\(E(\\epsilon_i) = 0\\)\n\\(V(\\epsilon_i) = \\sigma^2\\)\n\\(\\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n\nWe must check our assumptions!\n\nThere are statistical tests, but they’ll never tell you as much as a plot!\n\n\n\nResiduals versus fitted values: \\(\\hat{\\underline\\epsilon}\\) versus \\(\\hat{\\underline{y}}\\)\n\n\nWhy \\(\\hat{\\underline{y}}\\) instead of \\(\\underline y\\)?\n\nSee text. Try a regression of \\(\\hat{\\underline\\epsilon}\\) versus \\(\\underline{y}\\) yourself (mathematically and with code).\n\nWhy not \\(\\underline x\\)?\n\nFor simple linear regression, \\(\\hat{\\underline{y}}\\) is like a unit change for \\(\\underline x\\), so it doesn’t matter.\n\nFor multiple linear regression, it’s easier to have one variable for the \\(x\\) axis.\n\n\n\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(patchwork)\nlibrary(broom)\nlibrary(palmerpenguins)\n\npenguins &lt;- penguins[complete.cases(penguins),]\n\ng1 &lt;- ggplot(penguins) + \n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\",\n        title = \"y versus x\")\n\nplm &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\np2 &lt;- augment(plm)\n\ng2 &lt;- ggplot(p2) + \n    aes(x = .fitted, y = .resid) + \n    geom_point() +\n    geom_hline(yintercept = 0, col = \"grey\") +\n    labs(x = \"Fitted\", y = \"Residuals\", title = \"Residuals versus Fitted\")\n\n\ng1 / g2\n\n\n\n\n\n\n\n\nResidual Plots and Assumption Checking\nMathematics is the process of making assumptions and seeing if we can break them.\n\n\\(E(\\epsilon_i) = 0\\) is a given since \\(\\sum_{i=1}^n\\hat\\epsilon_i=0\\).\n\\(V(\\epsilon_i) = \\sigma^2\\)\n\nCheck if the variance looks stable.\n\n\\(\\epsilon_i \\sim N(0,\\sigma^2)\\) is harder to see\n\nExpect more points close to 0, fewer further away, no outliers\n\n\n\n\nResidual plots: unstable error\n\nx &lt;- runif(200, 0, 10)\ny0 &lt;- 2 - 3*x\ny &lt;- y0 + rnorm(length(x), 0, 2 * x)\n\ng1 &lt;- ggplot() + \n    aes(x = x, y = y) + \n    geom_point() + \n    geom_smooth(se = FALSE, method = \"lm\", formula = y~x) +\n    labs(x = NULL, y = NULL, title = \"y versus x\")\n\nxydf &lt;- augment(lm(y ~ x))\n\ng2 &lt;- ggplot(xydf) +\n    aes(x = .fitted, y = .resid) + \n    geom_point() +\n    geom_hline(yintercept = 0, col = \"grey\") +\n    labs(x = \"Fitted\", y = \"Residuals\", title = \"Residuals versus Fitted\")\n\ng1 + g2\n\n\n\n\n\n\nResidual plots: non-linear trend?\n\n## fig-height: 4\n## fig-width: 8\ng1 &lt;- ggplot(mtcars) + \n    aes(x = disp, y = mpg) + \n    geom_point() + \n    geom_smooth(se = FALSE, method = \"lm\", formula = y~x) +\n    labs(x = \"Engine Displacement\", y = \"Miles per Gallon\", title = \"y versus x\")\n\nmtdf &lt;- augment(lm(mpg ~ disp, data = mtcars))\n\ng2 &lt;- ggplot(mtdf) +\n    aes(x = .fitted, y = .resid) + \n    geom_point() +\n    geom_smooth(se = FALSE) +\n    geom_hline(yintercept = 0, col = \"grey\") +\n    labs(x = \"Fitted\", y = \"Residuals\", title = \"Residuals versus Fitted - non-linear trend?\")\n\ng1 + g2\n\n\n\n\n\n\nTesting Normality: Quantile-Quantile Plots\n\n\nConsider the data (2,3,3,4,5,5,6).\n\n50% of the data is below the median.\n\nFor a \\(N(0,1)\\) distribution, 50% of the data is below 0.\nPut the median on the y axis, 0 on the x axis.\n\n25% of the data is below Q1.\n\nFor a \\(N(0,1)\\) distribution, 25% is below qnorm(0.25) = -0.67\nPut a point at x = -0.67, y = Q1.\n\n75% of the data is below Q3.\n\nFor a \\(N(0,1)\\) distribution, 75% is below qnorm(0.75) = 0.67\nPut a point at x = 0.67, y = Q3.\n\n… and so on for the rest of the quantiles\n\n\nIf perfectly normal, expect a straight line!\n\nmydata &lt;- c(2, 3, 3, 4, 5, 5, 6)\nquants &lt;- qnorm(c(\n    0.125, 0.25, 0.375, 0.5, \n    0.625, 0.75, 0.875))\nplot(mydata ~ quants)\n\n\n\n\n\n\n\n\nOther Residual Plots:\nScale-Location\n\nScale: Standardized residual\nLocation: Fitted value\nMore on standardized residuals in Ch08\n\nCook’s Distance\n\nBasically, an outlier detection method.\nMore in Ch08\n\nLeverage\n\nMore in Ch08"
  },
  {
    "objectID": "L03-Residuals.html#participation-quiz",
    "href": "L03-Residuals.html#participation-quiz",
    "title": "3  L03: Assessing Fit",
    "section": "3.2 Participation Quiz",
    "text": "3.2 Participation Quiz\n\nQ01\nWhich of the following is not an assumption of linear models?\n\n$$\nThe variance is constant across all values of \\(X\\).\nThe height of the line is the mean value of \\(Y\\) for a given \\(X\\).\nNone of the above.\n\n\n\nQ02\nWhich of the following is a confidence interval for \\(s\\)?\n\n\\(\\chi^2_n(0.055) \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi^2_n(0.945)\\)\n\\(\\chi^2_n(0.945) \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi^2_n(0.055)\\)\n\\(s^2 \\pm \\text{Critical Value } * \\text{ se}(s^2)\\)\n\\(s \\pm \\text{Critical Value } * \\text{ se}(s)\\)\nNone of the above\n\n\n\nQ03\nWhich of the following is not a random variable?\n\n\\(Y\\)\n\\(\\hat\\epsilon_i\\)\n\\(\\hat Y\\)\n\\(\\underline\\epsilon\\)\n\n\n\nQ04\n\\(F = t^2\\)\n\nTrue\nFalse\nSometimes true\n\n\n\nQ05\nWhich of the following is the definition of a residual?\n\n\\(y_i - \\hat y_i\\)\n\\((y_i - \\hat y_i)^2\\)\n\\(\\hat y_i - y_i\\)\n\\((\\hat y_i - y_i)^2\\)\n\n\n\nQ06\nWhich of the following statements about \\(R^2\\) is false?\n\n\\(R^2 = SSReg / SST\\)\n\\(R^2 = r^2\\), where \\(r\\) is the correlation between \\(\\underline x\\) and \\(\\underline y\\)\n\\(R^2\\) compares the variance of the line to the variance of \\(y\\) alone\n\\(R^2\\) is not a random variable"
  },
  {
    "objectID": "L03-Residuals.html#maximum-likelihood",
    "href": "L03-Residuals.html#maximum-likelihood",
    "title": "3  L03: Assessing Fit",
    "section": "3.3 Maximum Likelihood",
    "text": "3.3 Maximum Likelihood\n\nMain Idea\nFind the values \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\), and \\(\\hat\\sigma^2\\) that maximize the likelihood of seeing our data.\nUnder the assumptions that \\(X\\) is fixed, \\(Y = \\beta_0 + \\beta_1X + \\epsilon_i\\), and \\(\\epsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\), \\[\nY \\sim N(\\beta_0 + \\beta_1X, \\sigma^2)\n\\]\n\n\nThe Likelihood\nThe probability of observering a data point is: \\[\nf_Y(y_i|x_i, \\beta_0, \\beta_1, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1x_i)^2\\right)\n\\]\nThe likelihood of the parameters, given the data, is: \\[\nL(\\beta_0, \\beta_1, \\sigma^2|x_i, y_i) = \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1x_i)^2\\right)\n\\]\n\nIt’s just a shift in perspective!\n\n\n\nSimple Coin Flip Example\nSuppose we flipped 10 bottle caps and got 6 “crowns”. Assume the probability of “crown” (\\(C\\)) is unknown, labelled \\(p\\).\n\nThe probability of one cap flip is \\(P(C|p) = p\\).\nThe probability of this is \\(P(C = 6|p) = p^6(1-p)^4\\).\n\nThis is just \\(P(\\underline y|p) = \\prod_{i=1}^nP(Y = y_i)\\).\n\nThe likelihood is \\(L(p|\\underline y) = \\prod_{i=1}^nP(Y = y_i)\\).\n\n\n\nMaximizing the Likelihood in LM\n\\[\nL(\\beta_0, \\beta_, \\sigma^2) = \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1x_i)^2\\right)\n\\]\n\nTo maximize w.r.t \\(\\beta_0\\), we set the derivative w.r.t \\(\\beta_0\\) to 0 and solve for \\(\\beta_0\\).\n\n\\(\\frac{\\partial L(\\beta_0, \\beta_, \\sigma^2)}{\\partial \\beta_0} = 0\\).\n\nRepeat for \\(\\frac{\\partial L(\\beta_0, \\beta_, \\sigma^2)}{\\partial \\beta_1} = 0\\) and \\(\\frac{\\partial L(\\beta_0, \\beta_, \\sigma^2)}{\\partial \\sigma^2} = 0\\)\n\nHWK: Show that the estimates for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are the same as the OLS estimates. The estimate for \\(\\hat\\sigma^2\\) should come out to: \\[\n\\hat\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n\\left(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i\\right)^2\n\\]\n\n\nBias-Variance Decomposition\nOn the next slide, we’ll show that \\(E\\left((Y_i - \\hat Y_i)^2\\right)\\) can be decomposed into \\(V(\\epsilon_i)\\), squared bias, and the variance of the fitted model.\n\n\\(V(\\epsilon_i)\\) is the variance of the true errors.\nBias is the difference between the true model and the estimated one.\n\nSystematic difference, not just random errors\n\ne.g. fitting a linear model to a non-linear trend\n\n\nVariance of the fitted model across all possible samples\n\nNote that this is a slight deviation from how the text presents it.\n\n\nDerivation\nSuppose the true value is \\(Y_i = g(x_i) + \\epsilon_i\\) (not necessarily linear), where \\(E(\\epsilon_i) = 0\\), \\(E(Y_i) = g(x_i)\\), and \\(V(\\epsilon_i) = \\sigma^2\\). \\[\\begin{align*}\nE\\left((Y_i - \\hat Y_i)^2\\right) &= E(Y_i - 2Y_i\\hat Y_i + \\hat Y_i^2)\\\\\n&= E(Y_i^2) - 2E(Y_i\\hat Y_i) + E(\\hat Y_i^2)\\\\\n&= E((g(x_i) + \\epsilon)^2) - 2E(g(x_i) + \\epsilon_i)E(\\hat Y_i) + E(\\hat Y_i^2)\\\\\n&= g^2(x_i) + 2g(x_i)E(\\epsilon_i) + E(\\epsilon_i^2) - 2g(x_i)E(\\hat Y_i) + E(\\hat Y_i^2)\\\\\n&= g^2(x_i) + \\sigma^2 - 2g(x_i)E(\\hat Y_i) + E(\\hat Y_i^2) + E(\\hat Y_i)^2 - E(\\hat Y_i)^2\\\\\n&= \\sigma^2 + (g(x_i) - E(\\hat Y_i))^2 + V(\\hat Y_i)\\\\\n&= \\text{Error Variance} + \\text{Bias}^2 + \\text{Fitted Model Variance}\n\\end{align*}\\]\n\n\nNot Interpreting the MSE\n\\[\nE((Y_i - \\hat{Y_i})^2) = \\sigma^2 + (g(x_i) - E(\\hat Y_i))^2 + V(\\hat Y_i)\n\\]\n\nWe don’t know any of the numbers on the right!!!\n\nThe decomposition is theoretical, we can’t tease apart \\(s^2\\) into these terms.\n\n\nInterpreting the MSE\nThe basic question of statistics: “How big is this number?”\n\nCompare to previous studies - is MSE larger than \\(\\sigma^2\\)?\n\nImplies that Bias\\(^2\\) and Fitted Model Variance are larger than expected.\nF-test\n\nCompare to “pure error” - direct estimate of \\(\\sigma^2\\).\n\ni.e. the variance in repeated trials on the same covariate values\nTextbooks devotes a lot to this, but it’s often not plausible.\n\nWon’t be on tests!\n\n\nCompare to another model\n\nWe’ll focus on this (later)!\n\n\n\n\nCompare to Previous Studies\nHypothesis test for \\(\\sigma^2 = \\sigma_0^2\\) versus \\(\\sigma^2 &gt; \\sigma_0^2\\), where \\(\\sigma_0\\) is the value from a previous study.\n\nIf significant, some of your error is coming from the study design!\n\n\n\nCompare to other models\nIt can be shown that \\(E(MSReg) = \\sigma^2 + \\beta_1S_{XX}\\).\nConsider the Null hypothesis \\(\\beta_1 =0\\) (why is this a good null?).\n\nUnder this null, \\(\\frac{MSReg}{s^2}\\sim F_{1, n-2}\\).\n\nObvious CI from this. \n\nThis is equivalent to the t-test for \\(\\beta_1\\)! (See text.)\n\n\n\nMSE of a Parameter: Bias of \\(s^2\\)\nFrom a previous class, we know that \\[\n\\frac{(n-2)s^2}{\\sigma^2}\\sim\\chi^2_{n-2}\n\\]\nFrom wikipedia, we know that the mean of a \\(\\chi^2_k\\) distribution is \\(k\\). Therefore, \\[\nE\\left(\\frac{(n-2)s^2}{\\sigma^2}\\right) = n-2 \\Leftrightarrow E(s^2) = \\sigma^2\n\\] and thus \\(s^2\\) is unbiased.\nThis does not necessarily mean that \\(s^2\\) is the best estimator for \\(\\sigma^2\\)!\n\n\nMSE of a Parameter: Bias of \\(s\\)\nEven though \\(s^2\\) is an unbiased estimator, \\(s = \\sqrt{s^2}\\) is biased! Specifically, \\(E(s) &lt; \\sigma\\)\nTo see why, first note that \\[\nV(s) = E(s^2) - (E(s))^2 \\Leftrightarrow E(s) = \\sqrt{E(s^2) - V(s)}\n\\] since \\(V(s) &gt; 0\\), \\(E(s^2) - V(s) &lt; E(s^2)\\), and therefore \\[\nE(s) &lt; \\sqrt{E(s^2)} = \\sqrt{\\sigma^2} = \\sigma\n\\]"
  },
  {
    "objectID": "L04-Matrix_Form.html#chapter-04-summary",
    "href": "L04-Matrix_Form.html#chapter-04-summary",
    "title": "4  L04: Regression in Matrix Form",
    "section": "4.1 Chapter 04 Summary",
    "text": "4.1 Chapter 04 Summary\n\nMatrix Forms of Things We’ve Seen\nUsing\n\\[\nY = \\begin{bmatrix}Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_n\\end{bmatrix};\\quad \\underline y = \\begin{bmatrix}y_1\\\\ y_2\\\\ \\vdots\\\\ y_n\\end{bmatrix};\\quad X = \\begin{bmatrix}1 & x_1\\\\1 &x_2\\\\ \\vdots & \\vdots\\\\ 1 & x_n\\end{bmatrix};\\quad \\underline\\beta = \\begin{bmatrix}\\beta_0\\\\\\beta_1\\end{bmatrix};\\quad\\underline\\epsilon \\begin{bmatrix}\\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots\\\\ \\epsilon_n\\end{bmatrix}\n\\]\n\\(Y = X\\underline\\beta + \\underline\\epsilon\\) is the same as \\[\\begin{align*}\nY_1 &= \\beta_0 + \\beta_1x_1 + \\epsilon_1\\\\\nY_2 &= \\beta_0 + \\beta_1x_2 + \\epsilon_2\\\\\n&\\vdots\\\\\nY_n &= \\beta_0 + \\beta_1x_n + \\epsilon_n\\\\\n\\end{align*}\\]\n\n\nSome Fun Matrix Forms\n\\[\\begin{align*}\n\\underline\\epsilon^T\\underline\\epsilon &= \\epsilon_1^2 + \\epsilon_2^2 + ... + \\epsilon_n^2 = \\sum_{i=1}^n\\epsilon_i^2\\\\\nY^TY &= \\sum Y_i^2\\\\\n\\mathbf{1}^T\\underline y &= \\sum y_i = n\\bar y\\\\\nX^TX &= \\begin{bmatrix}n & \\sum x_i\\\\ \\sum x_i & \\sum x_i^2\\end{bmatrix}\\\\\n(X^TX)^{-1} &= \\frac{1}{nS_{XX}}\\begin{bmatrix}\\sum x_i^2 & -n\\bar x\\\\-n \\bar x & n\\end{bmatrix}\\\\\nX^TY &= \\begin{bmatrix}\\sum Y_i\\\\ \\sum x_iY_i\\end{bmatrix}\n\\end{align*}\\]\n\n\nThe “Normal” Equations\n\n\nCopied from previous slide: \\[\\begin{align*}\nX^TX &= \\begin{bmatrix}n & \\sum x_i\\\\ \\sum x_i & \\sum x_i^2\\end{bmatrix}\\\\\nX^T\\underline y &= \\begin{bmatrix}\\sum y_i\\\\ \\sum x_iy_i\\end{bmatrix}\n\\end{align*}\\]\n\nThe textbook included the following equations in Ch02. The estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) in OLS (same as MLE) are the solution to:\n\\[\\begin{align*}\n\\hat\\beta_0 + \\hat\\beta_1\\sum x_i &= \\sum y_i\\\\\n\\hat\\beta_0\\sum x_i + \\hat\\beta_1\\sum x_i^2 &= \\sum x_iy_i\\\\\n\\end{align*}\\]\n\n\nPutting these together: \\[\nX^TX\\hat{\\underline\\beta} = X^T\\underline y\\quad \\Leftrightarrow\\quad \\hat{\\underline\\beta} = (X^TX)^{-1}X^T\\underline y\n\\] Try this out using the definition of \\((X^TX)^{-1}\\) on the previous slide.\n\n\n(Corrected) ANOVA in Matrix Form\n\n\n\n\n\n\n\n\n\nSource\n\\(df\\)\n\\(SS\\)\n\\(MS\\)\n\n\n\n\nRegression (corrected)\n1\n\\(\\hat{\\underline\\beta}^TX^T\\underline y - n\\bar{\\underline y}^2\\)\n\\(SS/df\\)\n\n\nError\n\\(n-2\\)\n\\(\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y\\)\n\\(SS/df\\)\n\n\nTotal (corrected)\n\\(n - 1\\)\n\\(\\underline y^t\\underline y - n\\bar{\\underline y}^2\\)\n\\(SS/df\\)\n\n\n\n\nThe “corrected” ANOVA is the ANOVA table for comparing the errors due to \\(\\hat\\beta_1\\) to the errors due to \\(\\hat\\beta_0\\).\nThis is different from comparing a model with \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) to a model with neither parameter.\n\nThe value of the “correction” is based on \\(\\bar x\\). If the slope is 0, then the estimate for \\(\\hat\\beta_0\\) is \\(\\bar x\\)."
  },
  {
    "objectID": "L04-Matrix_Form.html#variances",
    "href": "L04-Matrix_Form.html#variances",
    "title": "4  L04: Regression in Matrix Form",
    "section": "4.2 Variances",
    "text": "4.2 Variances\n\nVariance of a Vector: the Variance-Covariance Matrix\nIn general, for vector-valued random variable \\(Z = (Z_1, Z_2, \\dots, Z_n)\\), \\[\nV(Z) = \\begin{bmatrix}\nV(Z_1) & cov(Z_1, Z_2) & \\cdots & cov(Z_1, Z_n) \\\\\ncov(Z_2, Z_1) & V(Z_2) & \\cdots &  cov(Z_2, Z_n)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\ncov(Z_n, Z_1) & cov(Z_n, Z_2) & \\cdots & V(Z_n)\n\\end{bmatrix}\n\\]\n\n\n\\(V(\\hat{\\underline\\beta})\\)\n\n\nLet’s start with the covariance of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\): \\[\\begin{align*}\ncov(\\hat\\beta_0, \\hat\\beta_1) &= cov(\\bar y - \\hat\\beta_1\\bar x, \\hat\\beta_1)\\\\\n&= -\\bar x cov(\\hat\\beta_1, \\hat\\beta_1)\\\\\n&= -\\bar x V(\\hat\\beta_1)\\\\\n&= \\frac{-\\sigma^2\\bar x}{S_{XX}}\n\\end{align*}\\]\n\nThe var-covar matrix is: \\[\\begin{align*}\nV(\\hat{\\underline\\beta}) &= \\begin{bmatrix}\nV(\\hat\\beta_0) & cov(\\hat\\beta_0, \\hat\\beta_1)\\\\\ncov(\\hat\\beta_0, \\hat\\beta_1) & V(\\hat\\beta_1)\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\frac{\\sigma^2\\sum x_i^2}{S_{XX}} & \\frac{-\\sigma^2\\bar x}{S_{XX}}\\\\\n\\frac{-\\sigma^2\\bar x}{nS_{XX}} & \\frac{\\sigma^2}{S_{XX}}\n\\end{bmatrix}\\\\\n&= \\frac{\\sigma^2}{nS_{XX}}\\begin{bmatrix}\n\\sum x_i^2 & -n\\bar x\\\\\n-n\\bar x & n\n\\end{bmatrix}\\\\\n&= (X^TX)^{-1}\\sigma^2\n\\end{align*}\\]\n\n\n\n\nVariance of \\(\\hat Y_0\\)\nLet \\(X_0 = [1, x_0]\\) be an arbitrary observation. Then the predicted value of the line at \\(X_0\\), labelled \\(\\hat Y_0\\), is: \\[\\begin{align*}\nV(\\hat Y_0) &= V(X_0\\hat{\\underline\\beta})\\\\\n&= X_0V(\\hat{\\underline\\beta})X_0^T\\\\\n&= \\sigma^2X_0(X^TX)^{-1}X_0\n\\end{align*}\\]\nHWK: Verify that \\(X_0(X^TX)^{-1}X_0\\) is a \\(1\\times 1\\) matrix.\n\n\nVariance of \\(\\hat Y_{n+1}\\)\nFor a new observation, we have the variance of the line plus a new unobserved error \\(\\epsilon_{n+1}\\). \\[\\begin{align*}\nV(\\hat Y_{n+1}) &= V(X_{n+1}\\hat{\\underline\\beta} + \\epsilon_{n+1})\\\\\n&= X_{n+1}V(\\hat{\\underline\\beta})X_{n+1}^T + V(\\epsilon_{n+1})\\\\\n&= \\sigma^2X_{n+1}(X^TX)^{-1}X_{n+1}^T + \\sigma^2\\\\\n&= \\sigma^2\\left(X_{n+1}(X^TX)^{-1}X_{n+1}^T + 1\\right)\n\\end{align*}\\]\nHWK: Verify (empirically or mathematically) that this is smallest when \\(x_0 = \\bar x\\).\n\n\nSummary\nWhen we have one predictor, it is clear that: \\[\\begin{align*}\nY &= X\\hat{\\underline\\beta} + \\underline\\epsilon\\\\\n\\hat{\\underline\\beta} &= (X^TX)^{-1}X^T\\underline y\\\\\nV(\\hat{\\underline\\beta}) &= (X^TX)^{-1}\\sigma^2\\\\\nV(\\hat Y_0) &= \\sigma^2X_0(X^TX)^{-1}X_0^T\\\\\nV(\\hat Y_{n+1}) &= \\sigma^2(X_{n+1}(X^TX)^{-1}X_{n+1}^T + 1)\\\\\n\\end{align*}\\]\nThis will not change when we add predictors!"
  },
  {
    "objectID": "L04-Matrix_Form.html#participation-questions",
    "href": "L04-Matrix_Form.html#participation-questions",
    "title": "4  L04: Regression in Matrix Form",
    "section": "4.3 Participation Questions",
    "text": "4.3 Participation Questions\n\nQ01\nWhich statement about matrix multiplication is false?\n\n\\((AB)^T = B^TA^T\\)\n\\((AB)^{-1} = B^{-1}A^{-1}\\)\n\\((A + B)^T = A^T + B^T\\)\n\\(A^{-1}A = AA^{-1}\\)\n\n\n\nQ02\nThe Normal Equations come from:\n\nSetting the partial derivatives of \\(\\underline\\epsilon^T\\underline\\epsilon\\) to 0.\nSetting the partial derivatives of \\(\\hat{\\underline\\epsilon}^T\\hat{\\underline\\epsilon}\\) to 0.\nRearranging the equation \\(\\sum_{i=1}^n(y_i - \\hat y_i)^2\\) for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)\nRearranging the equation \\(\\sum_{i=1}^n(y_i - \\beta_0 + \\beta_1x_i + \\epsilon_i)^2\\) for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)\n\n\n\nQ03\n\\(MS_E = MS_T - MS_Reg\\)\n\nTrue\nFalse\n\n\n\nQ04\nFor vector-valued r.v. \\(Z\\), \\(V(Z)\\) is a symmetric matrix.\n\nNo, since the \\(i,j\\) element is \\(cov(Z_i, Z_j)\\) and the \\(j,i\\) element is \\(cov(Z_j, Z_i)\\).\nNo, since the \\(i,i\\) element is \\(V(Z_i)\\), which is different for different \\(i\\).\nYes, because it is a scalar.\nYes, because \\(cov(Z_i, Z_j) = cov(Z_j, Z_i)\\)\n\n\n\nQ05\nFor scalar-valued random variables \\(X\\) and \\(Y\\), which statement is true?\n\n\\(cov(a + bX, Y) = b cov(X,Y)\\)\n\\(cov(a + bX, bY) = b cov(X,Y)\\)\n\\(cov(bX, X) = b^2V(X)\\)\n\\(cov(a + bX, a + bY) = a + b cov(X,Y)\\)\n\n\n\nQ06\nThe variance of the line evaluated at an existing point is smaller than the variance of the line evaluated at a new point, even if the new point is the same as an existing point.\n\nTrue\nFalse"
  },
  {
    "objectID": "L05-General_Regression.html#chapter-summary",
    "href": "L05-General_Regression.html#chapter-summary",
    "title": "5  L05: The General Regression Situation",
    "section": "5.1 Chapter Summary",
    "text": "5.1 Chapter Summary\n\nThe Normal Equations\n\\[\\begin{align*}\n\\underline\\epsilon^T\\underline\\epsilon &= (Y - X\\underline\\beta)^T(Y - X\\underline\\beta)\\\\\n&= ... = Y^TY - 2\\underline\\beta^TX^TY + \\underline\\beta^TX^TX\\underline\\beta\n\\end{align*}\\] We then take the derivative with respect to \\(\\underline\\beta\\). Note that \\(X^TX\\) is symmetric and \\(Y^TX\\underline\\beta\\) is a scalar.. \\[\\begin{align*}\n\\frac{\\partial}{\\partial\\underline\\beta}\\underline\\epsilon^T\\underline\\epsilon &= 0 - 2X^TY + 2X^TX\\underline\\beta\n\\end{align*}\\]\n\nFor the 1 predictor case, make sure the equations look the same!\n\nSetting to 0, rearranging, and plugging in our data gets us the Normal equations: \\[\\begin{align*}\nX^TX\\underline{\\hat\\beta} &= X^T\\underline y\n\\end{align*}\\]\n\n\nFacts\n\\[X^TX\\underline{\\hat\\beta} = X^T\\underline y\\]\n\nNo distributional assumptions.\nIf \\(X^TX\\) is invertible, \\(\\hat{\\underline\\beta} = (X^TX)^{-1}X^T\\underline y\\).\n\n\\(\\hat{\\underline\\beta}\\) is a linear transformation of \\(\\underline y\\)!\nThis is the same as the MLE.\n\n\\(E(\\hat{\\underline\\beta}) = \\underline\\beta\\) and \\(V(\\hat{\\underline\\beta}) = \\sigma^2(X^TX)^{-1}\\).\n\nThis is the smallest variance amongst all unbiased estimators of \\(\\underline\\beta\\).\n\n\n\n\nExample Proof Problems\n\nProve that \\(\\sum_{i=1}^n\\hat\\epsilon_i\\hat y_i = 0\\).\nProve that \\((1/n)\\sum_{i=1}^n\\hat\\epsilon_i = 0\\).\nProve that \\(X^TX\\) is symmetric. Is \\(A^TA\\) symmetric in general?\n\n\n## Demonstration that they're true (up to a rounding error)\nmylm &lt;- broom::augment(lm(mpg ~ disp, data = mtcars))\nsum(mylm$.resid * mylm$.fitted)\n\n[1] 4.241496e-12\n\nmean(mylm$.resid)\n\n[1] 6.77236e-15\n\nX &lt;- model.matrix(mpg ~ disp, data = mtcars)\nall.equal(t(X) %*% X, t(t(X) %*% X))\n\n[1] TRUE\n\n\n\n\nAnalysis of Variance (Corrected)\n\n\n\n\n\n\n\n\nSource\n\\(df\\)\n\\(SS\\)\n\n\n\n\nRegression (corrected)\n\\(p - 1\\)\n\\(\\underline{\\hat{\\beta}}^TX^T\\underline y - n\\bar{\\underline y}^2\\)\n\n\nError\n\\(n - p\\)\n\\(\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y\\)\n\n\nTotal (corrected)\n\\(n - 1\\)\n\\(\\underline y^t\\underline y - n\\bar{\\underline y}^2\\)\n\n\n\n\nNote that \\(p\\) is the number of parameters, not the index of the largest param.\n\n\\(\\underline\\beta = (\\beta_0, \\beta_1, ..., \\beta_{p-1})\\)\n\nWe’ll always be using corrected sum-of-squares.\n\nEspecially next chapter!\n\n\n\n\n\\(F\\)-test for overall significance\nIf SSReg is significantly larger than SSE, then fitting the model was worth it!\n\nThis is a test for \\(\\beta_1 = \\beta_2 = ... = \\beta_{p-1} = 0\\), versus any \\(\\beta_j\\ne 0\\).\n\nAs before, we find a quantity with a known distribution, then use it for hypothesis tests.\n\\[\nF = \\frac{MS(Reg|\\hat\\beta_0)}{MSE} = \\frac{SS(Reg|\\hat\\beta_0)/(p-1)}{SSE/(n-p)} \\sim F_{p-1, n-p}\n\\]\nAgain, note that a regression with no predictors always has \\(\\hat\\beta_0 = \\bar y\\).\n\n\nExample: Significance of disp\n\n\n\nanova(lm(mpg ~ disp, data = mtcars)) |&gt; \n    knitr::kable()\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\ndisp\n1\n808.8885\n808.88850\n76.51266\n0\n\n\nResiduals\n30\n317.1587\n10.57196\nNA\nNA\n\n\n\n\n\n\n\\(p = 2\\), \\(n = 32\\).\n\n\\(df_R' + df_E' = df_T'\\), where \\(df'\\) is the df for corrected SS.\n\nVerified these numbers in the last lecture\n\n\n\nplot(mpg ~ disp, data = mtcars)\nabline(lm(mpg ~ disp, data = mtcars))\n\n\n\n\n\n\n\n\nExample: \\(F_{1,p-1} = t^2_{p-1}\\)\n\nanova(lm(mpg ~ qsec, data = mtcars)) |&gt; \n    knitr::kable()\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nqsec\n1\n197.3919\n197.39193\n6.376702\n0.017082\n\n\nResiduals\n30\n928.6553\n30.95518\nNA\nNA\n\n\n\n\nsummary(lm(mpg ~ qsec, data = mtcars))$coef |&gt;\n    knitr::kable()\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-5.114038\n10.0295433\n-0.5098974\n0.6138544\n\n\nqsec\n1.412125\n0.5592101\n2.5252133\n0.0170820\n\n\n\n\n\nWhat do you notice about these two tables?\n\n\nExample: Significance of Regression (\\(F_{2,p-1} \\ne t^2_{p-1}\\))\n\nanova(lm(mpg ~ qsec + disp, data = mtcars)) |&gt; \n    knitr::kable()\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nqsec\n1\n197.3919\n197.39193\n18.25740\n0.0001898\n\n\ndisp\n1\n615.1185\n615.11850\n56.89424\n0.0000000\n\n\nResiduals\n29\n313.5368\n10.81161\nNA\nNA\n\n\n\n\nsummary(lm(mpg ~ qsec + disp, data = mtcars))$coef |&gt;\n    knitr::kable()\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n25.5045079\n7.1840940\n3.5501356\n0.0013359\n\n\nqsec\n0.2122880\n0.3667758\n0.5787951\n0.5671961\n\n\ndisp\n-0.0398877\n0.0052882\n-7.5428272\n0.0000000\n\n\n\n\n\nWe’ll learn more about the ANOVA table next lecture.\n\n\n\\(R^2\\) again\n\\[\nR^2 = \\frac{SS(Reg|\\hat\\beta_0)}{Y^TY - SS(\\beta_0)} = \\frac{\\sum(\\hat y_i - \\bar y)^2}{\\sum(y_i - \\bar y)^2}\n\\]\nWorks for multiple dimensions!… kinda.\n\n\n\\(R^2\\) is bad?\n\n\n\nnx &lt;- 10 # Number of uncorrelated predictores\nuncorr &lt;- matrix(rnorm(50*(nx + 1)), \n    nrow = 50, ncol = nx + 1)\n## First column is y, rest are x\ncolnames(uncorr) &lt;- c(\"y\", paste0(\"x\", 1:nx))\nuncorr &lt;- as.data.frame(uncorr)\n\nrsquares &lt;- NA\nfor (i in 2:(nx + 1)) {\n    rsquares &lt;- c(rsquares,\n        summary(lm(y ~ ., \n            data = uncorr[,1:i]))$r.squared)\n}\nplot(1:10, rsquares[-1], type = \"b\",\n    xlab = \"Number of Uncorrelated Predictors\",\n    ylab = \"R^2 Value\",\n    main = \"R^2 ALWAYS increases\")\n\n\n\n\n\n\n\n\n\n\n\nAdjusted (Multiple) \\(R^2\\)\n\\[\nR^2_a = 1 - (1 - R^2)\\left(\\frac{n-1}{n-p}\\right)\n\\]\n\nPenalizes added predictors - won’t always increase!\n\nStill might increase by chance alone!\n\nF-test\n\n\\(R^2_a = R^2\\) when \\(p=1\\) (intercept model)\n\nStill not perfect!\n\nWorks for comparing different models on same data\nWorks (poorly) for comparing different models on different data.\n\nIn general you should use \\(R^2_a\\), but always be careful."
  },
  {
    "objectID": "L05-General_Regression.html#prediction-and-confidence-intervals-again",
    "href": "L05-General_Regression.html#prediction-and-confidence-intervals-again",
    "title": "5  L05: The General Regression Situation",
    "section": "5.2 Prediction and Confidence Intervals (Again)",
    "text": "5.2 Prediction and Confidence Intervals (Again)\n\n\\(R^2\\) and \\(F\\)\nRecall that \\[\nF = \\frac{MS(Reg|\\hat\\beta_0)}{MSE} = \\frac{SS(Reg|\\hat\\beta_0)/(p-1)}{SSE/(n-p)} \\sim F_{p-1, n-p}\n\\]\nFrom the definition of \\(R^2\\), \\[\\begin{align*}\nR^2 &= \\frac{SS(Reg|\\hat\\beta_0)}{SST}\\\\\n&= \\frac{SS(Reg|\\hat\\beta_0)}{SS(Reg|\\hat\\beta_0) + SSE}\\\\\n&= \\frac{(p-1)F}{(p-1)F + (n-p)}\n\\end{align*}\\] Conclusion: Hypothesis tests/CIs for \\(R^2\\) aren’t useful. Just use \\(F\\)!\n\n\nCorrelation of \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\), \\(\\hat\\beta_2\\), etc.\nWith a different sample, we would have gotten slightly different numbers!\n\nIf the slope changed, the intercept must change to fit the data\n\n(and )\nThe parameter estimates are correlated!\n\nSimilar things happen with multiple predictors!\nThis correlation can be a problem for confidence regions\n\n\n\nUncorrelated \\(\\hat\\underline{\\beta}\\)\n\\[\nV(\\hat{\\underline\\beta}) = \\sigma^2(X^TX)^{-1}\n\\]\nIn simple linear regression, \\[\n(X^TX)^{-1} = \\frac{1}{nS_{XX}}\\begin{bmatrix}\\sum x_i^2 & -n\\bar x\\\\-n \\bar x & n\\end{bmatrix}\n\\]\nso the correlation is 0 when \\(\\bar x = 0\\)!\n\n\nPrediction and Confidence Intervals for \\(Y\\)\n\\(\\hat Y = X\\hat\\beta\\).\n\nA confidence interval around \\(\\hat Y\\) is based on the variance of \\(\\hat\\beta\\).\n\\(\\hat Y \\pm t * se(X\\hat\\beta)\\)\n\n\\(Y_{n+1} = X\\beta + \\epsilon_{n+1}\\)\n\nA prediction interval around \\(Y_{n+1}\\) is based on the variance of \\(\\hat\\beta\\) and \\(\\epsilon\\)!\n\\(\\hat Y_{n+1} \\pm t * se(X\\hat\\beta + \\epsilon_{n+1})\\)"
  },
  {
    "objectID": "L05-General_Regression.html#participation-questions",
    "href": "L05-General_Regression.html#participation-questions",
    "title": "5  L05: The General Regression Situation",
    "section": "5.3 Participation Questions",
    "text": "5.3 Participation Questions\n\nQ1\nWhich of the following are the Normal equations?\n\n\\(X^TX\\underline\\beta = X^T\\underline y\\)\n\\(X^TX\\underline{\\hat\\beta} = X^T\\underline y\\)\n\\(\\hat{\\underline\\beta} = (X^TX)^{-1}X^T\\underline y\\)\n\\(f(\\epsilon_i) = \\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp\\left(\\frac{-1}{2}\\epsilon_i^2\\right)\\)\n\n\n\nQ2\nWhen is \\(X^TX\\) not invertible?\n\nOne of the predictors can be written as a linear combination of the others.\nThere are more predictors than observations.\nOne of the predictors has 0 variance.\nAll of the above\n\n\n\nQ3\nWhat does a significant F-test for the overall regression mean?\n\nThe variance in the line is significantly larger than the variance in the data.\nThe estimate of \\(\\beta_1\\) is significantly different from \\(\\beta_0\\),\nThe variance of the line is significantly different from 0.\nAt least one of the predictors in the model will have significant \\(t\\)-test.\n\n\n\nQ4\n\\(R^2\\) is best used for:\n\nDetermining whether a new predictor is worth including.\nComparing models with different numbers of predictors.\nComparing models based on different data sets.\nNone of the above.\n\n\n\nQ5\nWhich of the following describes a Prediction Interval?\n\nThe CI for the predicted value of the line\nThe CI for the predicted value of the line, including unobserved error at an \\(X\\) value\nThe CI for the predicted value of the line, including unobserved error at an \\(X\\) value that was not observed in the data\nThe CI for the predicted value of the line, including unobserved error at an \\(X\\) value that was not observed in the data, using the true value of \\(\\sigma^2\\)\n\n\n\nQ6\nWhich ANOVA table does the anova() function calculate?\n\n\n\n\n\n\n\n\n\n\n\nSource\n\\(df\\)\n\\(SS\\)\n\n\n\n\nRegression\n1\n\\(\\hat{\\underline\\beta}^TX^T\\underline y - n\\bar{\\underline y}^2\\)\n\n\nError\n\\(n-2\\)\n\\(\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y\\)\n\n\nTotal\n\\(n - 1\\)\n\\(\\underline y^t\\underline y - n\\bar{\\underline y}^2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\n\\(df\\)\n\\(SS\\)\n\n\n\n\nRegression\n1\n\\(\\hat{\\underline\\beta}^TX^T\\underline y\\)\n\n\nError\n\\(n-2\\)\n\\(\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y\\)\n\n\nTotal\n\\(n - 1\\)\n\\(\\underline y^t\\underline y\\)"
  },
  {
    "objectID": "L06-Extra_Sums_of_Squares-1.html#introduction",
    "href": "L06-Extra_Sums_of_Squares-1.html#introduction",
    "title": "6  L06: Extra Sum-of-Squares (Part 1)",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\n\nToday’s Main Idea\nIf you add or remove predictors, the variance of the residuals changes!\n\nAs always, we ask if it’s a “big” change.\nDifferent predictors have a different effect on the residuals.\n\nWhich predictors have a meaningful (significant?) effect on the residuals?\n\n\nSum-of-Squares due to Regression\n\nSince SSE = SST - SSReg and SST never changes, we’re focusing on SSReg.\nRecall: SSReg is the variance of the line itself!\n\n\\[\nSSReg = \\sum_{i=1}^n(\\hat y_i - \\bar y)\n\\]\n\n\nSSReg in two different penguin models\nIn the penguins data, we’re determining which predictors are associated with body mass.\n\n\\(SS_1\\) = SSReg for model 1\n\n\\(\\texttt{body\\_mass\\_g} = \\beta_0 + \\beta_1 \\texttt{flipper\\_length\\_mm} + \\beta_2 \\texttt{bill\\_length\\_mm} + \\beta_3 \\texttt{bill\\_depth\\_mm}\\)\n\n\\(SS_2\\) = SSReg for model 2\n\n\\(\\texttt{body\\_mass\\_g} = \\beta_0 + \\beta_1 \\texttt{flipper\\_length\\_mm} + \\beta_2 \\texttt{bill\\_length\\_mm}\\)\n\n\nNote: M2 is nested in M1 - M1 has all the same predictors and then some.\n\n\n\n\n\n\nImportant\n\n\n\n\\(\\beta_1\\) in the first model is different from \\(\\beta_1\\) in the second model.\n\n\n\n\nExtra Sum-of-Squares\nIf M2 is nested within M1, :\n\nM1: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength} + \\beta_3 \\texttt{billdepth}\\)\nM2: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength}\\)\n\nThen the Extra sum of squares is defined as: \\[\nSS(\\hat\\beta_3 | \\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) = S_1 - S_2\n\\]\nConvince yourself that S1 &gt; S2.\n\n\nSpecial Case: Corrected Sum-of-Squares\nWe’ve already seen this notation: \\[\nSSReg(\\hat\\beta_0) = n\\bar{\\underline y}^2\n\\] and \\[\nSSReg(corrected) = \\hat{\\underline\\beta}^TX^T\\underline y - n\\bar{\\underline y}^2 = S_1 - S_2\n\\] where \\(S_2\\) is the sum-of-squares for the null model!\n\n\nUnspecial Case: Correction doesn’t matter!\nConsider \\(S_{1c}\\) and \\(S_{2c}\\), the corrected versions of \\(S_1\\) and \\(S_2\\). Then\n\\[\\begin{align*}\nS_{1c} - S_{2c} = (S_2 - n\\bar{\\underline y}^2) - (S_1 - n\\bar{\\underline y}^2) = S_1 - S_2\n\\end{align*}\\]\nIn other words, the correction term doesn’t matter.\nThis is useful because R outputs the corrected versions.\n\n\nUnspecial Case: SSReg versus SSE doesn’t matter!\nConsider \\(SSE_1\\) and \\(SSE_2\\). Since SST is the same for both models,\n\\[\\begin{align*}\nSSE_2 - SSE_1 = (SST - S_1) - (SST - S_2) = S_2 - S_1\n\\end{align*}\\]\nNotice that the order is switched, which is fine.\n\n\nANOVA Tests for ESS\nConsider the models:\n\nM1: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength} + \\beta_3 \\texttt{billdepth}\\)\n\n\\(df_1 = 4\\)\n\nM2: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength}\\)\n\n\\(df_2 = 3\\)\n\n\nIf we choose \\(H_0: \\beta_3 = 0\\) in model 1, then \\[\n\\frac{S_1 - S_2}{(4 - 3)s^2} \\sim F_{1,\\nu}\n\\]\nwhere \\(s^2\\) is the error variance (MSE) in the larger model with degress of freedom \\(\\nu = df_1\\).\nThis is almost identical to the F-test for only one predictor (with one important difference).\n\n\nIn General\nIf M1 has \\(p\\) df, M2 has \\(q\\) df, and one is nested in the other, then \\(\\nu = \\max(p, q)\\) and\n\\[\n\\frac{S_1 - S_2}{(p - q)s^2} \\sim F_{|p-q|,\\nu}\n\\]\nNote that it doesn’t matter which is nested: \\(S_1 - S_2\\) has the same sign as \\(p-q\\), so it’s always positive.\n\n\nOmnibus Tests for Multiple Predictors\nSuppose we want to test if any bill measurement is useful.\n\nBill length and depth are highly correlated - marginal CIs won’t be valid.\nConfidence Regions are hard (and only work in 2D)\n\nInstead, we can use the ESS to test for a subset of predictors!\n\nM1: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength} + \\beta_3 \\texttt{billdepth}\\)\nM2: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength}\\)\n\n\\(S_1 = S_2\\) is equivalent to \\(\\beta_2 = \\beta_3 = 0\\), and it accounts for their covariance!\nIf significant, then at least one of \\((\\beta_2, \\beta_3)\\) is not 0.\n\n\nIn R\n\nlibrary(palmerpenguins)\npeng &lt;- penguins[complete.cases(penguins),]\nm1 &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n    data = peng)\nm2 &lt;- lm(body_mass_g ~ flipper_length_mm,\n    data = peng)\nanova(m1, m2) |&gt; knitr::kable()\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n329\n50814912\nNA\nNA\nNA\nNA\n\n\n331\n51211963\n-2\n-397050.9\n1.285349\n0.2779392\n\n\n\n\n\n\n\nNext time\n\nWhen to check ESS\nHow to check all ESS\nWhat is R’s anova() function even doing??"
  },
  {
    "objectID": "L07-Exampless.html#review",
    "href": "L07-Exampless.html#review",
    "title": "7  L07: ESS Exampless",
    "section": "7.1 Review",
    "text": "7.1 Review\nFrom last time, we basically learned what the following means:\n\\[\n\\frac{SS(\\hat\\beta_{q+1}, ..., \\hat\\beta_p | \\hat\\beta_0, ... \\hat\\beta_q)}{(p-q)s^2} =\\frac{S_1 - S(\\hat\\beta_0) - (S_2 - S(\\hat\\beta_0))}{(p-q)s^2}\\sim F_{p-q, \\max(p, q)}\n\\] where \\(s^2\\) is the MSE calculated from the larger model.\nThis allows us to do a test for whether \\(\\beta_{q+1} = \\beta_{q+2} = ... = \\beta_p = 0\\).\nThe R code to do this test is as follows. In this code, we believe that the bill length and bill depth are strongly correlated, and thus we cannot trust the CIs that we get from summary(lm()) (we saw “Confidence Regions” in the slides and code for L05).\n\nnrow(peng)\n\n[1] 333\n\n\n\nlm1 &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n    data = peng)\nlm2 &lt;- lm(body_mass_g ~ flipper_length_mm,\n    data = peng)\nanova(lm2, lm1)\n\nAnalysis of Variance Table\n\nModel 1: body_mass_g ~ flipper_length_mm\nModel 2: body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm\n  Res.Df      RSS Df Sum of Sq      F Pr(&gt;F)\n1    331 51211963                           \n2    329 50814912  2    397051 1.2853 0.2779\n\n\nLet’s try and calculate these values ourselves in a couple different ways!\n\nanova(lm1)\n\nAnalysis of Variance Table\n\nResponse: body_mass_g\n                   Df    Sum Sq   Mean Sq   F value Pr(&gt;F)    \nflipper_length_mm   1 164047703 164047703 1062.1232 &lt;2e-16 ***\nbill_length_mm      1    140000    140000    0.9064 0.3418    \nbill_depth_mm       1    257051    257051    1.6643 0.1979    \nResiduals         329  50814912    154453                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom this model, SSE is 50814912 on 329 degrees of freedom. This is the same as the SSE in the output of anova(lm2, lm1).\n\nanova(lm2)\n\nAnalysis of Variance Table\n\nResponse: body_mass_g\n                   Df    Sum Sq   Mean Sq F value    Pr(&gt;F)    \nflipper_length_mm   1 164047703 164047703  1060.3 &lt; 2.2e-16 ***\nResiduals         331  51211963    154719                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAgain, the SSE of 51211963 matches what we saw in anova(lm2, lm1), and we have 331 degrees of freedom (as expected, the differences in degrees of freedom is 2).\nNote that the F-value in anova() is just the ratio of the MSEs, but this is not the case here. Instead, we need to calculate \\(s^2\\).\nWe can calculate \\(s^2\\) as the MSE for the larger model:\n\ns2 &lt;- 50814912/329\ns2\n\n[1] 154452.6\n\n\nAnd now we can calculate the F-value as expected:\n\n(51211963 - 50814912)/ (2 * s2)\n\n[1] 1.285349\n\n\n\nprint(1- pf(1.28539, 2, 329))\n\n[1] 0.2779278\n\n\nIt is left as an exercise to calculate these values based on matrix multiplication. I highly suggest trying it with and without correction factors to convince yourself that both of them work (and to convince yourself that you know what the correction factor is and why it’s necessary)."
  },
  {
    "objectID": "L07-Exampless.html#ess-algorithms",
    "href": "L07-Exampless.html#ess-algorithms",
    "title": "7  L07: ESS Exampless",
    "section": "7.2 ESS Algorithms",
    "text": "7.2 ESS Algorithms\nThe idea above is based on testing a subset of predictors for at least one significant coefficient. This is usually what we want.\nHowever, there are also times where we want to check all predictors one-by-one. This is much less common than the textbook may lead you to believe, but it still happens.\nThere are three ways to calculate the ESS for all predictors. They are very helpfully labelled Types I, II, and III.\n\nType I: Sequential Sum-of-Squares (with interactions)\n\nCheck \\(SS(\\hat\\beta_1|\\hat\\beta_0)\\)\nCheck \\(SS(\\hat\\beta_2|\\hat\\beta_0, \\hat\\beta_1)\\)\nCheck \\(SS(\\hat\\beta_2:\\hat\\beta_1|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\n\n\\(\\hat\\beta_2:\\hat\\beta_1\\) is an interaction term, which means we use a formula like y ~ x1 + x2 + x1*x2 (although we’ll learn why R uses different notation than this).\n\nCheck \\(SS(\\hat\\beta_3|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\nCheck all interactions between x1, x2, and x3,\n…\n\n\nThis will give us every possible sum-of-squares. This is very very dubious, and can lead to a multiple comparisons problem!\n\nType 2: Sequential Sum-of-Squares (R’s Default)\n\nCheck \\(SS(\\hat\\beta_1|\\hat\\beta_0)\\)\nCheck \\(SS(\\hat\\beta_2|\\hat\\beta_0, \\hat\\beta_1)\\)\nCheck \\(SS(\\hat\\beta_3|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\n…\n\n\nThis gives us an ordered sequence of “is it worth adding x1?”, “if we have x1, is it worth adding x2?”, etc. This is only meaningful if the predictors are naturally ordered (such as polynomial regression, see below).\n\nType 3: Last-entry sum-of-squares\n\nCheck \\(SS(\\hat\\beta_1|\\hat\\beta_0, \\hat\\beta_2, \\hat\\beta_3)\\)\nCheck \\(SS(\\hat\\beta_2|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_3)\\)\nCheck \\(SS(\\hat\\beta_3|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\n\n\nThis checks whether adding predictor \\(x_i\\) is worth it, considering all other predictors are already in the model.\n\nType 2 ANOVA (Sequental Sum-of-Squares)\nBy default, R does sequential sum-of-squares. This is a very important fact to know!\nIn Types I and II, the order of the predictors matters. In fact, you cannot make any conclusions about the significance that doesn’t make reference to this fact.\n\n## Try changing the order to see how the significance changes!\nmylm &lt;- lm(mpg ~ qsec + disp + wt, data = mtcars)\nanova(mylm)\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nqsec       1 197.39  197.39  28.276 1.165e-05 ***\ndisp       1 615.12  615.12  88.116 3.816e-10 ***\nwt         1 118.07  118.07  16.914 0.0003104 ***\nResiduals 28 195.46    6.98                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mylm)$coef # No obvious connection to anova\n\n                 Estimate Std. Error     t value     Pr(&gt;|t|)\n(Intercept) 19.7775575655 5.93828659  3.33051585 0.0024420674\nqsec         0.9266492353 0.34209668  2.70873496 0.0113897664\ndisp        -0.0001278962 0.01056025 -0.01211109 0.9904228666\nwt          -5.0344097167 1.22411993 -4.11267686 0.0003104157\n\n\nHowever, there is at least one case where we do care about the order of the predictors. Consider polynomial regression, which we will return to later. For now, it is sufficient to know that we’re dealing with the model: \\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + ... + \\beta_{p-1}x_i^{p-1} + \\epsilon_i\n\\]In this model, notice that we only have one predictor \\(x\\), but we have performed non-linear transformations (HMWK: why is it important that the transformations are non-linear?).\nIn this case, a sequential SS setup makes quite a bit of sense. Given we have a linear model, is it worth making it quadratic? Given that we have a quadratic model, is it worth making it cubic? Given that we have a cubic model…\nIn the code below, I use the I() function (the I means identity) to make the polynomial model. The “formula” notation in R, y ~ x + z, has a lot of options. Including x^2, rather than I(x^2), makes R think we want to do one of the more fancy things, but the I() tells it that we want to literally square it. In the future, we’ll use a better way of doing this.\n\nx &lt;- runif(600, 0, 20)\ny &lt;- 2 - 3*x + 3*x^2 - 0.3*x^3 + rnorm(600, 0, 100)\nplot(y ~ x)\n\n\n\nmylm &lt;- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))\nanova(mylm)\n\nAnalysis of Variance Table\n\nResponse: y\n           Df   Sum Sq  Mean Sq   F value Pr(&gt;F)    \nx           1 49560412 49560412 4726.8201 &lt;2e-16 ***\nI(x^2)      1 19661307 19661307 1875.1955 &lt;2e-16 ***\nI(x^3)      1  1150679  1150679  109.7459 &lt;2e-16 ***\nI(x^4)      1      661      661    0.0630 0.8018    \nI(x^5)      1      112      112    0.0107 0.9177    \nI(x^6)      1     6274     6274    0.5984 0.4395    \nResiduals 593  6217568    10485                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom the table above, we can clearly see that this should just be a cubic model (which is the true model that we generated). Try changing things around to see if, say, it will still detect an order 5 polynomial if if there’s no terms of order 3 or 4.\n\n\nA note on calculations\nTake a moment to consider the following. Suppose I checked the following two (Type II) ANOVA tables:\n\nanova(lm(mpg ~ disp, data = mtcars))\nanova(lm(mpg ~ disp + wt, data = mtcars))\n\nBoth tables will have the first row labelled “disp” and include its sum-of-squares along with the F-value. Do you expect these two rows to be the same?\nThink about it.\nThink a little more.\nWhat values do you expect to be used in the calculation?\nWhich sums-of-squares? Which variances?\nLet’s test it out:\n\nanova(lm(mpg ~ disp, data = mtcars))\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndisp       1 808.89  808.89  76.513 9.38e-10 ***\nResiduals 30 317.16   10.57                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm(mpg ~ disp + wt, data = mtcars))\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndisp       1 808.89  808.89 95.0929 1.164e-10 ***\nwt         1  70.48   70.48  8.2852  0.007431 ** \nResiduals 29 246.68    8.51                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThey’re different! As homework, find out where the F value for disp is coming from in both tables. (All required values are in the table, and the answer was stated earlier!)\nWith both the polynomial and the disp example, we see that the interpretation of the anova table is highly, extremely, extraordinarily dependent on which predictors we choose to include AND the order in which we choose to include them. So, yeah. Be careful.\n\n\nType III SS in R\nThere isn’t a built-in function to do this. To create this, we can either use our math (my preferred method) or test each one individually.\n\nanova(\n    lm(mpg ~ disp + wt, data = mtcars),\n    lm(mpg ~ disp + wt + qsec, data = mtcars)\n)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp + wt\nModel 2: mpg ~ disp + wt + qsec\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     29 246.68                              \n2     28 195.46  1     51.22 7.3372 0.01139 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(\n    lm(mpg ~ disp + qsec, data = mtcars),\n    lm(mpg ~ disp + wt + qsec, data = mtcars)\n)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp + qsec\nModel 2: mpg ~ disp + wt + qsec\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     29 313.54                                  \n2     28 195.46  1    118.07 16.914 0.0003104 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(\n    lm(mpg ~ wt + qsec, data = mtcars),\n    lm(mpg ~ disp + wt + qsec, data = mtcars)\n)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ wt + qsec\nModel 2: mpg ~ disp + wt + qsec\n  Res.Df    RSS Df Sum of Sq     F Pr(&gt;F)\n1     29 195.46                          \n2     28 195.46  1 0.0010239 1e-04 0.9904"
  },
  {
    "objectID": "L07-Exampless.html#modelling-strategies",
    "href": "L07-Exampless.html#modelling-strategies",
    "title": "7  L07: ESS Exampless",
    "section": "7.3 Modelling Strategies",
    "text": "7.3 Modelling Strategies\nIf you are in a situation where you think to yourself “my predictors are logically ordered and I want to check for the significance of all of them one-by-one”, you want Type II.\nIf you think “they’re not ordered but I want to check significance”, you might want to check the overall F test for all predictors and then check t-tests for individual parameters.\nIf you think “what would happen if each predictor were the last one I put in the model”, then you want Type III. I can’t think of a good situation for Type I - you’re pretty much guaranteed to have a multiple comparisons issue.\nI also want to call attention to the fact that all of these algorithms assume that you have a set of predictors that you already know you want to check. If you noticed, there are other predictors in the mtcars dataset that we did not consider!\nWe’ll slowly build up some intuition over time, but my advice for choosing which predictors to include is as follows:\n\nStart with a lot of plots.\nBased on the plots and your knowledge of the context, create a candidate set of predictors that you think will be the final model.\nCheck the model fit (p-values, residuals, etc).\nBased on your knowledge of the context, check significance of groups of predictors that you think are highly correlated.\nYour final model will be based on the tests for groups of (or individual) predictors that you suspect would be relevant.\n\nThe purpose of this method for selecting predictors is to minimize the number of p-values that you check. The ESS techniques that we learned today (especially for the bill length/depth, where our knowledge of the problem informed us of which predictors to check) are an important part of the modelling process, but there is more to learn!"
  },
  {
    "objectID": "L09-Serial_Correlation.html#serial-correlation-in-the-residuals",
    "href": "L09-Serial_Correlation.html#serial-correlation-in-the-residuals",
    "title": "8  L09: Cereal Correlation in the Residuals",
    "section": "8.1 Serial Correlation in the Residuals",
    "text": "8.1 Serial Correlation in the Residuals\n\nAssumptions and Definitions\nThe time of each observation is recorded, and they are equally spaced.\n\nIn other words, \\(x_1\\) is observed at time 1, \\(x_2\\) is observed at time 2, etc.\n\nSerial Correlation: \\(cor(\\epsilon_{t-1}, \\epsilon_t)\\ne 0\\)\n\nSerial Correlation is not causation.\n\nKnowledge of one gives you more knowledge of the other.\n\nSerial correlation can be negative\n\nExample: didn’t hit quota today, so big push tomorrow.\n\n\n\n\nVisualizing Serial Correlation in the Residuals\nR"
  },
  {
    "objectID": "L09-Serial_Correlation.html#durbin-watson",
    "href": "L09-Serial_Correlation.html#durbin-watson",
    "title": "8  L09: Cereal Correlation in the Residuals",
    "section": "8.2 Durbin-Watson",
    "text": "8.2 Durbin-Watson\n\nStrong Assumption about Correlation\nThe usual model: \\(Y = X\\underline{\\beta} + \\underline \\epsilon\\).\nAssume that \\(cor(\\epsilon_{t-1}, \\epsilon_t) = \\rho\\), \\(cor(\\epsilon_{t-2}, \\epsilon_t)= \\rho^2\\), \\(cor(\\epsilon_{t-3}, \\epsilon_t) = \\rho^3\\), etc.\n\nThe correlation is proportional to the distance in time.\n\nThis can be written as: \\[\n\\epsilon_t = \\rho\\epsilon_{t-1} + z_t\n\\] where \\(z_t\\sim N(0,\\sigma^2)\\). Note that \\(V(\\hat\\epsilon_t) = \\frac{\\sigma^2}{1 - \\rho^2}\\).\n\n\nThe Durbin-Watson test statistic\nAs usual, we find a quantity with a known distribution: \\[\nd = \\dfrac{\\sum_{s=2}^n(\\hat\\epsilon_s - \\hat\\epsilon_{s-1})^2}{\\sum_{s=1}^n\\hat\\epsilon_s^2}\\sim\\text{ some complicated distribution}\n\\]\n\nDistribution has a closed form, but I hate it.\n\\(d\\in [0, 4]\\), with \\(d=2\\) corresponding to the null.\nR will calculate the values for you.\n\nTextbook has pages and pages of tables. Textbook was written before iPhones existed.\n\n\nI’ll show an example of DW later.\n\n\nCautions with Durbin-Watson\n\nTests the hypotheses \\(H_a:\\;cor(\\epsilon_{t-s}, \\epsilon_t) = \\rho^s\\) versus not that.\n\nThere are many, many other \\(H_a\\). DW has low power for these situations.\n\nGraphical summaries will reveal strong patterns; patterns found by DW might not be worrisome. \nIt’s more p-values to look at. We want to minimize the number of p-values we look at."
  },
  {
    "objectID": "L09-Serial_Correlation.html#graphical-methods",
    "href": "L09-Serial_Correlation.html#graphical-methods",
    "title": "8  L09: Cereal Correlation in the Residuals",
    "section": "8.3 Graphical Methods",
    "text": "8.3 Graphical Methods\n\nEmpirical Autocorrelation\n\n\n\nWe can simply find the correlation between \\(\\hat \\epsilon_t\\) and \\(\\hat \\epsilon_{t-1}\\)!\nWe can do the same for \\(\\hat \\epsilon_t\\) and \\(\\hat \\epsilon_{t-2}\\).\n\netc.\n\n\n\n\n\n\n\n\n\n\n\n\nt\n\\(\\hat \\epsilon_t\\)\n\\(\\hat \\epsilon_{t-1}\\)\n\\(\\hat \\epsilon_{t-2}\\)\n\n\n\n\n2\n\\(\\hat \\epsilon_1\\)\nNA\nNA\n\n\n2\n\\(\\hat \\epsilon_2\\)\n\\(\\hat \\epsilon_1\\)\nNA\n\n\n3\n\\(\\hat \\epsilon_3\\)\n\\(\\hat \\epsilon_2\\)\n\\(\\hat \\epsilon_1\\)\n\n\n4\n\\(\\hat \\epsilon_4\\)\n\\(\\hat \\epsilon_3\\)\n\\(\\hat \\epsilon_2\\)\n\n\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\n\n\n\n\n\n\nThe ACF: Empirical Autocorrelations of lag \\(k\\)\n\n\nACF: AutoCorrelation Function\nThe x-axis shows the lag, the y axis shows the correlations\nThe plot on the right shows an example of time series data.\n\n\npar(mfrow = c(2, 1))\nplot(co2, main = \"Time series of CO2 data\")\nacf(co2, main = \"ACF of CO2 data (not residuals)\")\n\n\n\n\n\n\n\n\nACF isn’t ideal\nIn the model we saw for DW, \\[\n\\epsilon_t = \\rho\\epsilon_{t-1} + z_t\n\\] which means that \\[\n\\epsilon_t = \\rho(\\rho\\epsilon_{t-2} + z_{t-1}) + z_t\n\\]\nThe lag 2 correlation (the \\(\\rho^2\\) term) includes the lag 1 correlation!\n\n\nPartial Autocorrelations\n\n\nIf we extend the model to: \\[\n\\epsilon_t = \\rho_1\\epsilon_{t-1} + \\rho_2\\epsilon_{t-2} + z_t,\n\\] then \\(\\rho_2\\) is the correlation in the lag 2 terms, accounting for lag 1 terms!\nThis is the PACF, and it’s often much more useful.\nThe plot on the right shows a cyclic trend.\n\n\npar(mfrow = c(2, 1))\nplot(co2, main = \"Time series of CO2 data\")\npacf(co2, main = \"PACF of CO2 data (not residuals)\")\n\n\n\n\n\n\n\n\nDW, ACF, and PACF in practice\nMost of the time, just check the PACF.\n\nIf you see something, check ACF.\nIf you’re in a field that requires p-values, show them the DW statistic.\n\nOr use a non-parametric test…\n\n\n\n\nWhat to do if there is autocorrelation?\n\nCorrelation in the residuals might mean correlation in the \\(y_i\\)’s\n\nTry time series modelling!\n\nIf it’s simple (lag 1) autocorrlation, the data could potentially be transformed to remove the autocorrelation.\n\n\\(y_t - y_{t-1} = X\\underline \\beta\\).\nChange estimation to account for autocorrelation.\n\nIf it’s complicated, get a PhD student to do it for you."
  },
  {
    "objectID": "L09-Serial_Correlation.html#participation",
    "href": "L09-Serial_Correlation.html#participation",
    "title": "8  L09: Cereal Correlation in the Residuals",
    "section": "8.4 Participation",
    "text": "8.4 Participation\n\nQ1\nSerial correlation can be tested for any data set.\n\n\nTrue\nFalse\n\n\n\n\nQ2\nA non-significant result from the DW test means there is no autocorrelation in the residuals.\n\nTrue\n\n\nFalse\n\n\n\nQ3\nThe quantity \\(d = \\dfrac{\\sum_{s=2}^n(\\hat\\epsilon_s - \\hat\\epsilon_{s-1})^2}{\\sum_{s=1}^n\\hat\\epsilon_s^2}\\) is a statistic because:\n\nIt’s a value calculated from the data, possibly including information from outside the data.\nIt’s a value calculated from the data only, with no other information.\nIt’s a value calculated from the data only, with no other information, and has a known distribution.\nIt’s not a statistic since it’s not estimating a population parameter.\n\n\n\nQ4\nThe “partial” in PACF refers to:\n\nThe PACF plot only contains some of (partial) information.\nThe PACF evaluates the lag \\(k\\) correlation after controlling for lags 1 to \\(k-1\\).\nThe PACF is only evaluated for a portion of the data.\nThe PACF for a lag of \\(k\\) cannot use the first \\(k-1\\) data points (they are the NAs in the table).\n\n\n\nQ5\nWhich of the following is not an assumption of the DW test?\n\nThe time points are all equally spaced.\nThe correlation between two residuals is equal to \\(\\rho\\).\nThe further apart two residuals are, the less correlated they are.\nThere is no missing data.\n\n\n\nQ6\nAutocorrelation means that there are no possible insights into the data.\n\nTrue, the study was worthless.\nFalse, there are standard methods that will work for any situation.\nFalse, but we won’t get into the details in this course."
  },
  {
    "objectID": "L09-Serial_Correlation.html#non-parametric-test-for-autocorrelation",
    "href": "L09-Serial_Correlation.html#non-parametric-test-for-autocorrelation",
    "title": "8  L09: Cereal Correlation in the Residuals",
    "section": "8.5 Non-Parametric Test for Autocorrelation",
    "text": "8.5 Non-Parametric Test for Autocorrelation\n\nRuns\n\\(\\sum_{i=1}^n\\hat\\epsilon_i = 0\\), so some residuals are positive and some are negative.\nThe runs test just looks at the sign of the residuals. Consider the signs:\n+ + - + - - - - + + - + + + + \nThere are 7 runs in these residuals. Is this a lot of runs?\n\n\nDefining “A Lot Of Runs”\np-value: Probability of a result at least as extreme as the one obtained, under the null hypothesis.\n\nNull: random +’s and -’s.\n\nFor small numbers, we can look at all sequences of +’s and -’s and count the runs!\n\nP(7 or more runs) is an upper tailed test (-ive autocorrelation)\nP(7 or fewer runs) is a test for +ive autocorrelation\n\n\n\nLarge Numbers: Of course it’s Normal!\nGiven \\(n_1\\) +’s and \\(n_2\\) -’s, the mean and variance of the number of runs is: \\[\n\\mu = \\frac{2n_1n_2}{n_1 + n_2} + 1\\text{, and }\\sigma^2 = \\frac{2n_1n_2(2n_1n_2 - n_1 - n_2)}{(n_1+n_2)^2(n_1 + n_2 - 1)}\n\\]\nIn the actual distribution, \\(P(runs\\le \\mu) = P(runs\\le \\mu -1/2) = P(runs &lt; \\mu + 1/2)\\).\nIn the normal distribution this isn’t true, so we apply a correction factor:\n\nLower-tailed test: \\(runs\\sim N(\\mu + 1/2, \\sigma^2)\\)\nUpper-tailed test: \\(runs\\sim N(\\mu - 1/2, \\sigma^2)\\)\nTwo-tailed test: \\(runs\\sim N(\\mu, \\sigma^2)\\) and we hope it averages out.\n\n\n\nExample\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/SerialCorrelation\")"
  },
  {
    "objectID": "L10-Hat-Resid_Plots-Cook.html#le-chapeau",
    "href": "L10-Hat-Resid_Plots-Cook.html#le-chapeau",
    "title": "9  L10: The Hat Matrix",
    "section": "9.1 Le Chapeau",
    "text": "9.1 Le Chapeau\n\nThe Hat Matrix\n\\[\nH = X(X^TX)^{-1}X^T\n\\]\nRecall: The hat matrix projects \\(Y\\) onto \\(\\hat Y\\), based on \\(X\\).\n\n\\(\\hat Y = HY\\)\n\n\\(\\hat Y_i = h_{ii} Y_i + \\sum_{j\\ne i}h_{ij}Y_j\\)\n\n\n\n\nVariance-Covariance matrix of \\(\\hat{\\underline\\epsilon}\\)\nJust like \\(\\beta_0\\) and \\(\\beta_1\\), each sample results in different \\(\\underline{\\hat\\epsilon}\\).\nAcross samples, we have: \\[\n\\underline{\\hat\\epsilon} - E(\\underline{\\hat\\epsilon}) = (I-H)(Y-X\\underline{\\beta}) = (I-H)\\underline{\\epsilon}\n\\] and therefore: \\[\\begin{align*}\nV(\\underline{\\hat\\epsilon}) &= E([\\underline{\\hat\\epsilon} - E(\\underline{\\hat\\epsilon})][\\underline{\\hat\\epsilon} - E(\\underline{\\hat\\epsilon})]^T)\\\\\n&= [I-H]E(\\underline{\\epsilon}\\underline{\\epsilon}^T)[I-H]^T\\\\\n&= [I-H]\\sigma^2[I-H]\\\\\n&= [I-H]\\sigma^2\n\\end{align*}\\] where we used the idempotency and symmetry of \\(I-H\\).\n\n\nThe variance of a residual\nGiven that \\(V(\\underline{\\hat\\epsilon}) = (I-H)\\sigma^2\\), \\[\nV(\\hat\\epsilon_i) = (1-h_{ii})\\sigma^2\n\\]\n\nThe variance of the residual depends on how much \\(Y_i\\) influences it’s own estimate.\n\nHigh influence = low variance.\n\n\nThe correlation between residuals is: \\[\n\\rho_{ij} = \\frac{Cov(\\hat\\epsilon_i, \\hat\\epsilon_j)}{\\sqrt{V(\\hat\\epsilon_i)V(\\hat\\epsilon_j)}} = \\frac{-h_{ij}}{\\sqrt{(1-h_{ii})(1-h_{jj})}}\n\\]\n\nThe covariance is negative! A large residual tells us there are small residuals.\n\n“Large” and “small” are relative\n\n\n\n\nMore H Facts\n\n\\(SS(\\hat{\\underline\\beta}) = \\hat{\\underline\\beta}^TX^TY = \\hat Y^TY = Y^TH^TY = Y^TH^THY = \\hat Y^T\\hat Y\\)\n\nWe used the facts \\(H^T= H^TH\\) and \\(\\hat Y = HY\\).\n\n\\(\\sum_{i=1}^nV(\\hat Y_i) = trace(H\\sigma^2) = p\\sigma^2\\)\n\n\\(p\\) is the number of parameters.\nProof is part of the assignment\n\n\\(H1 = 1\\) if the model contains a \\(\\beta_0\\) term.\n\n\\(1\\) is a column of 1s, not identity matrix.\nProof on next slide.\n\\(h_{ii} = 1 - \\sum_{j\\ne i}h_{ij}\\)\n\nNote that \\(h_{ij}\\in[-1, 1]\\).\n\n\n\n\n\nProof that \\(H1 = 1\\)\nNote that \\(HX = X\\) (as proven on A1).\n\nThe first column of \\(X\\) is all ones (\\(\\beta_0\\) term).\n\n\\([X]_{i1} = 1\\)\n\nTherefore \\([HX]_{i1}\\) is a column of ones.\n\nEvery row of \\(H\\) times the column of 1s in \\(X\\) results in a column of ones.\n\n\\([HX]_{i1}\\) is every row of \\(H\\) times the first column of \\(X\\).\n\nThe first column of \\(X\\) is 1s, which is equal to the first column of \\(HX\\), which is \\(H\\) times a column of ones.\nIn other words, \\(H1 = 1\\)"
  },
  {
    "objectID": "L10-Hat-Resid_Plots-Cook.html#studentized-residuals",
    "href": "L10-Hat-Resid_Plots-Cook.html#studentized-residuals",
    "title": "9  L10: The Hat Matrix",
    "section": "9.2 Studentized Residuals",
    "text": "9.2 Studentized Residuals\n\nInternally Studentized (not ideal)\nHow do you measure the size a residual?\nDivide by the variance, of course!\nWe know that \\(V(\\hat \\epsilon_i) = (1-h_{ii})\\sigma^2\\), and \\[\ns^2 = \\frac{\\sum_{y=1}^n(y_i-\\hat y_i)^2}{n-p} = \\frac{SSE}{df_E} = MSE\n\\] is an estimate of \\(\\sigma^2\\). Then, \\[\nr_i = \\frac{\\hat\\epsilon_i}{\\sqrt{s^2(1-h_{ii})}}\n\\] is called the internally studentized residual.\n\n\n“Internally” “Studentized”\nNote that \\[\ns^2 = \\frac{\\sum_{i=1}^n(y_i-\\hat y_i)^2}{n-p} = \\frac{\\sum_{i=1}^n(\\hat\\epsilon_i)^2}{n-p}\n\\] and therefore \\[\nr_i = \\frac{\\hat\\epsilon_i}{\\sqrt{s^2(1-h_{ii})}} = \\frac{\\hat\\epsilon_i}{\\sqrt{(\\hat\\epsilon_i^2 + \\sum_{j\\ne i}\\hat\\epsilon_j^2)(1-h_{ii})/(n-p)}}\n\\]\n\nIf \\(\\hat\\epsilon_i\\) is large, then \\(s^2\\) is large.\n\nIf \\(s^2\\) is large, then \\(s_i\\) is small!\n“Internally”: the variance includes the residual of interest.\n\n“Studentized” because Student made it popular.\nOften called “standardized”.\n\n\n\nExternally Studentized Step 1\nLike adding/removing predictors and checking the cahnge in SS, we can add/remove points!\n\nCalculate SS\nRemove the first point. Estimate the model again and calculate new SS.\nAdd the first point back, remove the second. Estimate the model again and check the SS.\n\nFor each point, we have an estimate of the variance without itself.\n\n\nExternally Studentized Step 2\nSkipping the math, \\[\ns^2_{(i)} = \\frac{(n-p)s^2 - \\hat\\epsilon_i^2/(1-h_{ii})}{n-p-1}\n\\] is the variance of the residuals without observation \\(i\\).\n\nThe influence tells us how much a point influenced the model\n\nWe can see what happened without it\nNo need to re-estimate the model!!!\n\n\n\n\nExternally Studentized Residuals\nUse \\(s^2_{(i)}\\) in place of \\(s^2\\). \\[\nt_i = \\frac{\\hat\\epsilon_i}{\\sqrt{s_{(i)}^2(1-h_{ii})}} \\sim t_{n-p-1}\n\\]\n\nFollows a \\(t\\) distribution!\n\nLarger than 2 is suspect, 3 is definitely an outlier!\n\nA large \\(t_i\\) is large relative to the other residuals\nUsually just called “Studentized”\n\nMost software uses Studentized residuals for plots/diagnostics!"
  },
  {
    "objectID": "L10-Hat-Resid_Plots-Cook.html#cooks-distance",
    "href": "L10-Hat-Resid_Plots-Cook.html#cooks-distance",
    "title": "9  L10: The Hat Matrix",
    "section": "9.3 Cook’s Distance",
    "text": "9.3 Cook’s Distance\n\nBetter Measures of Influence\nThe hat matrix is intepreted as influence, but it has problems.\n\n\\(y_i\\)’s influence on it’s own prediction,\n\ngiven all other points.\n\n\\(0 \\le h_{ii} \\le 1\\)\n\nWhat is a “big” influence?\n\nHow do you explain \\(h_{ii}\\) to nonstatisticians?\n\nA better measure is how much the predicted value changes with/without the obs.\n\n\nCooks Distance: Change in \\(\\hat y_i\\).\n\\[\nD_i = \\frac{\\sum_{i=1}^n(\\hat y_i - \\hat y_{(i)})^2}{ps^2}\n\\]\n\n\\(\\hat y_i\\) is the predicted value of \\(y_i\\) when all data are considered.\n\\(\\hat y_{(i)}\\) is the predicted value of \\(y_i\\) when observation \\(i\\) is removed.\n\\(s^2\\) is the MSE of the model with all of the data.\n\\(p\\) is the number of parameters\n\n\\(D_i\\) decreases as \\(p\\) increases!\n\n\nAgain, this would involve re-fitting the model \\(n\\) time (one for each obs).\n\n\nCook’s Distance: Alternate Form\n\\[\nD_i = \\left[\\frac{\\hat \\epsilon_i}{\\sqrt{s^2(1-h_{ii})}}\\right]^2\\left[\\frac{h_{ii}}{1 - h_{ii}}\\right]\\frac{1}{p} = r_i^2\\frac{\\text{variance of $i$th predicted value}}{\\text{variance of $i$th residual}}\\frac{1}{p}\n\\]\n\nAgain, use \\(H\\) rather than re-fitting the model.\nCook’s distance is a modification of the internally studentized residual.\n\nVariances are based on same “deletion” idea as studentized.\n\nRatio of Variances!\n\n\\(F\\) distribution, mean approaches 1 for large values of \\(n\\)\n\nCooks Distance of larger than 1 is suspect.\n\n\n\n\n\nNext Class\nPlots!"
  },
  {
    "objectID": "L10-Hat-Resid_Plots-Cook.html#participation",
    "href": "L10-Hat-Resid_Plots-Cook.html#participation",
    "title": "9  L10: The Hat Matrix",
    "section": "9.4 Participation",
    "text": "9.4 Participation\n\nQ1\nWhich does not describe a residual?\n\nThe observed difference between the measured value \\(y_i\\) and the one we predict with \\(\\hat y_i = X\\hat{\\underline\\beta}\\).\nGiven the true relationship \\(Y = X\\underline\\beta\\), residuals are deviations that cannot be observed.\nErrors that should be fixed.\n\n\n\nQ2\nWhich of the following is not true about the hat matrix (\\(H = X(X^TX)^{-1}X^T\\))?\n\n\\((I-H)(I-H)^T = (I-H)\\)\n\\(\\hat{\\underline\\beta} = (X^TX)^{-1}X^TY = X^{-1}HY\\)\n\\(h_{ii} = 1 - \\sum_{j\\ne i}h_{ij}\\)\n\\(H(I-H) = 0\\)\n\n\n\nQ3\nSuppose that \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\).\nWhich of the following statements is false?\n\n\\(V(\\hat\\epsilon_i) = (1 - h_{ii})\\sigma^2\\)\n\\(V(\\epsilon_i) = \\sigma^2\\)\n\\(V(Y_i) = \\sigma^2\\)\n\\(V(\\hat Y_i) \\ge V(\\hat \\epsilon_i)\\)\n\n\n\nQ4\nThe difference between internally and externally studentized residuals is:\n\n“Internal” uses all of the observations, “external” uses all of the observations except \\(i\\).\n“External” uses all of the observations, “internal” uses all of the observations except \\(i\\).\n\n\n\nQ5\nInternally studentized residuals follow a \\(t\\) distribution.\n\nTrue - they are a normal r.v. divided by a chi-square r.v.\nFalse - they are a normal r.v. divided by a chi-square r.v., but the two are not independent.\nFalse - They include a normal and a chi-square, but the \\(h_{ii}\\) make this not follow distributional assumptions.\nTrue - internally studentized residuals are a small modification to externally studentized residuals, which follow a \\(t\\) distribution.\n\n\n\nQ6\nWhich of the following is not useful for detecting outliers?\n\nStandardized residuals\nStudentized residuals\nCook’s distance\n\\(h_{ii}\\)"
  },
  {
    "objectID": "L11-Admin_Slides.html#participation-questions",
    "href": "L11-Admin_Slides.html#participation-questions",
    "title": "10  L11: The Hat Matrix 2",
    "section": "10.1 Participation Questions",
    "text": "10.1 Participation Questions\n\nQ1\nWhich of these does not give the diagonal of the hat matrix?\n\nhatvalues(mylm)\ndiag(X %*% solve(t(X) %*% X) %*% t(X))\naugment(mylm)$hat\n\n\n\nQ2\nRemoving an outlier will not change:\n\nThe diagonal of the hat matrix.\nThe off-diagonal of the hat matrix.\nSST\nAll of the above will change if we remove an outlier.\n\n\n\nQ3\nA large residual means large influence.\n\nTrue\nFalse\n\n\n\nQ4\nAll entries in the hat matrix are between -1 and 1.\n\nTrue\nFalse\n\n\n\nQ5\nWhich plot corresponds to the model with the most predictors?\n(All models are nested.)\n\n\n\n\n\n\n\nQ6\nIn the output of augment() in the broom package, the .sigma column refers to:\n\nThe MSE of the model.\nThe MSE of the model if it were fit without the observation in that row.\nThe variance of that residual, as calculated by \\(V(\\hat\\epsilon_i) = (1-h_{ii})s^2\\).\nThe variance of that residual, as calculated by \\(V(\\hat\\epsilon_i) = (1-h_{ii})s_{(i)}^2\\)."
  },
  {
    "objectID": "L12-Extra_Topics.html#standardizing-x",
    "href": "L12-Extra_Topics.html#standardizing-x",
    "title": "11  L12: Extra Topics",
    "section": "11.1 Standardizing \\(X\\)",
    "text": "11.1 Standardizing \\(X\\)\n\nMean-Centering\nConsider \\(y_i = \\beta_0 + \\beta_1 x'_i\\), where \\(x'_i\\) are the “centered” versions of \\(x_i\\): \\[\nx'_i = x_i - \\bar x\n\\]\nThen \\(\\bar{x'} = 0\\) and the coefficient estimates become: \\[\\begin{align*}\n\\hat\\beta_0 &= \\bar y - \\hat\\beta_1\\bar{x'} = \\bar y\\\\\n&\\text{and}\\\\\n\\hat\\beta_1 &= \\frac{S_{XX}}{S_{YY}} = \\frac{\\sum_{i=1}^n(x_i' - \\bar{x'})^2}{\\sum_{i=1}^n(x_i' - \\bar{x'})(y_i - \\bar{y})} = \\frac{\\sum_{i=1}^nx_i'^2}{\\sum_{i=1}^nx_i'(y_i - \\bar{y})}\n\\end{align*}\\]\n\n\nMean-Centering and Covariance\n\n\nFor un-centered data: \\[\\begin{align*}\nV(\\hat{\\underline\\beta}) &= (X^TX)^{-1}\\sigma^2,\\\\\n\\text{where }(X^TX)^{-1} &= \\frac{1}{nS_{XX}}\\begin{bmatrix}\\sum x_i^2 & -n\\bar x\\\\-n \\bar x & n\\end{bmatrix}\n\\end{align*}\\] Note also that \\(S_{x'x'} = \\sum_{i=1}^n{x'}_i^2\\), so \\[\\begin{align*}\nV(\\hat{\\underline\\beta}^c) &= \\frac{\\sigma^2}{nS_{X'X'}}\\begin{bmatrix}\\sum {x'}_i^2 & 0\\\\0 & n\\end{bmatrix}\\\\\n& = \\sigma^2\\begin{bmatrix}n^{-1} & 0 \\\\ 0 & (\\sum_{i=1}^n{x'}_i^2)^{-1}\\end{bmatrix}\n\\end{align*}\\]\n\\(\\implies\\) no covariance!!!\n\nSimulations with same data, but one uses centered data (code in L02 Rmd).\n\n\n\n\n\n\n\n\n\nComments on \\(V(\\hat{\\underline\\beta})\\)\n\\[\nV(\\hat{\\underline\\beta}^c) = \\sigma^2\\begin{bmatrix}n^{-1} & 0 \\\\ 0 & (\\sum_{i=1}^n{x'}_i^2)^{-1}\\end{bmatrix}\n\\]\n\n\\(V(\\hat\\beta_0) = \\sigma^2/n \\implies sd(\\hat\\beta_0) = \\sigma/\\sqrt{n}\\).\n\nThe t-test for significance of \\(\\beta_0\\) is just a hypothesis test for \\(\\bar y = 0\\). \n\nNote that \\(\\underline y\\) hasn’t changed, so \\(\\hat{\\underline\\epsilon}\\) and \\(\\sigma^2\\) are unchanged.\n\\(V(\\hat\\beta_0) = \\sigma^2/\\sum_{i=1}^n{x'}_i^2\\) isn’t all that interesting…\n\n\n\nStandardizing \\(\\underline x\\)\nIn addition to mean-centering, divide by the sd of \\(\\underline x\\): \\[\nz_i = \\frac{x_i - \\bar x}{\\sqrt{S_{XX}/(n-1)}}\n\\]\nThen \\(\\bar z = 0\\) and \\(sd(z) = 0 \\implies S_{ZZ} = n-1\\).\nIt can be shown that: \\[\nV(\\hat{\\underline\\beta}^s) = \\sigma^2\\begin{bmatrix}n^{-1} & 0 \\\\ 0 & (n-1)^{-1}\\end{bmatrix}\n\\]\n\n\\(\\underline x\\) doesn’t matter!!!\n\n\n\nStandardizing in Multiple Linear Regression\nSuppose we standardize each column of \\(X\\) (except the first).\nSeveral things happen:\n\nAll predictors are now in units of standard deviations!!!\n\nCoefficients are directly comparable!\n\nCovariances disappear!!!\n\nStandardizing doesn’t hurt and can often help \\(\\implies\\) it’s almost always worth it!"
  },
  {
    "objectID": "L12-Extra_Topics.html#general-linear-hypotheses",
    "href": "L12-Extra_Topics.html#general-linear-hypotheses",
    "title": "11  L12: Extra Topics",
    "section": "11.2 General Linear Hypotheses",
    "text": "11.2 General Linear Hypotheses\n\nDiet vs. Exercise\nWhich is more important for weight loss?\nWe can set this up in a linear regression framework: \\[\n\\text{Loss}_i = \\beta_0 + \\beta_1\\text{CaloriesConsumed}_i + \\beta_2\\text{ExercisesMinutes}_i\n\\] where we assume CaloriesConsumed and ExerciseMinutes are standardized.\nOur question about the importance of diet versus exercise becomes a hypothesis test: \\[\nH_0:\\; \\beta_1 = \\beta_2\\text{ vs. }H_a:\\; \\beta_1 \\ne \\beta_2\n\\] Alternatively, the null can be written as \\(\\beta_1 - \\beta_2 = 0\\).\n\n\nLinearly Independent Hypotheses\nIn some cases, we might have a collection of hypotheses. For ANOVA: \\[\nH_0:\\; \\beta_2 - \\beta_1 = 0,\\; \\beta_3 - \\beta_2 = 0,\\; \\beta_4 - \\beta_3 = 0,\\dots,\\; and\\; \\beta_{p-1} - \\beta_{p-2} = 0\n\\] These hypotheses are linearly indepenent. To see why, we can write them in matrix form: \\[\n\\begin{bmatrix}\n0 & -1 & 1 & 0 & 0 & ...\\\\\n0 & 0 & -1 & 1 & 0 & ...\\\\\n0 & 0 & 0 & -1 & 1 & ...\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & ...\\\\\n\\end{bmatrix}\\hat{\\underline\\beta} = \\underline 0\n\\] where none of the rows are linear combinations of the others.\nWe’ll use the notation \\(C\\underline\\beta = \\underline 0\\).\n\n\nLinearly Independent Hypotheses\nThe \\(C\\) matrix can be row-reduced to the hypotheses \\(\\beta_i=0\\;\\forall i&gt;0\\). In this case, our hypothesized model is: \\[\nY_i = \\beta_0 + \\underline\\epsilon\n\\]\nWe have reduced \\(Y = X\\underline\\beta + \\underline\\epsilon\\) to \\(Y = Z\\underline\\alpha + \\underline\\epsilon\\), where \\(\\underline\\alpha = (\\beta_0)\\) and \\(Z\\) is a column of ones.\n\n\nLinearly Dependent Hypotheses\nConsider the model \\(Y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{11}x_1^2 + \\underline\\epsilon\\) and the hypotheses: \\[\nH_0:\\; \\beta_{11} = 0,\\ \\beta_1 - \\beta_2 = 0,\\; \\beta_1 - \\beta_2 + \\beta_3 = 0,\\; 2\\beta_1 - 2\\beta_2 + 3\\beta_3 = 0\n\\] We can write this as: \\[\n\\begin{bmatrix}\n0 & 0 & 0  & 1\\\\\n0 & 1 & -1 & 0\\\\\n0 & 1 & -1 & 1\\\\\n0 & 2 & -2 & 3\n\\end{bmatrix}\\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\\\\\beta_{11}\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\\\0\\\\0\\end{bmatrix}\n\\] With a little work, we can show that this reduces to the model: \\[\nY = \\beta_0 + \\beta(x_1 + x_2) + \\underline\\epsilon \\Leftrightarrow Y = Z\\underline\\alpha + \\underline\\epsilon\n\\]\n\n\nTesting General Linear Hypotheses\nConsider an arbitrary matrix for \\(C\\) (not linearly dependent), such that we can row-reduce \\(C\\) to \\(q\\) linearly independent hypotheses.\n\nFull Model\n\n\\(SS_E = Y^TY - \\hat{\\underline\\beta}^TX^TY\\) on \\(n-p\\) df.\n\nHypothesized Model\n\n\\(SS_W = Y^TY - \\hat{\\underline\\alpha}^TZ^TY\\) on \\(n-p-q\\) df.\n\n\nFrom these, we get: \\[\n\\left(\\frac{SSW-SSE}{q}\\right)/\\left(\\frac{SSE}{n-p}\\right) \\sim F_{q, n-p}\n\\] In other words, we test whether the restrictions significantly change the \\(SS_E\\)!"
  },
  {
    "objectID": "L12-Extra_Topics.html#generalized-least-squares",
    "href": "L12-Extra_Topics.html#generalized-least-squares",
    "title": "11  L12: Extra Topics",
    "section": "11.3 Generalized Least Squares",
    "text": "11.3 Generalized Least Squares\n\nMain Idea\nWhat if the variance of \\(\\epsilon_i\\) isn’t the same for all \\(i\\)?\nIn other words, \\(V(\\underline\\epsilon) = V\\sigma^2\\) for some matrix \\(V\\).\n\nThe structure of \\(V\\) changes how we approach this.\n\nWeighted least squares: \\(V\\) is diagonal.\nGeneralized: \\(V\\) is symmetric and positive-definite, but otherwise arbitrary.\n\n\n\n\nTransforming the Instability Away\nIn the model \\(Y = X\\underline\\beta + \\underline\\epsilon\\), we want \\(V(Y) = I\\sigma^2\\), but we have \\(V(Y) = V\\sigma^2\\)\nSince \\(V\\) is symmetric and positive-definite, we can find a matrix \\(P\\) such that: \\[\nP^TP = PP = P^2 = V\n\\]\nWe can pre-multiply the model by \\(P^{-1}\\) so that \\(V(P^{-1}Y) = V^{-1}V\\sigma^2 = I\\sigma^2\\): \\[\nP^{-1}Y = P^{-1}X\\underline\\beta + P^{-1}\\underline\\epsilon \\Leftrightarrow Z = Q\\underline\\beta + \\underline f\n\\]\n\n\nGeneralized Least Squares Results\n\\[\\begin{align*}\n\\underline f^T\\underline f &= \\underline\\epsilon^TV^{-1}\\underline\\epsilon = (Y - X\\underline\\beta)^TV^{-1}(Y - X\\underline\\beta)\\\\\n\\hat{\\underline\\beta} &= (X^TV^{-1}X)^{-1}X^TV^{-1}Y\\\\\nSS_T &= \\hat{\\underline\\beta}^TQ^TZ = Y^TV^{-1}X(X^TV^{-1}X)^{-1}X^TV^{-1}Y\\\\\nSST &= Z^TZ = Y^TV^{-1}Y\\\\\n\\hat Y &= X\\hat{\\underline\\beta}\\\\\n\\hat{\\underline f} &= P^{-1}(Y-\\hat Y) = P^{-1}(I-X(X^TV^{-1}X)^{-1}X^TV^{-1})Y\n\\end{align*}\\]\nMost things are just switching \\(Y\\) with \\(P^{-1}Y\\), etc., except one…\n\n\nOLS when you should have used GLS\nSuppose the true model has \\(V(\\underline\\epsilon) = V\\sigma^2\\).\nLet \\(\\hat{\\underline\\beta}_O\\) be the estimate of \\(\\underline\\beta\\) if we were to fit with Ordinary Least Squares. Then:\n\n\\(E(\\hat{\\underline\\beta}_O) = \\underline\\beta\\)\n\\(V(\\hat{\\underline\\beta}_O) = (X^TX)^{-1}X^TVX(X^TX)^{-1}\\sigma^2\\)\n\nThe OLS estimate is unbiased, but has a much higher variance!\n\n\nChoosing \\(V\\)\n\nFor serially correlated data, \\(V_{ii} = 1\\) and \\(V_{ij} = \\rho^{|i-j|}\\)\n\nThis is choosing \\(V\\) based on model assumptions!\n\\(\\rho\\) must be estimated ahead of time.\n\nIf we have repeteated \\(x\\)-values, we can use the estimated variance from there.\n\nChoosing \\(V\\) based on the data\n\nIn a controlled experiment, where we have known weights for different \\(x\\)-values\n\nE.g., more skilled surgeons, machine age."
  },
  {
    "objectID": "L12-Extra_Topics.html#participation-questions",
    "href": "L12-Extra_Topics.html#participation-questions",
    "title": "11  L12: Extra Topics",
    "section": "11.4 Participation Questions",
    "text": "11.4 Participation Questions\n\nQ1\nWhich of the following will result in no correlation between \\(\\beta_0\\) and \\(\\beta_1\\)?\n\nCentering\nStandardizing\nBoth centering and standardizing\nNone of the above\n\n\n\nQ2\nWhat’s the primary reason for standardizing the predictors?\n\nRemove correlation between the \\(\\hat\\beta\\)s\nMake it so that the variance is not a function of the \\(X\\)-values.\nEnsure that the values of \\(\\hat\\beta\\) are comparable.\nMake it so that general linear hypotheses are possible.\n\n\n\nQ3\nIn a general linear hypothesis, \\(q\\) is the rank of the \\(C\\) matrix in \\(C\\underline\\beta = \\underline 0\\).\n\nTrue\nFalse\n\n\n\nQ4\nGeneralized Least Squares requires strong assumptions about the matrix \\(V\\).\n\nTrue\nFalse\n\n\n\nQ5\nIgnoring correlation/unequal variance in \\(\\underline\\epsilon\\) will lead to a biased estimate of \\(\\underline\\beta\\)\n\nTrue\nFalse\n\n\n\nQ6\nWhat do you expect the hat matrix to be for GLS?\n\n\\(X(X^TX)^{-1}X^TY\\)\n\\(X(X^TV^{-1}X)^{-1}X^TV^{-1}Y\\)\n\\(XV^{-1}(X^TV^{-1}X)^{-1}X^TV^{-1}Y\\)\n\\(XV^{-T}(X^TV^{-1}X)^{-1}X^TV^{-1}Y\\)"
  },
  {
    "objectID": "L13-Wrong_Model.html#the-wrong-model",
    "href": "L13-Wrong_Model.html#the-wrong-model",
    "title": "12  L13: Getting the Wrong Model",
    "section": "12.1 The Wrong Model",
    "text": "12.1 The Wrong Model\n\nThe Right Model?\nRecall: All models are wrong, some are useful!\nBut how wrong can a model be while still being useful?\n\nThis is an extraordinarily challenging philosophical question.\nWe will touch on a very small part of it\n\n\n\nThe Wrong Predictors\nSo far, we’ve talked about a model of the form \\(Y=X\\underline\\beta + \\underline\\epsilon\\).\n\n\\(E(\\hat{\\underline\\beta}) = E((X^TX)^{-1}X^TY) = (X^TX)^{-1}X^TX\\underline\\beta = \\underline\\beta\\)\n\nHowever, what if we are missing some predictors?\nWhat if the true model is \\(Y=X\\underline\\beta + X_2\\underline\\beta_2 + \\underline\\epsilon\\)? \\[\\begin{align*}\nE(\\hat{\\underline\\beta}) &= E((X^TX)^{-1}X^TY)\\\\\n& = (X^TX)^{-1}X^T(X\\underline\\beta + X_2\\underline\\beta_2) \\\\\n& = (X^TX)^{-1}X^TX\\underline\\beta + (X^TX)^{-1}X^TX_2\\underline\\beta_2 \\\\\n&= \\underline\\beta+ (X^TX)^{-1}X^TX_2\\underline\\beta_2\\\\\n&= \\underline\\beta + A\\underline\\beta_2\n\\end{align*}\\]\n\n\nBias due to wrong predictors\nThe bias of an estimator is: \\[\n\\text{Bias}(\\hat{\\underline\\beta}) = \\underline\\beta - E(\\hat{\\underline\\beta})\n\\]\nFor the case where \\(Y = X\\underline\\beta + X_2\\underline\\beta_2 +\\underline\\epsilon\\), \\[\n\\text{Bias}(\\hat{\\underline\\beta}) = \\underline\\beta - (\\underline\\beta + A\\underline\\beta_2) = A\\underline\\beta_2\n\\]\n\n\nExpected Mean Square\nSee text.\nUses the identity: For an \\(n\\times n\\) matrix \\(Q\\) and \\(n\\times 1\\) random vector \\(Y\\) with variance \\(V(Y)=\\Sigma\\), \\[\nE(Y^TQY) = (E(Y))^TQE(Y) + trace(Q\\Sigma)\n\\]\nThis may be useful for a future assignment question (will notify if you need it), but for now I’m going to explore this via simulation in the Rmd.\n\n\nSummary\n\nChoosing the wrong set of predictors can affect the model!"
  },
  {
    "objectID": "L13-Wrong_Model.html#participation-questions",
    "href": "L13-Wrong_Model.html#participation-questions",
    "title": "12  L13: Getting the Wrong Model",
    "section": "12.2 Participation Questions",
    "text": "12.2 Participation Questions\n\nQ1\nChoosing a model is easy.\n\nTrue\nFalse\n\n\n\nQ2\nWhich statement is false?\n\nIf you have the correct subset of predictors, you will have an unbiased model.\nIf you do not, your model is likely biased.\nIf you’re only interested in the estimate of one predictor, then it’s okay if the other estimates are biased.\nAll of the above are true.\n\n\n\nQ3\nProxy measures of important predictors help remove bias, but the coefficient has no relation to the data generating process.\n\nTrue\nFalse"
  },
  {
    "objectID": "L14-Nonlinear.html#non-linear-relationships",
    "href": "L14-Nonlinear.html#non-linear-relationships",
    "title": "13  L14: Non-Linear Relationships with Linear Models",
    "section": "13.1 Non-Linear Relationships",
    "text": "13.1 Non-Linear Relationships\n\nArbitrarily Shaped Functions\n\n\nThe plot on the right is the function: \\[\ny = 2 + \\frac{1}{5}x^2 - 8\\log(x) - 0.005x^3 + 20\\sin\\left(\\frac{x}{2}\\right) + \\epsilon\n\\]\n\n\n\n\n\n\n\n\nThe twist: The fitted line is just a polynomial model: \\(y = \\beta_0 + \\sum_{j=1}^{12}\\beta_jx^j\\)\n\n\nFitting a Polynomial\nTo fit a polynomial of order \\(k\\): \\[\ny = \\beta_0 + \\sum_{j=1}^{k}\\beta_jx^j + \\epsilon\n\\] we can simply fit a linear model to transformed predictors, i.e.: \\[\nx_1 = x;\\; x_2 = x^2;\\; x_3 = x^3;\\;...\n\\] and we can just fit a linear model as usual!\n… that seems too easy?\n\n\nChoosing Polynomial Order\nThere are two common options:\n\nDomain knowledge\n\nIs there a theoretical reason to use a cubic?\n\nReduce prediction error\n\nCross-validation or ANOVA, depending on problem.\n\n\n\n\nDomain Knowledge: Stopping Distance\n\n\nThe stopping distance is theoretically proportional to the square of the speed.\n\nA line might fit\n\nFits poorly at 0 (negative stopping distances for positive speed?)\n\nA quadratic fits better?\nA cubic does something funky at 0.\n\n\n\n\n\n\n\n\n\n\n\nChoosing Order with ANOVA\n\nX &lt;- data.frame(dist = cars$dist, x1 = cars$speed, x2 = cars$speed^2, \n    x3 = cars$speed^3, x4 = cars$speed^4)\nlm(dist ~ ., data = X) |&gt; anova()\n\nAnalysis of Variance Table\n\nResponse: dist\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nx1         1 21185.5 21185.5 92.5775 1.716e-12 ***\nx2         1   528.8   528.8  2.3108    0.1355    \nx3         1   190.4   190.4  0.8318    0.3666    \nx4         1   336.5   336.5  1.4707    0.2316    \nResiduals 45 10297.8   228.8                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this situation, Sequential Sum-of-Squares makes sense! (Disagrees with theory, though. Go with theory.)\n\n\nStopping Distance \\(\\propto\\) Speed\\(^2\\)\n\n\nA second order polynomial is: \\[\ny = \\beta_0 + \\beta_1x + \\beta_2x^2\n\\]\nThe implied model is: \\[\ny = \\beta_2x^2\n\\]\n\n\n\n\n\n\n\n\n\n\nUnconventional ESS\n\nX &lt;- data.frame(dist = cars$dist, x1 = cars$speed, x2 = cars$speed^2, \n    x3 = cars$speed^3, x4 = cars$speed^4)\nm1 &lt;- lm(dist ~ x1 + x2, data = X)\nm2 &lt;- lm(dist ~ -1 + x2, data = X)\nanova(m2, m1)\n\nAnalysis of Variance Table\n\nModel 1: dist ~ -1 + x2\nModel 2: dist ~ x1 + x2\n  Res.Df   RSS Df Sum of Sq      F Pr(&gt;F)\n1     49 11936                           \n2     47 10825  2    1111.2 2.4123 0.1006\n\n\nNot a significant difference in models, so go with simpler one: \\(y = \\beta_2x^2\\)\n\nThis is highly specific to this situation - see cautions later.\n\n\n\nPolynomial Models will Overfit!\nTrue model: \\(f(x) = 2 + 25x + 5x^2 - x^3\\) (cubic), Var(\\(\\epsilon\\)) = 40\n\n\n\n\n\n\n\nMultiple Regression Polynomial Models\nA full polynomial model of order 2 with two predictors is: \\[\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2 + \\beta_{12}x_1x_2\n\\] In R this can be specified as:\n\nlm(y ~ (x1 + x2)^2)\n\n\nThis is why you can’t use y ~ x + x^2 to get a polynomial model - R tries to interpret this as a model specification rather than a transformation. \nWe’ll learn more about interactions and transformations in the next few lectures."
  },
  {
    "objectID": "L14-Nonlinear.html#cautions-about-polynomials",
    "href": "L14-Nonlinear.html#cautions-about-polynomials",
    "title": "13  L14: Non-Linear Relationships with Linear Models",
    "section": "13.2 Cautions about Polynomials",
    "text": "13.2 Cautions about Polynomials\n\nLower Order Terms\nUnless there’s a strong physical reason,\n\n\\((ax - b)^2\\) is the model, not \\(\\beta_0 + \\beta_1x + \\beta_{11}x^2\\)\n\n\n\nOrders higher than 3 are rarely jutified\nRecall the interpretation of a slope:\n\nA one unit increase in \\(x\\) is associated with a \\(\\beta_1\\) unit increase in \\(y\\).\n\nThis interpretation fails in quadratrics, and fails spectacularly in higher orders.\n\n\nSee splines for more flexibility\n\n\nExtrapolation is Fraught with Peril\nUnless you have the true order (you don’t), polynomials diverge almost immediately.\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/polyFit\")\n\n\n\n\\(x\\) and \\(x^2\\) are correlated\nThis strongly affects parameter estimates.\n… unless…\n\n\npoly() uses orthogonal polynomials\n\nbetas &lt;- replicate(1000, \n    coef(lm(y ~ poly(x, 2, raw = TRUE),\n         data = data.frame(x = runif(30, 0, 10), \n            y = (x - 2)^2 + rnorm(30, 0, 10)))))\nplot(t(betas))\n\nAfter squaring, cubing, etc., each column of X is transformed to be orthogonal to the previous.\n\nTakes care of transformations for you when using predict().\nThe coef() function is useless.\nMean-centering also helps!\n\n\n\nOrthogonal Polynomials\n\nx &lt;- sort(runif(60, 0, 10))\npar(mfrow = c(2,3))\nfor(i in 1:3) {\n    plot(x, poly(x, 3, raw = TRUE)[,i])\n}\nfor(i in 1:3) {\n    plot(x, poly(x, 3, raw = FALSE)[,i])\n}"
  },
  {
    "objectID": "L14-Nonlinear.html#should-i-use-a-polynomial",
    "href": "L14-Nonlinear.html#should-i-use-a-polynomial",
    "title": "13  L14: Non-Linear Relationships with Linear Models",
    "section": "13.3 Should I Use a Polynomial?",
    "text": "13.3 Should I Use a Polynomial?\n\nExample: mtcars\n[code]\n\n\nSummary"
  },
  {
    "objectID": "L14-Nonlinear.html#participation-questions",
    "href": "L14-Nonlinear.html#participation-questions",
    "title": "13  L14: Non-Linear Relationships with Linear Models",
    "section": "13.4 Participation Questions",
    "text": "13.4 Participation Questions\n\nQ1\nGiven a correctly structured model matrix (\\(X\\)), polynomial models can be fit with the same routines as linear models (without modification).\n\nTrue\nFalse\n\n\n\nQ2\nAny smooth function can be approximated by a polynomial.\n\nTrue\nFalse\n\n\n\nQ3\nOrthogonal polynomials are used because:\n\nThey remove covariance between \\(x\\), \\(x^2\\), \\(x^3\\), etc.\nThey remove the covariance between \\(\\beta_1\\), \\(\\beta_{11}\\), \\(\\beta_{111}\\), etc.\nThey result in p-values that make sense.\nAll of the above.\n\n\n\nQ4\nWhich statement is true?\n\nWe should start with a high order polynomial and use Sequential Sum-of-Squares to choose the order.\nWe should try second order polynomials if we think there’s a curve to our model, but should generally avoid polynomials unless there’s a strong contextual reason.\nWe saw last week that estimates are still unbiased in the presence of extraneous predictors, so it’s fine to include a higher order polynomial in our model."
  },
  {
    "objectID": "L15-Transforming_Response.html#transformations",
    "href": "L15-Transforming_Response.html#transformations",
    "title": "14  L15: Transforming the Response",
    "section": "14.1 Transformations",
    "text": "14.1 Transformations\n\nTransforming the Predictors\nSuppose we found that the following second order polynomial model was a “good” fit: \\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_{11}x_{i1}^2 +\\beta_2x_{i2} + \\beta_{22}x_{i2}^2 + \\beta_{12}x_{i1}x_{i2} + \\epsilon_i\n\\]\nNow consider the model: \\[\n\\ln(y_i) = \\beta_0 + \\beta_1x_{i1} + \\beta-2x_{i2} + \\epsilon_i\n\\]\n\n3 parameters instead of 6!\n\nNo interaction term!\n\nIf we’re okay with the log scale for \\(y\\), easier to interpret.\n\n\n\nTransforming the predictors\n\n\nOriginal scale\n\n\n\n\n\n\nLog scale\n\n\n\n\n\n\n\nThe logarithm made it more linear, even if it’s not quite right.\n\n\nConsequences of Logarithms\nConsider the simple model \\(E(y_i) = \\beta x^2\\). Taking the logarithm of both sides: \\[\n\\ln(E(y_i)) = \\ln(\\beta) + 2\\ln(x) = \\beta_0 + \\beta_1 \\ln(x)\n\\] and we have something that looks more like a linear model.\n\nNote that, instead of \\(x^2\\), \\(x^{2.1}\\) would also work as a model.\n\nThe power of \\(x\\) can be estimated.\n\nIt’s also possible that the log scale is the correct scale for \\(y\\)\n\n\\(E(\\ln(y_i)) = \\beta_0 + \\beta_1x\\)\nIn other words, don’t get too bogged down by whether we take the ln of \\(x\\).\n\n\n\n\nLogarithms and Errors\nIf we believe that the log scale is a better scale for \\(y\\), we may postulate the model: \\[\n\\ln(y_i) = \\beta_0 + \\beta_1\\ln x_{i1} + \\beta_2\\ln x_{i2} + \\epsilon\n\\] which implies that the orginal scale for \\(y\\) has the form: \\[\ny_i = e^{\\beta_0}x_{i1}^{\\beta_1}x_{i2}^{\\beta_2}e^{\\epsilon_i}\n\\] The errors are multiplicative!!!\n\nOption 1: Accept this\n\nAllows us to use least squares.\n\nOption 2: Use the model \\(y_i = e^{\\beta_0}x_{i1}^{\\beta_1}x_{i2}^{\\beta_2} + \\epsilon_i\\)\n\nMight be better, but requires a bespoke estimation algorithm.\n\n\n\n\nGeneral Practice\nWe often simply use the model: \\[\n\\ln \\underline y = X\\beta + \\underline \\epsilon\n\\] and do everything on the log scale.\n\nSimpler, but still useful.\nGood predictions of \\(\\ln y_i\\) can be transformed to good predictions of \\(y_i\\).\n\nIn general: Decide on a functional relationship between \\(f(y)\\) and \\(X\\), then use additive errors on the scale of \\(f(y)\\).\nThis has consequences:"
  },
  {
    "objectID": "L15-Transforming_Response.html#residuals-in-transformed-space",
    "href": "L15-Transforming_Response.html#residuals-in-transformed-space",
    "title": "14  L15: Transforming the Response",
    "section": "14.2 Residuals in Transformed Space",
    "text": "14.2 Residuals in Transformed Space\n\nVariance Stabilization\nThe two main purposes of transformations:\n\nFit non-linear functional forms.\nStabilize the variance!\n\nScale-Location plot in the R defaults.\n\n\nFor example, the log function brings large values down a lot, small values down a little.\n\nThe scale of large residuals is decreased more than the scale of small residuals.\n\n\n\n\\(f(\\underline y) = X\\beta + \\underline\\epsilon\\)\nThe estimated residuals are \\(\\hat\\epsilon_i = f(y_i) - \\widehat{f(y_i)}\\)\n\nNote the awkwardly long hat!\n\nWe’re estimating the value of the function, not the value of \\(y_i\\).\nIf \\(f(y_i) - \\widehat{f(y_i)} = f(y_i - \\widehat{y_i})\\), then the original function must have been linear (and a transformation was useless).\n\nWe’re assuming \\(\\epsilon_i\\stackrel{iid}{\\sim} N(0, \\sigma^2)\\), which is difficult to translate to \\(f^{-1}(X\\beta + \\underline\\epsilon)\\).\n\nIn the special case of \\(\\ln\\), \\(\\exp{\\epsilon_i} \\sim \\text{LogNormal}(0, \\sigma^2)\\).\nNo assumption of independence on the original scale!!!\n\nWe assume that the residuals have the same variance on the transformed scale.\n\nLikely not true for the original scale of \\(y\\).\n\n\n\n\nSome Good News\nIf \\((a,b)\\) is a \\((1-\\alpha)\\) CI on the scale of \\(f(y)\\), then \\((f^{-1}(a), f^{-1}(b))\\) is a valid CI on the scale of \\(y\\).\n\nIt’s not the only valid CI!\n\nNote that it’s not a symmetric CI!\n\nWorks for \\(y\\) as well as the \\(\\beta\\) parameters.\n\nTransformation might induce dependence among the parameters.\nA CI for \\(\\beta_1\\) is useless if there’s high covariance with \\(\\beta_2\\)."
  },
  {
    "objectID": "L15-Transforming_Response.html#choosing-transformations",
    "href": "L15-Transforming_Response.html#choosing-transformations",
    "title": "14  L15: Transforming the Response",
    "section": "14.3 Choosing Transformations",
    "text": "14.3 Choosing Transformations\n\nMethods for Choosing Transformations\n\nTheory.\n\nIf theory says that the log transform makes sense, use that.\n\nDon’t even consider the next steps. Just go with theory.\n\nExample: Forest fire burn sizes are right skewed, the log-transform makes sense.\n\nIn my research, I used the lognorman lodel for the residuals to acheive the same effect.\n\n\nExperimentation after looking at the Scale-Location plot.\n\nIf log or sqrt don’t work, move on to step three.\n\nThe Box-Cox Transformation\n\nFinds an appropriate transformation using maximum likelihood.\n\n\n\n\nBox-Cox\nWe use the transformation: \\[\nV = \\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda \\dot{Y}^{\\lambda - 1}} & \\text{if }\\lambda \\ne 0\\\\\n\\dot{Y}\\ln(Y) & \\text{if }\\lambda = 0\n\\end{cases}\n\\] where \\(\\dot Y\\) is the geometric mean of \\(y\\).\n\\(\\lambda\\) is chosen through maximum likelihood\n\nEssentially, refit with each value of \\(\\lambda\\) and see which minimizes the residual variance.\n\nPlot the likelihhods and choose the highest.\n\n\n\n\nSimpler Box-Cox\nThe textbook recommends the previous formula, however R uses: \\[\nW = \\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda} & \\text{if }\\lambda \\ne 0\\\\\n\\ln(Y) & \\text{if }\\lambda = 0\n\\end{cases}\n\\]\n\n\nVariance of \\(\\lambda\\)\nIf we had a different data set, we’d get a different value of \\(\\lambda\\)!\nR reports the the log-likelihood values, along with the top 5%.\n\nAnything in the top 5% is reasonable.\n\nIt’s not an exact science.\n\nUsually, we check the best \\(\\lambda\\) values and round to something nice.\n\nlog, sqrt, squared, inverse, etc.\n\n\n\n\nSummary\n\nChoosing a transformation:\n\nTheory\nExploration\nRound the value from Box-Cox.\n\nWorking with a transformation:\n\nChoose functional form, assume additive errors (usually, not always!)\nStay on the transformed scale\n\nAll assumptions about residuals apply to the transformed scale!\n\n\n\nTo be useful, all transformations should consider the context of the problem!"
  },
  {
    "objectID": "L15-Transforming_Response.html#participation-questions",
    "href": "L15-Transforming_Response.html#participation-questions",
    "title": "14  L15: Transforming the Response",
    "section": "14.4 Participation Questions",
    "text": "14.4 Participation Questions\n\nQ1\nIf the true relationship has the form \\(y = f(X) + \\epsilon_i\\), we can always find a transformation \\(f^{-1}\\) to make it linear.\n\nTrue\nFalse\n\n\n\nQ2\nIf we find that \\(\\lambda = 2\\) is the best transformation, then the following models are equivalent: \\[\n\\frac{Y^2 - 1}{2} = X\\beta\\quad\\text{and}\\quad Y^2 = 2X\\beta + 1\n\\]\n\nTrue\nFalse\nTechnically not, but good enough in practice.\n\n\n\nQ3\nA transformation of the form \\(y = f(X) + \\epsilon\\) leads to multiplicative errors.\n\nTrue\nFalse\n\n\n\nQ4\nWhich of the following is not a good reason to investigate transformations?\n\nIf the variance looks unstable.\nIf the theory says a transformation is necessary.\nIf a transformation might lead to a much simpler model.\nIf \\(y\\) doesn’t look normal.\n\n\n\nQ5\nThe default residual plots in R can help diagnose the need for a transformation.\n\nTrue\nFalse"
  },
  {
    "objectID": "L16-Dummies.html#predictors",
    "href": "L16-Dummies.html#predictors",
    "title": "15  L16: Dummy Variables",
    "section": "15.1 0/1 Predictors",
    "text": "15.1 0/1 Predictors\n\nDummy Coding\n“Dummy” variables are just predictors that only take the values 0 and 1.\n\n0 pairs of glasses versus 1 pair of glasses\n\nThis is a count that can only be 0 or 1\n\n0 means automatic, 1 means manual\n\nArbitrary choice of 0/1\n\n0 means off, 1 means on\n\nNatural choice of 0/1, but still arbitrary\n\n\n\n\nSlopes with a Dummy Variable\nUsual interpretation: as \\(x\\) increases by 1, \\(y\\) increases by \\(\\beta\\).\nThis doesn’t go away, but we get a new interpretation!\n\n\\(x\\) can only increase by one (from 0 to 1).\n\n\\(\\beta\\) is the difference in groups.\n\n\nNote that we assume constant variance; this means the variance is the same in both groups\n\nExact same assumptions as a t-test.\n\n(Different from a “Welch” t-test)\n\n\n\n\nCategorical Variables\nConsider the cyl column in mtcars. We could code three dummy variables:\n\n\\(I(cyl == 4)\\)\n\\(I(cyl == 6)\\)\n\\(I(cyl == 8)\\)\n\nThis would lead to the model: \\[\ny = \\beta_0 + \\beta_1I(cyl == 4) + \\beta_2I(cyl == 6) + \\beta_3I(cyl == 8)\n\\]\nWhat is the intercept here?\n\n\nCategorical Variable Dummy Coding\nInstead, we set one as a reference variable and let the intercept “absorb” it:\n\n\\(I(cyl == 6)\\)\n\\(I(cyl == 8)\\)\n\nThis would lead to the model: \\[\ny = \\beta_0 + \\beta_1I(cyl == 6) + \\beta_2I(cyl == 8)\n\\] where\n\n\\(\\beta_0\\) is the mean of mpg when cyl = 4.\n\\(\\beta_1\\) is the difference in mean of mpg between 4 and 6 cylinder cars.\n\\(\\beta_2\\) is the difference in means for 4 versus 8.\n\nDifference btwn 6 and 8 can be found with some cleverness.\n\n\n\n\nModels with Categorical Variables\nThe model \\(y = \\beta_0 + \\beta_1I(cyl == 6) + \\beta_2I(cyl == 8)\\) is equivalent to: \\[\ny_i = \\begin{cases}\\beta_0 & \\text{if }\\; cyl == 4\\\\ \\beta_0 + \\beta_1 & \\text{if }\\; cyl == 6\\\\\\beta_0  + \\beta_2 & \\text{if }\\; cyl == 8\\end{cases}\n\\]\nThis is equivalent to fitting an intercept-only model \\(y = \\beta_0\\) for subsets of the data.\nBy putting them in the same model, we can easily test for significance."
  },
  {
    "objectID": "L16-Dummies.html#interactions",
    "href": "L16-Dummies.html#interactions",
    "title": "15  L16: Dummy Variables",
    "section": "15.2 Interactions",
    "text": "15.2 Interactions\n\nDifferent Intercepts, Same Slope\nIf we have cyl and disp in the model, we get the following:\n\\[\ny = \\beta_0 + \\beta_1I(cyl == 6) + \\beta_2I(cyl == 8) + \\beta_3 disp\n\\] which is equivalent to: \\[\ny_i = \\begin{cases}\\beta_0  + \\beta_3 disp& \\text{if }\\; cyl == 4\\\\ \\beta_0 + \\beta_1  + \\beta_3 disp& \\text{if }\\; cyl == 6\\\\\\beta_0  + \\beta_2  + \\beta_3 disp& \\text{if }\\; cyl == 8\\end{cases}\n\\] This is three different models of mpg versus disp, but with a different intercept depending on the value of cyl.\n\n\nInteraction Terms: Different Intercepts, Different Slopes\nWe can expand the model above with an interaction term. \\[\ny = \\beta_0 + \\beta_1I(6) + \\beta_2I(8) + \\beta_3 disp + \\beta_4I(6)disp + \\beta_5I(8)disp\n\\] where \\(I(6)\\) is just shorthand for \\(I(cyl == 6)\\).\nThis is the same as: \\[\ny_i = \\begin{cases}\\beta_0  + \\beta_3 disp& \\text{if }\\; cyl == 4\\\\ (\\beta_0 + \\beta_1)  + (\\beta_3 + \\beta_4) disp& \\text{if }\\; cyl == 6\\\\(\\beta_0  + \\beta_2)  + (\\beta_3 + \\beta_5) disp& \\text{if }\\; cyl == 8\\end{cases}\n\\] In this case, we might as well fit 3 completely different models!\n(Except we can test for significance!)\n\n\nANCOVA\nIf g is a categorical variable, then:\n\nlm(y ~ g) is a t-test if g is binary\nlm(y ~ g) is an ANOVA if g has more than 2 categories\nlm(y ~ x * g) is an ANCOVA model\n\nAnalysis of Covariance.\n\n\nMain idea: Is the covariance (or correlation) between \\(x\\) and \\(y\\) different for different categories of \\(g\\)?\n\nOnly a small extension to ANOVA\n\nt-test: exactly 2 means\nANOVA: 2+ means\nANCOVA: 2+ covariances\n\n\n\n\nBeyond \\(x\\) and \\(g\\)\n\nIn the simple cases, we’re doing t-test, ANOVA, or ANCOVA.\nBeyond this, we’re just doing regression, no special names.\n\n“Controlling for” is a term we’ll use later.\n\nChoosing interaction terms is hard.\n\nggplot2 makes parts of it a lot easier."
  },
  {
    "objectID": "L16-Dummies.html#participation-questions",
    "href": "L16-Dummies.html#participation-questions",
    "title": "15  L16: Dummy Variables",
    "section": "15.3 Participation Questions",
    "text": "15.3 Participation Questions\n\nQ1\nA dummy variable is:\n\nA stupid variable. Just a dumb, stupid variable that only idiots use.\nA variable that naturaly takes the values 0 and 1.\nA variable that has been coded as 0 and 1 to indicate different groups.\n\n\n\nQ2\nA reference category is used because:\n\nWe are always more interested in one particular category and how the other categories compare to it.\nThe intercept is the mean of y when all predictors are 0, so we need a case where all predictors are 0.\n\n\n\nQ3\nIn the mpg ~ factor(cyl) there will be a dummy variable labelled:\n\n6\ncyl6\nfactor(cyl)6\nfactor(cyl)4\n\n\n\nQ4\nWhich statement is false?\n\nt-test is a special case of linear regression.\nANCOVA is a special case of linear regression.\nWelch’s t-test for samples with unequal variances is a special case of regression.\nANOVA is a special case where Sequential Sum-of-Squares makes sense.\n\n\n\nQ5\nIt is possible to have interaction terms between two categorical variables.\n\nTrue\nFalse\n\n\n\nQ6\nIn the regression mpg ~ factor(cyl) + disp + factor(cyl):disp, if we find that disp is not significant then we can safely remove it.\n\nTrue\nFalse"
  },
  {
    "objectID": "L16-Dummies.html#significance-of-a-group",
    "href": "L16-Dummies.html#significance-of-a-group",
    "title": "15  L16: Dummy Variables",
    "section": "15.4 Significance of a Group",
    "text": "15.4 Significance of a Group\n\nOutput of summary.lm()\nThe output compares each slope to 0.\n\nFor a categorical predictor, this tests if a category is different from the reference.\n\nThere is not an easy built-in way to check significance of a specific group\n\nFor example, we may want to test for equality of intercepts and slopes for 6 and 8 cylinder cars, allowing 4 to have separate values.\n\nTest for equality of slopes is something covered in the textbook\nAlternative: Change Reference group and use ESS\n\n\n\n\nChanging the reference group\nSuppose we want to compare 6 and 8 cylinder cars. We can set up our model as: \\[\ny = \\beta_0 + \\beta_1I(8) + \\beta_2I(4) + \\beta_3disp + \\beta_4dispI(8) + \\beta_5dispI(4)\n\\] where now 6 is the reference group.\nWe can test \\(\\beta_1=\\beta_4 = 0\\) using ESS to test whether mpg versus disp is the same in these two categories.\nIn R, we need to set up our own dummies to do this."
  },
  {
    "objectID": "L17-Multicollinearity.html#the-problem",
    "href": "L17-Multicollinearity.html#the-problem",
    "title": "16  L17: Multicollinearity",
    "section": "16.1 The Problem",
    "text": "16.1 The Problem\n\nThe Problem with Multicollinearity\n\n\n\nMultiple regression fits a hyperplane\nIf the points form a “tube”, an infinite number of hyperplanes work.\n\nRotate plane around axis of tube.\n\n\n\n\n\n\n\n\n\n\n\n\nConsequences of the Problem\n\n\nHigh cor. in \\(X\\) \\(\\implies\\) high cor. in \\(\\hat{\\underline\\beta}\\).\n\nMany combos of \\(\\hat{\\underline\\beta}\\) are equally likely\nNo meaningful CIs\n\n\n\nset.seed(2112)\nreplicate(1000, {\n    y &lt;- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n    coef(lm(y ~ x1 + x2))[-1]\n}) |&gt; \n    t() |&gt; \n    plot(xlab = expression(hat(beta)[1]), \n        ylab = expression(hat(beta[2])),\n        main = \"Estimated betas for correlated\\npredictors, many samples\")\n\n\n\n\n\n\n\n\nAnother Formulation of the Problem\nConsider the model \\(y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\epsilon_i\\), where \\[\nx_{i1} = a + bx_{i2} + z_i\n\\] where \\(z_i\\) represents some extra uncertainty.\nFitting the model, we could:\n\nSet \\(\\hat\\beta_1\\) to 0, let \\(x_2\\) model all of the variance.\nSet \\(\\hat\\beta_2\\) to 0, let \\(x_1\\) model all of the variance.\nLet \\(x_1\\) model any proportion of the variance, let \\(x_2\\) model the rest.\n\nThe parameter estimates are not unique.\n\n\nThe Source of the Problem\n\\[\n\\hat{\\underline{\\beta}} = (X^TX)^{-1}X^TY,\\quad V(\\hat{\\underline{\\beta}}) = (X^TX)^{-1}\\sigma^2\n\\]\n\nIf two columns of \\(X\\) are linearly dependent, then \\(X^TX\\) is singular.\n\nConstant predictor value (linearly dependent with column of 1s).\nUnit change (one column for Celcius, one for Fahrenheit).\n\nIf two columns of \\(X\\) are nearly linearly dependent, then some elements of \\((X^TX)^{-1}\\) are humungous.\n\nTwo proxy measure for the same thing (e.g., daily high and low temperatures).\nNearly linear transformation (e.g., polynomial or BMI)\n\n\n\n\nDetecting the Problem\nThe variance-covariance matrix of \\(X\\) can be useful: \\[\nCov(X) = \\begin{bmatrix}\n0 & 0 & 0 & 0 & \\cdots\\\\\n0 & V(X_1) & Cov(X_1, X_2) & Cov(X_1, X_3) & \\cdots\\\\\n0 & Cov(X_1, X_2) & V(X_2) & Cov(X_2, X_3) & \\cdots\\\\\n0 & Cov(X_1, X_3) & Cov(X_2, X_3) & V(X_3) & \\cdots\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n\\end{bmatrix}\n\\] Why are the first column/row 0?\n\n\nPlotting \\(Cor(X)\\)\n\nlibrary(palmerpenguins); library(GGally)\nggcorr(penguins)\n\nWarning in ggcorr(penguins): data in column(s) 'species', 'island', 'sex' are\nnot numeric and were ignored\n\n\n\n\n\n\n\nDetecting the Problem: \\(V(\\hat{\\underline\\beta})\\)\nUnfortunately, the var-covar matrix is hard to get from R.\n\nWe can look at the SE column of the summary output!\n\nVery very very much not conclusive.\n\nThe Variance Inflation Factor\n\n\n\nThe Variance Inflation Factor\nWe can write the variance of each estimated coefficeint as: \\[\nV(\\hat\\beta_i) = VIF_i\\frac{\\sigma^2}{S_{ii}}\n\\] where \\(S_{ii} = \\sum_{k=1}^n(x_{ki} - \\bar{x_i})^2\\) is the “SS” for the \\(i\\)th column of \\(X\\).\n\nIf there is no “Variance Inflation”, then VIF = 1\n\n“Inflation” comes from the idea of rotating a plane around a “tube”.\nAlso interpreted as a measure of linear dependence with other columns of \\(X\\).\n\n\n\n\nInterpreting the Variance Inflation Factor\nConsider a regression of \\(X_i\\) against all other columns of \\(X\\).\n\nThe \\(R^2\\) measures how well the other predictors can model \\(X_i\\)\n\nLabel this \\(R_i^2\\) to indicate it’s the \\(R^2\\) for \\(X_i\\) against other columns.\n\nImportant: We’re not considering \\(\\underline y\\) at all!\n\nThe VIF can be calculated as: \\[\nVIF_i = \\frac{1}{1 - R_i^2}\n\\]\n\nIf \\(R_i^2=0\\), then \\(VIF_i = 1\\)\nIf \\(R_i^2\\rightarrow 1\\), then \\(VIF_i \\rightarrow \\infty\\)"
  },
  {
    "objectID": "L17-Multicollinearity.html#will-scaling-fix-the-problem",
    "href": "L17-Multicollinearity.html#will-scaling-fix-the-problem",
    "title": "16  L17: Multicollinearity",
    "section": "16.2 Will Scaling Fix the Problem",
    "text": "16.2 Will Scaling Fix the Problem\n\nScaling the Predictors\nIf we subtract the mean and divide by the sd, some of the correlation goes away.\n\nThis is actually kinda bad - we’ve hidden some multicollinearity from ourselves!\n\nIf \\(Z\\) is the standardized version of \\(X\\), then \\[\nCor(X) = Z^TZ/(n-1)\n\\]\nIf \\(Z\\) is the mean-centered version of \\(X\\), then \\[\nCov(X) = Z^TZ/(n-1)\n\\]"
  },
  {
    "objectID": "L17-Multicollinearity.html#fixing-the-problem",
    "href": "L17-Multicollinearity.html#fixing-the-problem",
    "title": "16  L17: Multicollinearity",
    "section": "16.3 Fixing The Problem",
    "text": "16.3 Fixing The Problem\n\nOne way to fix the problem\nDon’t.\nWe can’t get good estimates of the \\(\\hat\\beta\\)s, but we can still get good predictions.\n\nThis only works if the new values are in the same “tube” as the others.\nIf the multicollinearity is real, what estimates do you expect?\n\nWithout a controlled experiment, there isn’t a good way to estimate the effect of \\(X_1\\) on it’s own!\n\n\n\n\nRemoving predictors\nIf two predictors are measuring the same thing, then just include one?\n\nThis might lose some information!\n\nIt also might not!\n\nThe estimated \\(\\beta\\) won’t be meaningful.\n\nInferences will be difficult."
  },
  {
    "objectID": "L17-Multicollinearity.html#participation-questions",
    "href": "L17-Multicollinearity.html#participation-questions",
    "title": "16  L17: Multicollinearity",
    "section": "16.4 Participation Questions",
    "text": "16.4 Participation Questions\n\nQ1\nMulticollinearity can come from:\n\nUnit changes\nPolynomial terms\nProxy measures\nAll of the above\n\n\n\nQ2\nMulticollinearity is a problem because\n\nStrong correlation in \\(X\\) makes estimates of \\(\\beta\\) invalid.\nStrong correlation in \\(X\\) means there are many values of \\(\\underline\\beta\\) that are equally probable.\nThere’s no way to fix strong correlation in \\(X\\).\n\n\n\nQ3\nWhen multicollinearity is present, which of the following is still valid?\n\nInferences about the effect of one of the predictors.\nConfidence intervals for a single coefficients.\nPredictions.\nOverall F test for significance of any slope parameter.\n\n\n\nQ4\nThe VIF is defined as:\n\nThe amount that the MSE increases due to the variance in \\(\\hat{\\underline\\beta}\\).\nThe coefficient of determination of \\(X_i\\) against all other predictors.\nThe correlation between \\(X_i\\) and all other predictors.\nThe \\(R^2\\) value for \\(Y\\) against \\(X_i\\)."
  },
  {
    "objectID": "L18-Modelling_Poorly.html#motivation",
    "href": "L18-Modelling_Poorly.html#motivation",
    "title": "17  L18: Modelling Poorly",
    "section": "17.1 Motivation",
    "text": "17.1 Motivation\n\nSelecting Predictors\nAs you’ve seen from the assignment, choosing predictors is hard!\nWouldn’t it be nice if the computer would choose the best model for you?\n\n\nThe “Best” Model\n\nBest represents the Data Generating Process (DGP)\n\nBest for inference\n\nMisses the DGP, but provides useful insights into the relationships\n\nAlso best for inference\n\nFits the current data the best\n\nOverfitting?\n\nBest able to predict new values\n\nRandom Forests and Neural Nets\n\n\nThe “best” model depends on the goal of the study!"
  },
  {
    "objectID": "L18-Modelling_Poorly.html#automatic-predictor-selection",
    "href": "L18-Modelling_Poorly.html#automatic-predictor-selection",
    "title": "17  L18: Modelling Poorly",
    "section": "17.2 Automatic Predictor Selection",
    "text": "17.2 Automatic Predictor Selection\n\nModel Comparison Criteria\n\n\\(R^2\\), or adjusted \\(R^2\\)\n\nLower is better, but \\(R^2\\) increases as we add predictors\n\n\\(s^2\\), the residual variance.\n\nAdding predictors always decreases \\(s^2\\)\n\nMallow’s \\(C_p\\) statistic\n\n\\(C_p = RSS_p/s^2 - (n - 2p)\\)\n\n\\(RSS_p\\) is the RSS of the smaller model, with \\(p\\) parameters\n\\(s^2\\) is the MSE from the largest model under consideration\n\nAdding predictors does not increase this statistic.\n\nAIC, the Akiake Information Criterion\n\n\\(AIC = 2p - 2\\ln(\\hat L)\\), where \\(\\hat L\\) is the likelihood evaluated at the estimated parameters.\n\nE.g., when \\(\\epsilon_i\\sim N(0,\\sigma^2)\\), the likelihood is the product of normal distributions with a mean of \\(X\\hat{\\underline\\beta}\\) and variance \\(s^2\\).\n\nDoes not increase with added predictors.\n\n\n\n\nMore on AIC\n\\[\nAIC = 2p - 2\\ln(\\hat L)\n\\]\nRecall from Maximum Likelihood Estimation, the likelihood is the likelihood of observing the particular data, given the parameters: \\[\nL(y|\\underline\\beta, X, \\sigma) = \\prod_{i=1}^nf_Y(X|\\underline\\beta, \\sigma^2),\n\\] where \\(f_Y(X|\\underline\\beta, \\sigma^2)\\) is the normal distribution.\n\nA high AIC means we either:\n\nHave too many parameters, or\nOur model doesn’t fit the data well.\n\nA low AIC means we’ve got a good model that isn’t overly complicated\n\n“Low” is relative to other models\n\n\n\n\nBest Subset\n\nFind the collection of predictors that optimizes the statistic of interest.\n\nThat’s it. You just try them all.\n\n\nBackward and Forward Selection\n\nBackward Selection\n\nInclude all predictors, try removing one\n\nCheck the \\(R^2\\), p-values, Mallow’s Cp, or AIC\n\nPut that one back in the model, try removing another\n\nForward Selection\n\nFind the best predictor to include first\nFind the best predictor to include second\n\n\nBoth have some sort of stopping criteria.\n\n\nBackward Selection\n\nFit a model with all \\(p\\) predictors\nTry all models with \\(p-1\\) predictors.\n\nIdentify the best one, say remove \\(x_j\\)\nCheck stopping criteria.\n\nIf stopping critera not met, try all models with \\(p-2\\) predictors, not including \\(x_j\\).\n\n\n\nBackward Selection Example\n\nStart with mpg ~ disp + wt + am + cyl + qsec.\nCheck all of the AICs, remove cyl.\nCheck all of the AICs, remove disp.\nCheck all AICs, stop.\n\nFinal model: mpg ~ wt + am + qsec\n\n\nForward Selection\n\nStart with mpg ~ 1.\nTest each predictor individually, check AIC, keep wt.\nTest each remaining predictor, check AIC, keep cyl.\nTest each remaining predictor, stop.\n\nFinal model: mpg ~ wt + cyl\n\n\nBest Subset Selection\nTest every combination of predictors, keep the one with the lowest AIC (or other stat).\n\n\nThink-Pair-Share\nWhat might these methods be missing?\nWhen would these methods be useful?\n\n\nEvaluating Algorithmic Predictor Selection\nSuppose we have measured 30 predictors that we know are not related to the response.\nHow many predictors should Backwards Selection select?"
  },
  {
    "objectID": "L18-Modelling_Poorly.html#paticipation-questions",
    "href": "L18-Modelling_Poorly.html#paticipation-questions",
    "title": "17  L18: Modelling Poorly",
    "section": "17.3 Paticipation Questions",
    "text": "17.3 Paticipation Questions\n\nQ1\nWhy do Forward and Backward selection procedures not choose the same model?\n\nThey do choose the same model.\nBecause the order that the predictors enter the model matters.\nBecause the best model is different depending on which approach you use.\n\n\n\nQ2\nBest Subset selection will always choose the same model regardless of which statistic it’s based on (\\(R^2\\), Mallow’s \\(C_p\\), AIC, etc).\n\nTrue\nFalse\n\n\n\nQ3\nUnder the null hypothesis, p-values follow a uniform distribution.\n\nTrue\nFalse\nTrue, but only if you do the study right.\n\n\n\nQ4\nSuppose we have 30 predictors in the model. How many should we keep?\n\nMost of them.\nAbout half of them.\nA few of them.\nWhat? What kind of question is that? How could I possibly know without seeing the context of the problem?!?"
  },
  {
    "objectID": "L18-Modelling_Poorly.html#the-best-model-1",
    "href": "L18-Modelling_Poorly.html#the-best-model-1",
    "title": "17  L18: Modelling Poorly",
    "section": "17.4 The Best Model",
    "text": "17.4 The Best Model\n\nNeural Networks and Random Forests\n\nNeural Networks\n\nEssentially a series of linear regressions with a minor non-linear transformation.\nA “deep” neural network is non-linear transformations and all interactions.\nVery finicky, but very powerful when necessary.\n\nRandom Forests\n\nAlso a series of non-linear effects with interactions.\nMuch much much less finicky.\n\n\n\n\nCausal Inference\nExperiments are our way of controlling variables so that we can isolate their effect.\nMost data we tend to use is observational.\n\nCausal inference is statistical magic to determine causality from observation.\n\n… with varying degrees of success.\n\n\n\n\nWhy are you telling us this, Devan?\nWhich model is “best”?\n\nBest predictions?\n\nNN and RF, with cross-validation.\n\nBest inference?\n\nBuild a model based on the context of the problem.\nChoose transformations and interactions appropriately.\nOnly check p-values at the very end.\n\nBest subset of predictors?\n\nRecall: multicollinearity. Without an experiment, correlated predictors mean that there’s no way to tell which predictors are best.\n\n\nOpinion: Algorithmic selection methods are bad approximations to better techniques that are outside out the scope of this course."
  },
  {
    "objectID": "L19-Example_Analysis_mtcars.html#exploratory-data-analysis",
    "href": "L19-Example_Analysis_mtcars.html#exploratory-data-analysis",
    "title": "18  L19: Analysis of MTCars",
    "section": "18.1 Exploratory Data Analysis",
    "text": "18.1 Exploratory Data Analysis\n\nUnderstanding Data\nWhat do the column names mean?\nThe help file gives a (very) brief description. I spent a few minutes just looking at the descriptions and trying to guess what relationships I might find.\nOverall, most of the predictors are trying to answer the question “Is this a powerful car?”\n\n\nPlotting the Data\nFrom a pairs plot (pairs(mtcars), which I have not included to reduce the amount of output):\n\nOnly 1 car has carb = 6, 1 has carb = 8\nwt and drat are (-ively) correlated\n\ndisp and hp\ndisp and drat (-ive)\ndisp and wt\nhp and wt\nhp and qsec\n\n\nwt and disp are clearly multicollinear, and they’re measuring the same thing so I might want to include just one of them.\n\n\nPatterns in the Predictors\nIn the following code, I tried x as am, cyl, gear, and carb. The y axis was wt, disp, drat, and qsec. I essentially tried every combination of these and wrote down the most interesting patterns.\n\n# Continuous versus categorical\nggplot(mtcars) +\n    aes(x = factor(am), y = wt) +\n    geom_boxplot()\n\n\n\n\n\nwt is different across categories of am, cyl, carb, gear (all positive)\n\ndisp has same relationships\nhp has same relationships, except 4 gear cars have lower hp than 3 and 5 gear cars\ndrat has opposite relationships\n\n\nI did something similar with the following code, checking every combination of all relevant predictors and writing down anything that stuck out to me.\n\n# Continuous vs. continuous\nggplot(mtcars) +\n    aes(x = disp, y = wt, colour = factor(cyl)) +\n    geom_point()\n\n\n\n\n\nClear separation between disp and wt when coloured by am or cyl.\n\nIn other words, there are distinct groups. This probably means that one of the continuous predictors has all of the information necessary, and it won’t be necessary to include an interaction between continuous predictors (it rarely is).\n\nOtherwise, there are not many relationships that might be present.\n\nThe following plot was also used with all combinations of categorical predictors.\n\n# categorical variables\nggplot(mtcars) + \n    aes(x = factor(am), fill = factor(vs)) +\n    geom_bar(position = \"dodge\")\n\n\n\n\n\nSome kind of “correlation” between am and cyl.\n\nMeasuring something similar, but from different perspectives.\n\nVery little relation between am and vs - they’re measuring different things.\n\nMight be worth checking models where am is switched with vs. \n\n\n\n\nConclusions\nMost things are measuring “how powerful is this car”, so we should just choose the ones that make sense to us and check a few categorical predictors.\nwt and disp make the most sense as measures for mpg, and am and cyl also make some sense. I’ll try switching out some of the other predictors, but I expect that the final model will either be wt*am or disp*cyl."
  },
  {
    "objectID": "L19-Example_Analysis_mtcars.html#more-eda-relationships-with-the-response-interactions",
    "href": "L19-Example_Analysis_mtcars.html#more-eda-relationships-with-the-response-interactions",
    "title": "18  L19: Analysis of MTCars",
    "section": "18.2 More EDA: Relationships with the Response / Interactions",
    "text": "18.2 More EDA: Relationships with the Response / Interactions\nNow we’re finally looking at mpg!\n\nggplot(mtcars) +\n    aes(y = mpg, x = disp, colour = factor(cyl)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = TRUE, formula = y ~ x)\n\n\n\n\nFrom looking at many many plots, I propose the following candidate models:\n\nmpg versus disp * cyl\nmpg versus wt * am (cyl?)\nmpg versus wt * vs (maybe not an interaction)\nmpg versus wt * gear?\n\nI had also considered including qsec, but a plot of mpg versus qsec with colours from cyl revealed that cyl explains the relationship; if we include cyl, then the slope for mpg versus qsec is 0. The same thing happens with drat, so cyl is probably enough to include in the model rather than either qsec or drat."
  },
  {
    "objectID": "L19-Example_Analysis_mtcars.html#modelling",
    "href": "L19-Example_Analysis_mtcars.html#modelling",
    "title": "18  L19: Analysis of MTCars",
    "section": "18.3 Modelling",
    "text": "18.3 Modelling\nLet’s test out our models!\nAgain, to reduce the amount of output I have to wade through, I changed the following code a bunch and left it at something meaningful to my final analysis.\n\ndispmodel &lt;- lm(mpg ~ disp * factor(cyl), data = mtcars)\n\npar(mfrow = c(2,2))\nplot(dispmodel, col = mtcars$cyl)\n\n\n\n\n\nResiduals versus fitted looks good\nQQ norm looks great! For this small of a data set, we don’t expect much from the qq-plot, so this is actually very nice.\nScale-Location has a slight U shape, which isn’t ideal. There may still be a predictor that’s worth including.\nThere’s a high influence point. This is likely due to the interaction between cyl and disp.\n\nWhen we have this kind of interaction, there are essentially three lines, each with fewer observations. It is much easier for a point to be influential with interaction present.\n\n\n\nwtmodel &lt;- lm(mpg ~ wt * am, data = mtcars)\n\npar(mfrow = c(2,2))\nplot(wtmodel)\n\n\n\n\n\nFirst plot looks good!\nQQplot has some heavy tails - not bad, but not ideal. dispmodel was better.\nScale-location is great!\nNo high leverage points.\n\nBoth models are good in different ways. Let’s check their summaries.\n\nsummary(dispmodel)\n\n\nCall:\nlm(formula = mpg ~ disp * factor(cyl), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4766 -1.8101 -0.2297  1.3523  5.0208 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        40.87196    3.02012  13.533 2.79e-13 ***\ndisp               -0.13514    0.02791  -4.842 5.10e-05 ***\nfactor(cyl)6      -21.78997    5.30660  -4.106 0.000354 ***\nfactor(cyl)8      -18.83916    4.61166  -4.085 0.000374 ***\ndisp:factor(cyl)6   0.13875    0.03635   3.817 0.000753 ***\ndisp:factor(cyl)8   0.11551    0.02955   3.909 0.000592 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.372 on 26 degrees of freedom\nMultiple R-squared:  0.8701,    Adjusted R-squared:  0.8452 \nF-statistic: 34.84 on 5 and 26 DF,  p-value: 9.968e-11\n\n\n\nsummary(wtmodel)\n\n\nCall:\nlm(formula = mpg ~ wt * am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6004 -1.5446 -0.5325  0.9012  6.0909 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  31.4161     3.0201  10.402 4.00e-11 ***\nwt           -3.7859     0.7856  -4.819 4.55e-05 ***\nam           14.8784     4.2640   3.489  0.00162 ** \nwt:am        -5.2984     1.4447  -3.667  0.00102 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.591 on 28 degrees of freedom\nMultiple R-squared:  0.833, Adjusted R-squared:  0.8151 \nF-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11\n\n\nThe \\(R^2\\) for dispmodel is a fair bit higher (although there’s no standard for how much an \\(R^2\\) should change, so this might not be a meaningful difference). As we saw in class, the \\(R^2\\) is based on the same quantities as the F-test for different models.\n\nanova(dispmodel, wtmodel)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp * factor(cyl)\nModel 2: mpg ~ wt * am\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     26 146.23                              \n2     28 188.01 -2   -41.773 3.7136 0.03814 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe models fit significantly differently. Which one fits better?\n\n# MSE values\nsummary(dispmodel)$sigma\n\n[1] 2.371581\n\nsummary(wtmodel)$sigma\n\n[1] 2.591247\n\n\ndispmodel has a higher \\(R^2\\) and a lower MSE, so it seems to be the winner.\nFrom the pairs plot, I saw that disp has a slight relationship with other continuous predictors, and the scale-location plot wasn’t perfect. Perhaps another predictor will help?\nI can do this with the magical update() function. The ~ . + hp notation means the response versus (~) everything ., then add hp. The ~ means “versus” (with the response on the left, which isn’t allowed to change in this case, and the predictors on the right), and the . means “everything”, which in this case refers to everything that was already in the model. The form lm(mpg ~ ., data = mtcars) will fit mpg against everything else it sees in the mtcars dataset.\n\nsummary(update(dispmodel, ~ . + hp)) \n\n\nCall:\nlm(formula = mpg ~ disp + factor(cyl) + hp + disp:factor(cyl), \n    data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3442 -1.7647  0.0994  1.4480  4.4796 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        41.53521    3.04752  13.629 4.48e-13 ***\ndisp               -0.13037    0.02798  -4.660 9.00e-05 ***\nfactor(cyl)6      -19.95406    5.48572  -3.637 0.001249 ** \nfactor(cyl)8      -16.99535    4.83011  -3.519 0.001685 ** \nhp                 -0.01410    0.01184  -1.191 0.245018    \ndisp:factor(cyl)6   0.12975    0.03685   3.521 0.001675 ** \ndisp:factor(cyl)8   0.11199    0.02946   3.801 0.000824 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.353 on 25 degrees of freedom\nMultiple R-squared:  0.8771,    Adjusted R-squared:  0.8476 \nF-statistic: 29.74 on 6 and 25 DF,  p-value: 3.199e-10\n\n\nI checked qsec, drat, and hp, and none seemed worth including in the model. I’ll just leave it as is.\nTo interpret the model we must be careful about the interaction term!\n\\[\nmpg = \\begin{cases}\n\\beta_0 + \\beta_1 disp & \\text{if }cyl == 4\\\\\n(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_4) disp & \\text{if }cyl == 6\\\\\n(\\beta_0 + \\beta_3) + (\\beta_1 + \\beta_5) disp & \\text{if }cyl == 8\\\\\n\\end{cases}\n\\]\n\nFor 4 cylinder cars, the baseline mpg is 40 and decreases by 0.135 for each one unit increase in disp.\nFor 6 cylinder cars, the baseline mpg is about 21.5 and isn’t really related to the displacement.\nFor 8 cylinder cars, the baseline mpg is about 24.5 and decreases by about 0.02 for each one-unit increase in displacement.\n\nNote that displacement has really large units, so 0.02 over hundreds of one-unit increases is still a lot!"
  },
  {
    "objectID": "L20-Degrees_of_Freedom.html#generating-data-for-the-project",
    "href": "L20-Degrees_of_Freedom.html#generating-data-for-the-project",
    "title": "19  L20: Degrees of Freedom",
    "section": "19.1 Generating Data (for the Project)",
    "text": "19.1 Generating Data (for the Project)\n\nmodel.matrix() and Matrix Multiplication\n\nYou may find it convenient to use the model.matrix() function to set up dummy variables and polynomial terms for you. It will make sure that there is a reference category, which it will choose alphabetically.\nAlso note that you’ll want to set raw = TRUE to ensure that people can find the coefficient values that you set. With raw = FALSE they will get the exact same predictions (and thus the same error), but not the same coefficient values.\nIn the code below, I also demonstrate a log-transform for \\(y\\). To get a log on the left side of the equation, we use an exponential on the right.\n\n\nmycat &lt;- c(\"category\", \"category\", \"dogegory\", \"category\", \"birdegory\")\nmycont &lt;- c(12, 14, 4, 10,  20 )\n\nX &lt;- model.matrix(~ mycat + poly(mycont, 2, raw = TRUE))\nX\n\n  (Intercept) mycatcategory mycatdogegory poly(mycont, 2, raw = TRUE)1\n1           1             1             0                           12\n2           1             1             0                           14\n3           1             0             1                            4\n4           1             1             0                           10\n5           1             0             0                           20\n  poly(mycont, 2, raw = TRUE)2\n1                          144\n2                          196\n3                           16\n4                          100\n5                          400\nattr(,\"assign\")\n[1] 0 1 1 2 2\nattr(,\"contrasts\")\nattr(,\"contrasts\")$mycat\n[1] \"contr.treatment\"\n\n# Names are just for my own purposes, they don't need to be included.\n# The names help me keep track of which column in X they correspond to.\nbetas &lt;- c(intercpt = 10, cat = 20, dog = 0, c1 = 0, c2 = 0.05)\nX %*% betas\n\n  [,1]\n1 37.2\n2 39.8\n3 10.8\n4 35.0\n5 30.0\n\nmydf &lt;- data.frame(\n    y = exp(X %*% betas + rnorm(5, 0, 0.01)), \n    mycat = mycat, \n    mycont = mycont\n)\ncoef(lm(log(y) ~ mycat + poly(mycont, 2, raw = TRUE), data = mydf))\n\n                 (Intercept)                mycatcategory \n                9.712145e+00                 1.978742e+01 \n               mycatdogegory poly(mycont, 2, raw = TRUE)1 \n                5.406038e-04                 8.415754e-02 \npoly(mycont, 2, raw = TRUE)2 \n                4.652628e-02 \n\n\nFor the project, you ould only need to submit the mydf object; the others should be able to recover your coefficient values from that alone!"
  },
  {
    "objectID": "L20-Degrees_of_Freedom.html#df",
    "href": "L20-Degrees_of_Freedom.html#df",
    "title": "19  L20: Degrees of Freedom",
    "section": "19.2 df",
    "text": "19.2 df\n\nDegrees of Freedom\nA measure of how much information you chose to put in the model.\n\nContinuous predictors add 1\nCategorical predictors add \\(k-1\\)\nEach term in a polynomial adds 1\nInteractions add 1\netc.\n\n\n\nChoose it and Use it\n\nDecide on the df, try to use all of them\n\nChoice is based on how much information you think you can extract.\nMany rows = much information; you can probably use more predictors.\n\n\n\n\nBad use of df\n\nDiscretising a categorical variable\n\nYou better have a reeeaaaalllllyyyy good justification\nFor example, discretising age to match up with insurance categories.\nYou should almost never choose to discretise based on your own logic; only to fit in with other analyses or use cases.\n\nPolynomial terms when interactions would work.\nRedundant categories\n\nCategories that are redundant\nIf you have redundant categories, then you have categories that are redundant\nFor example, if the “JobTitle” column has entries like “Data Scientist” as well as “Data Scientist and Machine Learning Expert”, you could just code those both as “DS”.\n\n\n\n\nSaving df\n\nCombine categories\n\nJust “Data Scientist” or “Software Engineer”\n\nCombine predictors\n\nVolume of beak = \\(\\pi r^2h/3\\)?\n\nTransform response rather than add polynomial terms\n\nNot always recommended.\n\n\n\nHere’s an example of using transformations to (1) match the context of the problem and (2) get better results with fewer degrees of freedom.\nWe’ll use the trees dataset that’s built into R. The “Girth” column is actually the diameter, and it’s the only column measured in inches rather than feet. I’m going to make a new predictor based on Girth that’s more useful for later models.\n\n# Saving df\nhead(trees) # \"Girth\" is actually diameter, according to help file\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\ntrees$Radius &lt;- trees$Girth/24\n\nA naive model might be a basic multiple linear regression.\n\nmultiple_lm &lt;- lm(Volume ~ Radius + Height, data = trees)\nsummary(multiple_lm)\n\n\nCall:\nlm(formula = Volume ~ Radius + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nRadius      112.9959     6.3424  17.816  &lt; 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: &lt; 2.2e-16\n\n\nWe could make this fit better by blindly adding polynomial terms and doing a transformation:\n\ntransformed_and_poly &lt;- lm(log(Volume) ~ poly(Girth, 2) + poly(Height, 2), data = trees)\nsummary(transformed_and_poly)\n\n\nCall:\nlm(formula = log(Volume) ~ poly(Girth, 2) + poly(Height, 2), \n    data = trees)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.16094 -0.04023 -0.00295  0.05474  0.13434 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3.27273    0.01492 219.360  &lt; 2e-16 ***\npoly(Girth, 2)1   2.51150    0.09732  25.808  &lt; 2e-16 ***\npoly(Girth, 2)2  -0.26046    0.09206  -2.829  0.00887 ** \npoly(Height, 2)1  0.54845    0.09746   5.628 6.47e-06 ***\npoly(Height, 2)2 -0.05518    0.09191  -0.600  0.55349    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08307 on 26 degrees of freedom\nMultiple R-squared:  0.9784,    Adjusted R-squared:  0.9751 \nF-statistic: 294.5 on 4 and 26 DF,  p-value: &lt; 2.2e-16\n\n\n\nIt is interesting that the squared term for height is not significant. Why is this so interesting to me? Look at the next equation in this lesson…\n\nA slightly better model might be one of the form \\[\nV = \\pi r^2h\n\\] which assumes that trees are perfect cylinders. This can be accomplished by modelling: \\[\\begin{align*}\n\\log(V) &= \\beta_0 + \\beta_1\\log(r) + \\beta_2\\log(h) + \\epsilon\\\\\n\\implies V &= \\exp(\\beta_0)r^\\beta_1h^\\beta_2\\exp(\\epsilon)\n\\end{align*}\\] and expecting that \\(\\exp(\\beta_0)\\) is close to \\(\\pi\\), \\(\\beta_1 = 2\\), and \\(\\beta_2 = 1\\).1\n\nvolume_logs &lt;- lm(log(Volume) ~ log(Girth) + log(Height), data = trees)\nsummary(volume_logs)\n\n\nCall:\nlm(formula = log(Volume) ~ log(Girth) + log(Height), data = trees)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.168561 -0.048488  0.002431  0.063637  0.129223 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.63162    0.79979  -8.292 5.06e-09 ***\nlog(Girth)   1.98265    0.07501  26.432  &lt; 2e-16 ***\nlog(Height)  1.11712    0.20444   5.464 7.81e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08139 on 28 degrees of freedom\nMultiple R-squared:  0.9777,    Adjusted R-squared:  0.9761 \nF-statistic: 613.2 on 2 and 28 DF,  p-value: &lt; 2.2e-16\n\n\n\nWe get something close to our hopes!\n\nExcept for \\(\\beta_0\\), which we’ll talk about later.\n\nWith this model, we could do a hypothesis test for \\(\\beta_1 = 2\\) and \\(\\beta_2 = 1\\).\n\nIf these are reasonable values, loggers could confidently calculate the volume of a tree assuming that it’s a cylinder!\n\n\nWe could also assume these values from the start, and include a “naive” volume.\n\ntrees$naive_volume &lt;- pi * trees$Radius^2 * trees$Height\n\nWe could then model this according to \\[\\begin{align*}\nV & = \\beta_0N + \\epsilon\n\\end{align*}\\] where \\(N\\) is our “naive” volume. We might have the expectation that \\(\\beta_0 = 1\\) if the naive volume is correct.\n\n# Assuming trees are cylinders\ndiff_from_cylinder &lt;- lm(Volume ~ -1 + naive_volume, data = trees)\nsummary(diff_from_cylinder)\n\n\nCall:\nlm(formula = Volume ~ -1 + naive_volume, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6696 -1.0832 -0.3341  1.6045  4.2944 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \nnaive_volume 0.386513   0.004991   77.44   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.455 on 30 degrees of freedom\nMultiple R-squared:  0.995, Adjusted R-squared:  0.9949 \nF-statistic:  5996 on 1 and 30 DF,  p-value: &lt; 2.2e-16\n\n\nThis tells us that the estimated usable lumber from a given tree is about 40% of what we would expect if the tree were a perfect cylinder.\nImportantly for this lecture, we have an \\(R^2\\) of 0.9949 on a single degree of freedom! \\(R^2\\) is not the greatest measure, but it’s informative in this case:\n\n\n\n\n\nmodel\nR2\ndf\n\n\n\n\nmultiple_lm\n0.9442322\n2\n\n\ntransformed_and_poly\n0.9750853\n4\n\n\nvolume_logs\n0.9760840\n2\n\n\ndiff_from_cylinder\n0.9948560\n1\n\n\n\n\n\nBy choosing our transformations carefully, we have a model that is both better and simpler! The coefficient estimate also relates to a physical quantity that is useful to us - the percent of usable wood we can get from a tree! Statistics is amazing.2\n\n\n\nResearcher Degrees of Freedom\nYou add information that isn’t measured by df!\n\nChoosing one predictor rather than another.\n\nBill length or bill depth?\n\nTransforming a predictor/response\nRemoving outliers\nUsing/not using autoregressive error structures\netc.\n\nThis is why we use RMarkdown/Quarto/Jupyter - all of this is (should be) recorded!"
  },
  {
    "objectID": "L20-Degrees_of_Freedom.html#footnotes",
    "href": "L20-Degrees_of_Freedom.html#footnotes",
    "title": "19  L20: Degrees of Freedom",
    "section": "",
    "text": "It makes me very happy that we’re taking the “log” when talking about lumber.↩︎\nT-shirt idea: “If you don’t think stats is lit af then you ain’t woke, fam!”↩︎"
  },
  {
    "objectID": "L21-Regularization.html#loss-functions",
    "href": "L21-Regularization.html#loss-functions",
    "title": "20  L21: Regularization Methods",
    "section": "20.1 Loss Functions",
    "text": "20.1 Loss Functions\n\nWhy are we minimizing the sum of squares?\nThe MSE is defined as: \\[\nMSE(\\underline\\beta) = \\frac{1}{n}\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\n\\] This is the Maximum Likelihood Estimate, which seeks to model the mean of \\(Y\\) at each value of \\(X\\), \\(E(Y) = X\\underline\\beta\\), with Gaussian errors. \nMSE, seen as a function of \\(\\underline\\beta\\), is a loss function, i.e. the function we minimize to find our estimates.\nBut it’s FAR from the only loss function.\n\n\nOther loss functions\nBy minimizing \\[\nMAE(\\underline\\beta) = \\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right|\n\\] we end up estimating the \\(median\\) of \\(Y\\). \nOthers:\n\nMean Absolute Log Error\n\nLower penalty for larger errors.\nMore robust to outliers?\n\nMean Relative Error\n\nPenalize errors relative to size of \\(y\\) (larger errors at large \\(y\\) values aren’t as big of a deal).\nAssumes that variance depends on mean (kinda like Poisson).\n\netc.\n\n\n\nExamples\n\n0.5 hectares verus 4 hectares can make a huge difference\n\n100 versus 150, congrats on the great prediction!\n\nPredicting an income of 15,000 versus 25,000 is big\n\nModelling the average income is not usually reasonable.\n\n\n\n\nLoss Function Summary\n\nMinimize the loss function with respect to the parameters of interest.\nFor the same parameters, there can be many loss functions.\nOther names:\n\nLikelihood function (special case of loss function)\nCost function (synonym)"
  },
  {
    "objectID": "L21-Regularization.html#regularization",
    "href": "L21-Regularization.html#regularization",
    "title": "20  L21: Regularization Methods",
    "section": "20.2 Regularization",
    "text": "20.2 Regularization\n\nRegular Methods\nOrdinary least squares is a minimization problem: \\[\nRSS = \\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\n\\]\nWhat if I don’t like how big the \\(\\underline\\beta\\) values are?\n\n\nRegularization Constraints\nLet’s arbitrarily say that \\(\\sum_{j=1}^p \\beta_j = 10\\).\nWith this constraint, one large \\(\\beta_j\\) can be countered by a large negative \\(\\beta_k\\).\n\n\nRegularizing with Norms\nThe \\(L_p\\)-norm of a vector is: \\[\n||\\beta||_p = \\left(\\sum_{j=1}^p|\\beta|^p\\right)^{1/p}\n\\]\n\nWhen \\(p = 2\\), this is the Euclidean distance.\n\nPythagoras strikes again!\n\nWhen \\(p = 1\\), this is the sum of absolute values.\nWhen \\(p=\\infty\\), this ends up being max(\\(|\\underline\\beta|\\)).\n\nNot useful for our purposes, but interesting!\n\n\n\n\nWhy choose \\(||\\underline\\beta||_p = 10\\)?\nOr, in general, why choose a particular value of \\(s\\) in \\(||\\underline\\beta||_p = s\\)?\n\nThere’s no good reason to choose a particular value of \\(s\\), but regularizing stops us from having steep slopes for predictors that aren’t actually related.\nIn other words, we ignore spurious patterns!\nToo little regularization and we just have the OLS estimate. Too much regularization and we restrict the parameters too much.\n\n\n\nChoosing \\(s\\) in \\(||\\underline\\beta||_p = s\\)\n\nRecall: more flexible models are able to estimate more subtle patterns, but may find patterns that aren’t there.\n\nToo flexible = bad out-of-sample prediction.\n\nFor linear models, the least flexible model is one where all \\(\\beta_j\\) values are given a fixed value.\n\nFor example, all are 0. \n\n\nFor a linear model, restricting the values with \\(||\\underline\\beta||_p = s\\) reduces flexibility, which can improve out-of-sample prediction performance.\n\n\nMLE estimates of \\(\\underline\\beta\\) are unbiased\n… therefore constrained estimates are biased.\n\n\nBut what about the scales of the features?\nWhat a great question! Thank you so much for asking! You must be smart.\nFor \\(||\\underline\\beta||_p\\) to make sense, the predictors must all have the same scale.\nThis is accomplished by standardizing the features: Replace each \\(x_{ij}\\) with \\[\n\\frac{x_{ij} - \\bar{\\mathbf{x}_{j}}}{\\frac{1}{n}\\sum_{i=1}^n(x_{ij} - \\bar{\\mathbf{x}_{j}})^2}\n\\]\n\n\nChoosing \\(\\lambda\\) via cross-validation\n\nFor each value of \\(\\lambda\\):\n\nSplit data into 5 “folds”.\nFor each “fold”:\n\nSet aside the data points in the current fold.\nFit the model to data in all other “folds” using your value of \\(\\lambda\\).\nPredict the missing points, record the average error.\n\n\n\nChoose the lambda with the lowest out-of-sample prediction error.\n\n\nCross-Validation\n\n\n\nSpecial Cases of Regularization: \\(L_1\\) or \\(L_2\\)?\nSo far, we’ve been talking about general \\(L_p\\) norms, i.e. \\(||\\underline\\beta||_p\\).\n\n\\(L_1\\): LASSO\n\\(L_2\\): Ridge\n\n\n\nGeometric Interpretation (Contours of the RSS)\n\n\n\n\n\nLASSO will set coefficients to 0.\n\n“Least Absolute Shrinkage and Selection Operator”\n\nRidge has less variance (why?)\n\n\n\n\n\nLangrangian Multipliers and Estimation\nWikipedia screenshot:\n\n\n\nLagrangian Multipliers and Estimation\nMinimize \\(MSE(\\underline\\beta)\\) subject to \\(||\\underline\\beta||_p\\).\nis equivalent to\nMinimize \\(MSE(\\underline\\beta) + \\lambda||\\underline\\beta||_p\\)\nFor the rest of your life, this is the way you’ll see Ridge and LASSO.\n\nRidge: Analytical solution, can calculate an arbitrary number of \\(\\lambda\\) values at once.\nLASSO: Non-iterative numerical technique\n\n\n\nRidge Regularization\n\nOne of the coefficients increases with a tighter constraint!\n\n\n\nLASSO Feature Selection as we Vary \\(\\lambda\\)\n\n\n\nAs \\(\\lambda\\) increases, more coefficients are allowed to be non-zero.\nIf \\(\\lambda\\) doesn’t constrain, we get the least squares estimate.\n\nDenoted as \\(\\hat\\beta\\) in the plot."
  },
  {
    "objectID": "L21-Regularization.html#participation-questions",
    "href": "L21-Regularization.html#participation-questions",
    "title": "20  L21: Regularization Methods",
    "section": "20.3 Participation Questions",
    "text": "20.3 Participation Questions\n\nQ1\nFor Ridge regression (L2 norm): as \\(\\lambda\\rightarrow\\infty\\), \\(\\sum_{j=1}^p|\\beta_j| \\rightarrow\\infty\\).\n\nTrue\nFalse\n\n\n\nQ2\nWhen we set the restriction on the sum of the absolute value of the coefficients, the intercept is included.\n\nTrue\nFalse\n\n\n\nQ3\nWhy would we restrict the value of the parameters?\n\nDepending on the context of the problem, we might have a specific maximum value for the sum of the coefficients.\nWe want to avoid overfitting the data, and restricting the parameter stops us from modelling irrelevant patterns.\nBecause in any given regression problem there are always predictors that should have a slope of 0.\nBecause some parameter estimates are impossible.\n\n\n\nQ4\nWhat’s the primary practical difference between Ridge and LASSO?\n\nLASSO has a tighter restriction on the parameters than Ridge.\nLASSO will set parameters to 0, whereas Ridge will just shrink them towards 0 without making them exactly 0.\nBecause coefficients are set to 0, LASSO has higher variance in its estimates.\nThe only difference is the norm that they use, which has no meaningful impact on the results.\n\n\n\nQ5\nWhat is cross-validation trying to minimize?\n\nThe out-of-sample prediction error.\nThe bias in the estimates.\nThe loss function (MSE).\n\n\n\nQ6\nWhen should you use regularization?\n\nWhen you want un unbiased estimate of the population parameters.\nWhen you want to avoid overfitting.\nWhen you want to be regular/normal/usual/typical.\n\n\n\nQ7\nWhen should you use LASSO instead of Ridge?\n\nWhen you want to do subset selection in a way that minimizes out-of-sample prediction error.\nWhen you want most of the coefficients to be 0.\nWhen you want to minimize out-of-sample prediction error at all costs.\nWhen you only want coefficients with significant p-values left over in your model.\n\n\n\nPersonal Opinion Time\nWith the existence of LASSO, there’s no reason to do automated feature selection.\nBest subset selection can be written as: \\[\n\\text{Minimize } MSE(\\underline\\beta)\\text{ subject to }\\sum_{j=1}^pI(\\beta\\ne 0) \\le s\n\\] This can minimize out-of-sample error, but results in something that could be mistaken for inference.\nWith LASSO, you know the estimates are biased and you know why. Best subset tricks you into thinking your \\(\\underline\\beta\\) estimates are accurate - they are not.\n\n\nImplementation in R: glmnet\n\nThe glm in glmnet is because it fits all GLMs.\n\nIncluding Logistic Regression.\nThe family = binomial argument works as in glm()\n\nHowever, family = \"binomial\" is an optimized version.\n\n\nThe net in glmnet refers to elasticnet.\n\nNext slide or two.\n\n\n\n\nElastic Net: Like a lasso, but more “flexible”\n\\[\n\\text{Minimize } MSE(\\underline\\beta) + \\lambda\\left[\\alpha||\\underline\\beta||_1 + (1-\\alpha)||\\underline\\beta||_2\\right]\n\\]\nElastic Net is “doubly regularized”.\nElastic net needs more time to fit and needs more data.\n\n\nElasticnet and LASSO/Ridge\n\\[\n\\text{Minimize } MSE(\\underline\\beta) + \\lambda\\left[\\alpha||\\underline\\beta||_1 + (1-\\alpha)||\\underline\\beta||_2\\right]\n\\]\n\n\\(\\alpha = 0 \\implies\\) Ridge\n\\(\\alpha = 1 \\implies\\) LASSO\n\n\nHere’s an example of LASSO in R. We’ll load in the Wage data from ISLR2 package1.\nThis data set has a column for wage and a column for logwage. We’re going to use wage as our response, and removing wage makes it easier to tell R to use all columns other than logwage. I also remove region since there are some regions with too few observations and I am not going to set up cross-validation appropriately for this scenario.\n\nlibrary(glmnet) # cv.glmnet() and glmnet()\nlibrary(ISLR2) # Wage data set\n\nWage &lt;- ISLR2::Wage\n# From names(Wage), I want to remove \"region\" and \"wage\"\nWage &lt;- Wage[, -c(6, 11)]\n\nglmnet doesn’t use the formula notation (y ~ x); we have to manually set up the design matrix (including dummy variables) and the response vector.\n\nX &lt;- model.matrix(logwage ~ ., data = Wage)[,-1]\ny &lt;- as.numeric(Wage$logwage)\n\nThe first step to fitting a LASSO model is choosing \\(\\lambda\\) via cv. The cv.glmnet() function does this for us. The results are not a final model; the resultant object gives us an idea of which value of \\(\\lambda\\) is appropriate.\n\ncv_check &lt;- cv.glmnet(x = X, y = y, alpha = 1)\nplot(cv_check)\n\n\n\n\nThe first dotted line indicates the value of \\(\\lambda\\) that minimizes the “loss function.” However, across different samples we would get different values of \\(\\lambda\\). Because we know there’s randomness, we know that a slightly larger (more restrictive) value of \\(\\lambda\\) would also be consistent with our data. Since cross-validation emulates the idea of having many samples, we can get an estimate of the standard error of \\(\\lambda\\). We can then choose the value of \\(\\lambda\\) that is within 1 standard error of the minimum. This gives a much simpler model while still having a plausible \\(\\lambda\\).2\nNow that we have a way of telling R what value we want for lambda, we can fit the model.\n\nmy_lasso &lt;- glmnet(X, y, lambda = cv_check$lambda.1se)\nmy_lasso\n\n\nCall:  glmnet(x = X, y = y, lambda = cv_check$lambda.1se) \n\n  Df  %Dev Lambda\n1  9 35.59 0.0127\n\n\nThe output isn’t very informative, but the model can make predictions via the predict() function and these will be comparable or better than the predictions from an unconstrained linear model.\nLet’s compare the coefficient values to see the shrinkage in action! Of course, glmnet standardizes by default, so we need to ensure that the linear model is based on standardized predictors.\nIn the output, I include a column for the difference in the coefficients. Specifically, it’s lm minus lasso, so we may expect “shrinkage” to mean that the lasso estimates are smaller.\n\nstandardized_X &lt;- apply(X, 2, scale)\nstandardized_lm &lt;- lm(y ~ standardized_X)\ncoef_mat &lt;- cbind(coef(my_lasso),\n    coef(standardized_lm))\n\nres &lt;- cbind(\n        coef_mat, \n        apply(coef_mat, 1, function(x) abs(x[2]) - abs(x[1]))\n    ) |&gt; \n    round(3)\ncolnames(res) &lt;- c(\"lasso\", \"lm\", \"|lm|-|lasso|\")\nres\n\n17 x 3 sparse Matrix of class \"dgCMatrix\"\n                             lasso     lm |lm|-|lasso|\n(Intercept)                 -7.830  4.654       -3.176\nyear                         0.006  0.026        0.019\nage                          0.002  0.029        0.027\nmaritl2. Married             0.128  0.076       -0.052\nmaritl3. Widowed             .      0.004        0.004\nmaritl4. Divorced            .      0.012        0.012\nmaritl5. Separated           .      0.017        0.017\nrace2. Black                 .     -0.012        0.012\nrace3. Asian                 .     -0.005        0.005\nrace4. Other                 .     -0.007        0.007\neducation2. HS Grad          .      0.038        0.038\neducation3. Some College     0.058  0.075        0.018\neducation4. College Grad     0.166  0.119       -0.047\neducation5. Advanced Degree  0.313  0.151       -0.162\njobclass2. Information       0.017  0.013       -0.004\nhealth2. &gt;=Very Good         0.041  0.027       -0.014\nhealth_ins2. No             -0.189 -0.089       -0.100\n\n\n\nThe estimates aren’t all smaller! Lasso chose to set some to 0, which freed up some coefficient “budget” to spend elsewhere."
  },
  {
    "objectID": "L21-Regularization.html#footnotes",
    "href": "L21-Regularization.html#footnotes",
    "title": "20  L21: Regularization Methods",
    "section": "",
    "text": "ISLR stands for Introduction to Statistical Learning with R, a fantastic (and free) book if you want to learn more advanced topics in predictive modelling!↩︎\nThis is similar to the Box-Cox transformation, where we find a bunch of plausible transformations, and go with a simple one like \\log() or sqrt().↩︎"
  },
  {
    "objectID": "L22-Logistic.html#logistic-regression",
    "href": "L22-Logistic.html#logistic-regression",
    "title": "21  L22: Classification",
    "section": "21.1 Logistic Regression",
    "text": "21.1 Logistic Regression\n\nGoal: Predict a 1\n\nResponse: 0 or 1\n\nPredictions: probability of a 1?\n\n\n\n\nThe Logistic Function - A Sigmoidal Function\nIf \\(t\\in\\mathbb{R}\\), then \\[\n\\sigma(t) = \\dfrac{\\exp(t)}{1 + \\exp(t)}\\in[0,1]\n\\] where \\(\\sigma(\\cdot)\\) is the logistic function.\n\n\n\n\n\n\n\nLogistic Function - Now with Parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Function - Now with Parameters Estimated from DATA\n\\[\\begin{align*}\n\\eta(x_i) &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + ...\\\\\np(x_i) &= \\sigma(\\eta(x_i)) = \\dfrac{\\exp(\\eta(x_i))}{1 + \\exp(\\eta(x_i))}\\\\\n\\implies \\log\\left(\\frac{p_i(x_i)}{1-p_i(x_i)}\\right) &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + ...\n\\end{align*}\\]\n\nlibrary(ISLR2)\nlibrary(ggplot2)\nlibrary(dplyr)\n#| echo: false\nDefault %&gt;% \n    mutate(default = as.numeric(factor(default)) - 1) %&gt;%\n    ggplot() + theme_minimal() +\n        aes(x = balance, y = default) + \n        geom_jitter(width = 0, height = 0.05) +\n        geom_smooth(method = \"glm\", se = FALSE,\n            method.args = list(family = \"binomial\")) +\n        labs(x = \"Credit Card Balance\",\n            y = \"Default?\")\n\n\n\n\n\\(\\eta(x_i) = -10.65 + 0.0054\\cdot\\text{balance}_i\\)\n\n\nLogistic Regression\n\nThe response is 0 or 1 (no or yes, dont’ default or default, etc.)\nThe probability of a 1 increases according to the sigmoid function.\n\nThe linear predictor is \\(\\eta(x_i) = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\cdots\\)\nThe probability of class 1 is \\(P(\\text{class }1 | \\text{predictors}) = \\sigma(\\eta(x_i))\\)\n\nInstead of normality assumptions, we use a binomial distribution.\n\nIt’s just one step away from a linear model!\n\n\nInterpreting Parameters\n\nGeneral structure: “For each one unit increase in \\(x_i\\), some function of \\(y_i\\) changes by some function of \\(\\beta\\)”“.\nFor logistic regression:\n\nFor each one unit increase in \\(x_i\\), \\(\\log\\left(\\frac{p(x_i)}{1-p(x_i)}\\right)\\) increases by \\(\\beta\\).\n\nThe odds are \\(\\frac{p(x_i)}{1-p(x_i)}\\).\n\n“1 in 5 people with offs of 1/4 will default on their loan.”\n\n\\(\\beta\\) represents the change in log odds for a one unit increase.\n\n“log odds ratio”.\n\n\n\n\nEstimating Parameters: Maximum Likelihood\nFor all observations:\n\nIf \\(y_i = 0\\), we want \\(p(x_i)\\) to be as low as possible.\n\nMaximize \\(1 - P(Y_{i'} = 1|\\beta_0,\\beta_1,X)\\)\n\nIf \\(y_i = 1\\), we want \\(p(x_i)\\) to be as high as possible.\n\nMaximize \\(P(Y_{i'} = 1|\\beta_0,\\beta_1,X)\\)\n\n\nThese can be combined as: \\[\n\\ell(\\beta_0,\\beta_1) = \\prod_{i':y_{i'} = 0}(1 - P(Y_{i'} = 1|\\beta_0,\\beta_1,X))\\prod_{i:y_i=1}P(Y_i = 1|\\beta_0,\\beta_1,X)\n\\] Which is NOT just the sum of squared errors!\nUnlike linear regression, there’s no closed form for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) \\(\\Rightarrow\\) need numerical methods.\n\n\nExamples: Two different predictors in the Default data\n\n\n\n\n\n\n\n\\[\n\\eta(x_i) = -3.5 + 0.5\\cdot\\text{student}\n\\]\nThe odds of a student defaulting are \\(\\exp(0.5)\\approx1.65\\) times as high as a non-student.\n\n\n\n\n\n\n\\[\n\\eta(x_i) = -10.65 + 0.005\\cdot\\text{balance}\n\\]\nEach extra dollar of credit card balance increases the odds of defaulting by a factor of 1.005.\n\n\nThe scale of the predictors matters.\n\n\nOdds versus Probabilities\n“The odds of a student defaulting are \\(\\exp(0.5)\\approx1.65\\) times as high as a non-student.”\n\\[\n\\frac{P(\\text{defaulting} | \\text{student} = 1)}{1 - P(\\text{defaulting} | \\text{student} = 1)} \\biggm/ \\frac{P(\\text{defaulting} | \\text{student} = 0)}{1 - P(\\text{defaulting} | \\text{student} = 0)} = 1.65\n\\] This cannot be solved for \\(P(\\text{defaulting} | \\text{student} = 1)\\)!\n\\[\nP(\\text{defaulting} | \\text{student} = 1) = \\dfrac{\\exp(\\eta(x_i))}{1 + \\exp(\\eta(x_i))} = \\dfrac{\\exp(-3.5 + 0.5\\cdot 1)}{1 + \\exp(-3.5 + 0.5\\cdot 1)} \\approx 0.047\n\\]\n\n\nMultiple Linear Logistic Regression\n\nPredictors can be multicollinear, confounded, and have interactions.\n\nLogistic is just Linear on a transformed scale!\n\nWe do not look for transformations of the response.\n\nIt’s already a transformation of the response \\(p_i(x_i)\\)!\n\nWe do look for transformations of the predictors!\n\nSigmoid + Polynomial is where the real fun is.\n\n\n\n\nErrors in Logistic Regression: Deviance\n\nAll “errors” are either \\(p(x_i)\\) or \\(1 - p(x_i)\\).\n\nDistance from either 0 or 1.\n\n\nInstead, we use the deviance.\n\nIf \\(p(x_i)\\) were the true probability in a binomial distribution, what’s the probability of the observed value (0 or 1)?\n\nThis is used more broadly in Generalized Linear Models (GLMs). Logistic Regression is one of many GLMs.\n\n\n\n\nLogistic Decision Boundaries\n\\[\nP(\\text{defaulting} | \\eta(x_i)) &gt; p \\implies a + bx_1 + cx_2 + dx_3 &gt; e\n\\]\nFor some (linear) hyperplane \\(a + bx_1 + cx_2 + dx_3\\) and some value \\(e\\).\n\nChoosing \\(p=0.5\\) is logical, but other thresholds can be chosen.\n\nCancer example: want to be more admissive of false positives\n\nWould rather operate and be wrong than falsely tell the patient that they’re healthy!\n\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(ISLR2)\n\nDefault$default &lt;- as.numeric(factor(Default$default)) - 1\nDefault$student &lt;- as.numeric(factor(Default$student)) - 1\n\ndecision_grid &lt;- expand.grid(\n    student = c(0,1),\n    balance = seq(0, 2655, length.out = 250),\n    income= seq(770, 73555, length.out = 250)\n)\n\nmy_glm &lt;- glm(default ~ student + balance + income,\n    data = Default, family = binomial)\ndecision_grid$pred &lt;- predict(my_glm, newdata = decision_grid)\n\nstudent_labels &lt;- c(\"Not Student\", \"Student\")\nnames(student_labels) &lt;- c(0, 1)\n\nggplot() + theme_minimal() +\n    geom_tile(data = decision_grid,\n        mapping = aes(x = balance, y = income, fill = factor(pred &gt; 0.5))) +\n    scale_fill_manual(values = c(\"firebrick\", \"green\", \"firebrick\", \"green\")) +\n    geom_point(data = Default, \n        mapping = aes(x = balance, y = income, \n            fill = factor(default == 1)),\n        shape = 21) +\n    facet_wrap(~ student, \n        labeller = labeller(student = student_labels)) +\n    labs(x = \"Credit Card Balance\",\n        y = \"Income\",\n        fill = \"Default?\")\n\n\n\n\n\n\nPredictions - Just Plug it In!\n\n\n\n\nIntercept\nStudent\nBalance\nIncome\n\n\n\n\n\\(\\beta\\)\n-10.09\n-0.65\n0.0057\n0.000003\n\n\n\nWe can make a prediction for a student with $2,000 balance and $20,000 income: \\[\\begin{align*}\n\\eta(x) &= \\beta_0 + \\beta_1\\cdot 1 + \\beta_2\\cdot 2000 + \\beta_3\\cdot 20000 \\approx 0.0178\\\\\n&\\\\\nP(\\text{defaulting} | x) &= \\dfrac{\\exp(\\eta(x))}{1 + \\exp(\\eta(x))} \\approx \\dfrac{\\exp(0.0178)}{1 + \\exp(0.0178)} \\approx 0.504\\\\\n&\\\\\n&P(\\text{defaulting} | x) &gt; 0.5 \\implies \\text{Predict Default}\n\\end{align*}\\]"
  },
  {
    "objectID": "L22-Logistic.html#classification-basics",
    "href": "L22-Logistic.html#classification-basics",
    "title": "21  L22: Classification",
    "section": "21.2 Classification Basics",
    "text": "21.2 Classification Basics\n\nGoal: Predict a Category\n\nBinary: Yes/no, success/failure, etc.\nCategorical: 2 or more categories.\n\nA.k.a. qualitative, but that’s a social science word.\n\n\nIn both: predict whether an observation is in category \\(j\\) given its predictors. \\[\nP(Y_i = j| x = x_i) \\stackrel{def}{=} p_j(x_i)\n\\]\n\n\nClassification Confusion\nConfusion Matrix: A tabular summary of classification errors.\n\n\n\n\n\n\nTrue Pay (\\(\\cdot 0\\))\nTrue Def (\\(\\cdot 1\\))\n\n\n\n\nPred Pay (\\(0 \\cdot\\))\nGood (00)\nBad (01)\n\n\nPred Def (\\(1 \\cdot\\))\nBad (10)\nGood (11)\n\n\n\n\n\nTwo ways to be wrong\nTwo ways to be right\nDifferent applications have different needs\n\n\n\nAccuracy: \\(\\dfrac{\\text{Correct Predictions}}{\\text{Number of Predictions}} =\\frac{00 + 11}{00 + 01 + 10 + 11}\\)\n\n\nIs “Accuracy” Good?\nTask: Predict whether a person has cancer\n(In this made up example, 0.02% of people have cancer).\n\n\n\n\nTrue Healthy\nTrue Cancer\n\n\n\n\nPred. Healthy\nSave a Life\nLose a Life\n\n\nPred. Cancer\nExpensive/Invasive\nAll good\n\n\n\n\n\n\nEasy: 99.8% accuracy.\n\nAlways guess “Not Cancer”\n\n\n\n\nVery Hard: 99.82% accuracy.\n\n\n\n\n\nThe Confusion Matrix for Default Data\n\n\n\n\nTrue Payment\nTrue Default\n\n\n\n\nPred Payment\n9627\n228\n\n\nPred Default\n40\n105\n\n\n\n\nThis model: 97.32% accuracy.\n\nNaive model: always predict “Pay” - 96.67% accuracy!\n\n\nOther important measures (not on exam):\n\nSensitivity: \\(\\dfrac{\\text{True Positives}}{\\text{All Positives in Data}} = \\dfrac{9627}{9627 + 40} = 99.58%\\) (Naive: 100%)\nSpecificity: \\(\\dfrac{\\text{True Negatives}}{\\text{All Negatives in Data}} = \\dfrac{105}{105 + 228} = 31.53\\) (Naive: 0%)\n\n\n\nLogistic Regression in R\nSee Course Notes\n\nModel building works very similarly, but it’s very difficult to interpret the residual plots.\n\nlibrary(palmerpenguins)\npeng &lt;- penguins[complete.cases(penguins), ]\n\nlog_cont &lt;- glm(sex ~ bill_length_mm + \n        bill_depth_mm + flipper_length_mm,\n    data = peng, family = \"binomial\")\n\nanova(log_cont, test = \"Chisq\") # Sequential\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: sex\n\nTerms added sequentially (first to last)\n\n                  Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                                332     461.61              \nbill_length_mm     1   41.185       331     420.42 1.385e-10 ***\nbill_depth_mm      1   96.786       330     323.64 &lt; 2.2e-16 ***\nflipper_length_mm  1   72.666       329     250.97 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlog_spec &lt;- update(log_cont, ~ . + species * flipper_length_mm)\n\nanova(log_cont, log_spec, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm\nModel 2: sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + \n    flipper_length_mm:species\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       329     250.97                          \n2       325     174.71  4   76.265 1.076e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nfull_spec &lt;- glm(sex ~ species*(bill_length_mm + \n        bill_depth_mm + flipper_length_mm),\n    data = peng, family = \"binomial\")\n\nanova(log_spec, full_spec, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + \n    flipper_length_mm:species\nModel 2: sex ~ species * (bill_length_mm + bill_depth_mm + flipper_length_mm)\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       325     174.71                     \n2       321     170.32  4   4.3894   0.3559\n\nanova(log_spec, update(log_spec, ~ . - species:flipper_length_mm), test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + \n    flipper_length_mm:species\nModel 2: sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       325     174.71                     \n2       327     178.78 -2  -4.0697   0.1307\n\nsummary(log_spec)\n\n\nCall:\nglm(formula = sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm + \n    species + flipper_length_mm:species, family = \"binomial\", \n    data = peng)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.2365  -0.3125   0.0033   0.3252   2.5852  \n\nCoefficients:\n                                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        -69.63032   10.83543  -6.426 1.31e-10 ***\nbill_length_mm                       0.65617    0.10521   6.237 4.47e-10 ***\nbill_depth_mm                        1.96180    0.29827   6.577 4.79e-11 ***\nflipper_length_mm                    0.04329    0.04375   0.990   0.3224    \nspeciesChinstrap                   -38.57778   22.93391  -1.682   0.0925 .  \nspeciesGentoo                      -34.58899   20.48998  -1.688   0.0914 .  \nflipper_length_mm:speciesChinstrap   0.15957    0.11696   1.364   0.1725    \nflipper_length_mm:speciesGentoo      0.15988    0.09666   1.654   0.0981 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 461.61  on 332  degrees of freedom\nResidual deviance: 174.71  on 325  degrees of freedom\nAIC: 190.71\n\nNumber of Fisher Scoring iterations: 7\n\ncoef(log_spec)\n\n                       (Intercept)                     bill_length_mm \n                      -69.63031836                         0.65616618 \n                     bill_depth_mm                  flipper_length_mm \n                        1.96180392                         0.04329349 \n                  speciesChinstrap                      speciesGentoo \n                      -38.57777987                       -34.58899421 \nflipper_length_mm:speciesChinstrap    flipper_length_mm:speciesGentoo \n                        0.15956974                         0.15987824 \n\n\nThe residual plots are the same as before:\n\npar(mfrow = c(2, 2))\nplot(log_spec)\n\n\n\n\nThe predictions can either be on the logit scale (type = \"link\", the default) or on the response scale (probabilities).\n\npredict(log_spec, type = \"response\") |&gt; head()\n\n        1         2         3         4         5         6 \n0.6335863 0.1789059 0.6382736 0.6613775 0.9918045 0.2059974 \n\n\nRegularization is often used with logistic regression (in python’s scikit-learn package, Ridge regularization is used by default without warning the user).\n\nlibrary(glmnet)\n\nX &lt;- model.matrix(sex ~ species*(bill_length_mm + \n        bill_depth_mm + flipper_length_mm),\n    data = peng)\ny &lt;- as.factor(peng$sex)\n\nmycv &lt;- cv.glmnet(X, y, family = binomial)\n\nmylasso &lt;- glmnet(X, y,\n    data = peng, family = \"binomial\", alpha = 1,\n    lambda = mycv$lambda.1se)\ncoef(mylasso)\n\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n                                             s0\n(Intercept)                        -46.90824021\n(Intercept)                          .         \nspeciesChinstrap                    -3.49235983\nspeciesGentoo                        .         \nbill_length_mm                       0.32778935\nbill_depth_mm                        1.26452716\nflipper_length_mm                    0.05765817\nspeciesChinstrap:bill_length_mm      .         \nspeciesGentoo:bill_length_mm         .         \nspeciesChinstrap:bill_depth_mm       .         \nspeciesGentoo:bill_depth_mm          .         \nspeciesChinstrap:flipper_length_mm   .         \nspeciesGentoo:flipper_length_mm      ."
  },
  {
    "objectID": "L22-Logistic.html#multinomial-regression",
    "href": "L22-Logistic.html#multinomial-regression",
    "title": "21  L22: Classification",
    "section": "21.3 Multinomial Regression",
    "text": "21.3 Multinomial Regression\n\nMultinomial Logistic Regression: K Classes\nWe have a total probability of 1 to distribute across the classes,\n\nStick breaking\n\nFit a logistic regression of class 1 versus not class 1.\n\nRemove obs. with class 1\n\nFit a logistic regression of class 2 versus not class 1.\n\nRemove obs. with class 2\n\n…\nClass \\(K\\) gets whatever probability is left over.\n\nSoftmaxing\n\nFor all classes, fit a logistic regression of class k versus not class k.\nIn the end, divide by the total probability to make sure they sum to 1.\n\n\nVery often used in machine learning!\n\n\nThese two give the same results!"
  },
  {
    "objectID": "L23-Review.html#summaries",
    "href": "L23-Review.html#summaries",
    "title": "22  Final Exam Review",
    "section": "22.1 Summaries",
    "text": "22.1 Summaries\n\nGenerally important things\n\nThe bias and variance of \\(\\hat{\\underline\\beta}\\).\nInterpreting coefficients and inference.\nVariance, rather than point estimates.\nInterpreting residual plots.\nChoosing a reasonable model given the context of the problem.\n\n\n\n“Extra Topics” Lecture\n\nStandardizing\n\nEffect on parameter estimates\n\nGeneral Linear Hypotheses\n\nFull versus hypothesized model, and the form of the F test\n\n(Not the math, but possibly the degrees of freedom)\n\n\nWeighted/Generalized Least Squares\n\nUnderstand what \\(V\\) and \\(P\\) represent.\nManipulate formulas by replacing \\(\\underline f\\) with \\(P^{-1}\\underline \\epsilon\\).\nSuggest \\(V\\) based on the context of the question.\n\n\n\n\nGetting the Wrong Model\n\nBias due to missing predictors.\nWhat does it even mean to have the “right” model?\n\nProxy measures and their effect on the other parameter estimates\n\n\n\n\nTransforming the Predictors\n\nPolynomial regression\n\nWhen to use them\nLower order terms\nExtrapolation\n\nOther transformations (e.g. log, combining predictors, etc.)\n\n\n\nTransforming the Response\n\nExplain “Stabilizing the variance”\nDiagnosing the need for a transformation\nChoosing transformations\nThe effect on the model\n\nE.g. multiplicative errors, changes to the parameter estimates\n\n\n\n\nDummy Variables\n\nDefinition and interpretation\n\nfactor(cyl)8 in the coefficients table\n\nIf there are three categories, we need two dummies\n\n“Reference” category is absorbed into the intercept.\n\nInteraction terms: different intercept, different slope.\nSignificance of a dummy variable (or interaction with one)\nExtra sum-of-squares to test whether categories are statistically different\n\nWhat kind of test is this? ANOVA? ANCOVA?\n\n\n\n\nMulticollinearity\n\nWhy it increases variance (many different parameter combinations are equivalent)\nDetecting via the variance inflation factor\n\nApprox 10 is bad, but this is just a rule-of-thumb.\n\nWhat to do about it, and what it means for interpreting coefficients.\n\n\n\nBest Subset Selection\n\nGoal: Inference or prediction?\n\nHow does Subset Selection fit into this?\n\nGeneral idea of the algorithms.\n\nDon’t need to know Mallow’s Cp etc.\n\nWhy the p-values can’t really be trusted.\nUseful as a preliminary step (sometimes).\n\n\n\nDegrees of Freedom\n\nGeneral modelling strategies.\nChoosing transformations based on domain knowledge.\nBeing explicit about the decisions made while modelling.\n\n\n\nRegularization\n\nIt’s just linear regression with “smaller” slope estimates!\n\nIntercept isn’t constrained.\nSome slopes can be bigger than the slopes for linear regression, but sum of abs/squares is smaller.\n\nRegularization prevents overfitting.\n\nChoose the penalty parameter \\(\\lambda\\) by minimizing out-of-sample prediction error, measured via cross-validation.\nWithin 1 SE of minimum MSE leads to a simpler (more regularized) model.\n\nAdds bias, which is a good thing?\nRequires standardization of the predictors.\nLASSO sets parameters to 0, Ridge does not.\n\n\n\nClassification\n\nResponse values are 0 or 1\n\nExpected value of \\(y\\) is the proportion of 1s given a value of \\(x\\).\nPredictions can be converted to 0s and 1s, with two types of errors (confusion matrix).\n\nLogistic function looks like an “S”\n\nExact shape determined by linear predictor \\(\\eta(X) = X\\underline\\beta\\)\n\nOther than transformations of response, all lm topics apply.\n\nIncluding regularization!\n\n“Residuals” are weird\n\nNot “observed minus expected” anymore!"
  },
  {
    "objectID": "L23-Review.html#midterm-solutions",
    "href": "L23-Review.html#midterm-solutions",
    "title": "22  Final Exam Review",
    "section": "22.2 Midterm Solutions",
    "text": "22.2 Midterm Solutions\n\nANOVA\nExplain how a hypotheses test based on the ratio \\(MS_{Reg}/MS_E\\) in the ANOVA table is a test for whether any of the slope parameters are 0.\n\nA line with all slopes equal to 0 is a horizontal line, which will have 0 variance around \\(\\bar y\\), i.e. \\(MS_{Reg} = 0\\).\nDue to random chance, we will never actually get \\(MS_{Reg} = 0\\). Dividing by MSE gives us a way to evaluate the size of \\(MS_{Reg}\\).\n\n\n\nBias/Variance\nFor this question, assume that \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\).\n\nGiven the model \\(y_i = \\beta_0 + \\epsilon_i\\), show that \\(\\hat\\beta_0 = \\bar y\\) minimizes the sum of squared error.\n\n\\[\\begin{align*}\n\\frac{1}{n}\\sum(y_i - \\hat y)^2 &= \\frac{1}{n}\\sum(y_i - \\beta_0)^2\\\\\n\\frac{d}{d\\beta}\\frac{1}{n} &= \\frac{1}{n}\\sum(y_i - \\beta_0)^2\\\\\n&= \\frac{2}{n}(\\sum y_i - n\\beta_0) \\stackrel{set}{=}0\\\\\n\\implies \\frac{2}{n}\\sum y_i &= \\frac{2}{n}n\\beta_0 \\implies \\bar y = \\beta_0\n\\end{align*}\\]\nThis is a minimum since this is a polynomial in \\(\\beta_0\\) with a positive coefficient for \\(\\beta_0\\).\n\n\nBias/Variance\nFor this question, assume that \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\).\n\nGiven the model \\(y_i = \\beta_0 + \\epsilon_i\\), show that \\(E(\\hat\\beta_0) = \\beta_0\\) and \\(V(\\hat\\beta_0) = \\sigma^2/n\\).\n\n\\[\\begin{align*}\nE(\\hat\\beta_0) &= E\\left(\\frac{1}{n}\\sum y_i\\right)= \\frac{1}{n}\\sum E(y_i)\\\\\n&= \\frac{1}{n}\\sum E(\\beta_0 + \\epsilon_i)= \\frac{1}{n}n\\beta_0 = \\beta_0\n\\end{align*}\\]\n\\[\\begin{align*}\nV(\\hat\\beta_0) &= V\\left(\\frac{1}{n}\\sum y_i\\right)= \\frac{1}{n^2}\\sum V(y_i)\\\\\n&= \\frac{1}{n^2}\\sum V(\\beta_0 + \\epsilon_i)= \\frac{1}{n^2}n\\sigma^2 = \\frac{\\sigma^2}{n}\n\\end{align*}\\]\n\n\nBias/Variance\nFor this question, assume that \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\).\n\nNow consider the multiple linear regression model \\(Y = X\\beta + \\underline\\epsilon\\), where we know that \\(\\hat{\\underline{\\beta}} = (X^TX)^{-1}X^TY\\). Show that \\(E(\\hat{\\underline{\\beta}}) = \\underline{\\beta}\\).\n\n\\[\nE(\\hat{\\underline{\\beta}}) = E((X^TX)^{-1}X^TY) = (X^TX)^{-1}X^TE(Y) = (X^TX)^{-1}X^TX\\underline\\beta = \\underline\\beta\n\\]"
  },
  {
    "objectID": "L23-Review.html#participation-questions",
    "href": "L23-Review.html#participation-questions",
    "title": "22  Final Exam Review",
    "section": "22.3 Participation Questions",
    "text": "22.3 Participation Questions\n\nQ1\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2\\sin(\\beta_3 x_{i2} + \\beta_4) + \\epsilon_i\n\\]\nWhich of the following could we minimize to find the estimates of \\(\\hat{\\underline\\beta}\\)?\n\n\\(\\sum(y_i - \\beta_0 - \\beta_1x_{i1} - \\beta_2\\sin(\\beta_3 x_{i2} - \\beta_4))^2\\)\n\\(\\sum(y_i - \\beta_0 - \\beta_1x_{i1} - \\beta_2\\sin(\\beta_3 x_{i2} - \\beta_4 - \\epsilon_i))^2\\)\n\\(\\sum(\\hat y_i - \\beta_0 - \\beta_1x_{i1} - \\beta_2\\sin(\\beta_3 x_{i2} - \\beta_4))^2\\)\n\\(\\sum(\\hat y_i - \\beta_0 - \\beta_1x_{i1} - \\beta_2\\sin(\\beta_3 x_{i2} - \\beta_4 - \\hat\\epsilon_i))^2\\)\nNone of the above - this is not a linear model and cannot be minimized.\n\n\n\nQ2\n\\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2\\sin(\\beta_3 x_{i2} + \\beta_4) + \\epsilon_i\n\\]\nWhat is \\(E(y_i)\\)?\n\n\\(\\beta_0 + \\beta_1x_{i1} + \\beta_2\\sin(\\beta_3 x_{i2} + \\beta_4)\\)\n\\(\\beta_0 + \\beta_1x_{i1} + \\beta_2\\sin(\\beta_3 x_{i2} + \\beta_4) + \\epsilon\\)\n\\(\\hat\\beta_0 + \\hat\\beta_1x_{i1} + \\hat\\beta_2\\sin(\\hat\\beta_3 x_{i2} + \\hat\\beta_4)\\)\n\\(\\hat\\beta_0 + \\hat\\beta_1x_{i1} + \\hat\\beta_2\\sin(\\hat\\beta_3 x_{i2} + \\hat\\beta_4) + \\epsilon\\)\n\n\n\nQ3\nWhich statement is false?\n\nLinear models are never the correct model, but are often useful.\nWe expect that our parameter estimates will correspond to a specific relationship in the population.\nAssuming we have a representative sample, we expect that the relationships in our data are representative of the relationships in the population.\nThere are situations in which a linear model is not appropriate.\n\n\n\nQ4\nWhich assumption does not always need to be confirmed before attempting to make inferences or predictions?\n\n\\(V(\\epsilon_i) = \\sigma^2\\) (stable variance).\nApparent linear relationship.\nNo serial correlation in residuals.\nIndependence between observations"
  }
]