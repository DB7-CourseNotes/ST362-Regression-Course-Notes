[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Analysis",
    "section": "",
    "text": "Introduction\n\nAbout This Course\nThis book contains the course notes for the Spring 2023 offering of ST362 Regression Analysis, based on the following sources:\n\nApplied Regression Analysis, 3rd edition, by Draper and Smith\n\nA PDF of this textbook is available through the WLU library\n\nIntroduction to Linear Regression Anaysis, 2nd edition, by Montgomery and Peck\n\nThis textbook is excellent but expensive, and I am striving to use free and OER materials.\n\nThe online course notes from Stat 501 at Penn State.\n\n\n\nAbout This Book\nSome features:\n\nThe GitHub logo takes you to the repo for this book. Feel free to fork and adapt as you please (under the CC BY-SA 4.0 license).\nThe little toggle next to the logo puts this into night mode. Try it out!\nEach lecture had a “Jam”, where I played music at the start of class that related to a particular slide. When that slide showed up, a student would say “That’s my Jam!” and I would throw chocolate at them.\n\nThe jams are still there, and you may wish to listen to them while reading the slides!\n\n\nThis book is very much a work in progress. There are missing sections and typos in the slides. I am working on adding speaker notes to the slides, which will show up as text in this book.\nI am also working on a major re-write of the first few chapters to walk through the concepts in a better order. In particular, I would like to stay in simple linear regression for a lot longer, demonstrating correlation, Cook’s distance, correlation between \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\), etc., then moving into binary and categorical predictors as a first step into multiple regression, polynomial as a second step, then a lecture demonstrating that all of these concepts generalize into multiple dimensions.\nThis is a quarto book based on my lecture slides. The “Lectures” are quarto files that were rendered into beamer PDF slides. I have included the configs to render the slides. To re-create my slides, you can use the code:\nquarto render L01-Introduction.qmd --profile slides\nThe --profile argument tells Quarto to use the configuration in the file _quarto-slides.yml. I will add speaker notes in a notes environment, which means that the notes will appear in the book but not the pdf slides.\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 Unported License."
  },
  {
    "objectID": "L01-Introduction.html#introduction",
    "href": "L01-Introduction.html#introduction",
    "title": "1  L01: Distributions",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\n\nToday’s Learning Outcomes\n\nImportant facts about distributions.\nCIs and t-tests."
  },
  {
    "objectID": "L01-Introduction.html#distributions",
    "href": "L01-Introduction.html#distributions",
    "title": "1  L01: Distributions",
    "section": "1.2 Distributions",
    "text": "1.2 Distributions\n\nNormal\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{(x-\\mu)^2}{-2\\sigma^2}\\right);\\;-\\infty&lt;x&lt;\\infty\n\\]\nShiny: https://shiny.abdn.ac.uk/Stats/apps/app_normal/#section-theory\n\nCompletely determined by \\(\\mu\\) and \\(\\sigma\\).\nIf \\(X \\sim N(\\mu,\\sigma^2)\\), then \\(Z=\\frac{X-\\mu}{\\sigma} \\sim N(0,1)\\).\n\nOften called “standardizing”\n\nYou won’t need to memorize the pdf, but it’s useful!\n\n\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pnorm\")\n\n\n\n\n\\(t\\)\n\n\nBut first, Gamma!\n\n\n\\(t\\) is based on the Gamma (\\(\\Gamma\\)) function:\n\\[\n\\Gamma(q) = \\int_0^\\infty e^{-t}t^{q-1}dt\n\\]\n\nInteresting property: \\(\\Gamma(k+1) = k!\\) for integer \\(k\\).\n\nIn general, \\(\\Gamma(q) = (q-1)\\Gamma(q-1)\\)\n\nAlso, \\(\\Gamma(1/2) = \\pi^{1/2}\\)\n\n\\(\\Gamma(3/2) = (3/2-1)\\gamma(1/2) = \\sqrt{\\pi}/2\\)\n\n\n\n\n\n\n\n\nThe \\(t\\) Distribution\n\\[\nf(x; \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu + 1}{2}\\right)}{\\sqrt{\\nu\\pi}\\Gamma(\\nu/2)}\\left(1 + \\frac{x^2}{\\nu}\\right)^{-(\\nu + 1)/2}\n\\]\n\nThe notation \\(f(x; \\nu)\\) means “function of \\(x\\), given a value of \\(\\nu\\).”\nCompletely determined by \\(\\nu\\)\nAs \\(\\nu\\rightarrow\\infty\\), this becomes \\(N(0,1)\\).\n\n\\(\\nu&gt;30\\) is pretty much normal already.\nFor \\(\\nu&lt;\\infty\\), \\(t\\) has wider tails than normal.\n\n\n\n\nThe \\(\\chi^2\\) distribution - variances\nIf \\(Z_1, Z_2, ..., Z_k\\) are iid \\(N(0,1)\\), then \\(\\chi^2_k = \\sum_{i=1}^kZ_i^2\\) has a chi-square distribution on \\(k\\) degrees of freedom.\n\\[\nf(x; k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2}\n\\]\n\nIf \\(X_i\\sim N(\\mu_i\\sigma_i)\\), then we can just standardize each first.\nAs \\(k\\rightarrow\\infty\\), \\((\\chi^2_k-k)/\\sqrt{2k}\\stackrel{d}{\\rightarrow} N(0,1)\\).\n\nThat is, for large \\(k\\) this can be approximated by a normal distribution.\n\nVery related to the variance\n\n\\((n-1)s^2/\\sigma^2\\) follows a \\(\\chi^2_n\\) distribution.\n\n\n\n\nThe \\(F\\) distribution - ratio of variances\nIf \\(S_1\\) and \\(S_2\\) are independent \\(\\chi^2\\) distributions with degrees of freedom \\(\\nu_1\\) and \\(\\nu_2\\), then \\(\\frac{S_1/\\nu_1}{S_2/\\nu_2}\\) follows an \\(F_{\\nu_1,\\nu_2}\\) distribution.\n\\[\nf(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}\n\\]\n\nIf \\(s_1^2\\) and \\(s_2^2\\) are the sample-based estimates of the true values \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\), then \\(\\frac{s_1^2/\\sigma_1^2}{s_2^2/\\sigma_2^2}\\) follows an \\(F_{\\nu_1, \\nu_2}\\) distribution.\n\nUnder \\(H_0\\), \\(\\sigma_1=\\sigma_2\\), so we don’t need the true values.\nNote: \\(s_1^2\\) is calculated from \\(S_1^2/\\nu_1\\)."
  },
  {
    "objectID": "L01-Introduction.html#confidence-intervals",
    "href": "L01-Introduction.html#confidence-intervals",
    "title": "1  L01: Distributions",
    "section": "1.3 Confidence Intervals",
    "text": "1.3 Confidence Intervals\n\nGeneral Idea: Terminology\nConsider the statistic \\(t = \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)}\\) (a statistic is any number that can be calculated from data alone).\n\n\\(\\theta\\) is the “““true”“” value of the parameter.\n\nUnknown, unknowable, not used in formula - hypothesize \\(\\theta = \\theta_0\\) instead.\n\n\\(\\hat\\theta\\) is our estimator of \\(\\theta\\).\n\nEstimator is a function, like \\(\\hat\\mu = \\bar X =(1/n)\\sum_{i=1}^nX_i\\).\n\n\\(X\\) is a random variable.\n\nEstimate is a number, like the calculated mean of a sample.\n\n\\(se(\\hat\\theta)\\) is the standard error of \\(\\hat\\theta\\).\n\nIf we had a different sample, the value of \\(\\hat\\theta\\) would be different. It has variance.\nStandard error: the standard deviation of the sampling distribution.\n\n\n\n\nGeneral Idea: Distributional Assumptions\nConsider the quantity (not statistic) \\(t = \\frac{\\hat\\theta - \\theta}{se(\\hat\\theta)}\\).\nIf we assume \\(X\\sim N(\\mu, \\sigma^2)\\) and \\(\\hat\\theta = \\bar X\\) is an unbiased estimate of \\(\\theta\\) on \\(\\nu=n-1\\) degrees of freedom, then \\(t \\sim t_\\nu\\)\nFrom this, we can find the lower and upper \\(\\alpha/2\\)% of the \\(t\\)$ curve.\n\n\\(t_\\nu(\\alpha/2)\\) is the lower \\(\\alpha/2\\)%.\n\ni.e., if \\(\\alpha = 0.11\\), then it’s \\(t(\\nu, 0.055)\\), the lower 5.5% area.\n\n\\(t_\\nu(1 - \\alpha/2)\\) is the upper \\(\\alpha/2\\)%.\nSince \\(t\\) is symmetric, \\(t_\\nu(1 - \\alpha/2) = -t_\\nu(\\alpha/2)\\).\n\n\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pvalues\")\n\n\n\n\nGeneral Idea: Distribution to Quantiles\nUnder the null hypothesis, \\(\\theta = \\theta_0\\), so \\[\nt = \\frac{\\hat\\theta - \\theta}{se(\\hat\\theta)} = \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)} \\sim t_\\nu\n\\]\nWe know that \\(\\bar X\\) is a random variable, and we want everything in between its 5.5% and 94.5% quantiles. \n\nThis is a confidence interval!\n\n\n\nGeneral Idea: Distribution to CI\nWe can do this easily for the \\(t_\\nu\\) distribution: we want all values \\(t_0\\) such that\n\\[\\begin{align*}\nt_\\nu(5.5) &\\le t_0 \\le t_\\nu(94.5)\\\\\nt_\\nu(5.5) &\\le \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)} \\le t_\\nu(94.5)\\\\\nse(\\hat\\theta)t_\\nu(5.5) &\\le \\hat\\theta - \\theta_0 \\le se(\\hat\\theta)t_\\nu(94.5)\\\\\n\\hat\\theta + se(\\hat\\theta)t_\\nu(5.5) &\\le \\theta_0 \\le \\hat\\theta + se(\\hat\\theta)t_\\nu(94.5)\n\\end{align*}\\]\nThe CI is all values \\(\\theta_0\\) that would not be rejected by the null hypothesis \\(\\theta = \\theta_0\\) at the \\(\\alpha\\)% level.\nSince \\(t_\\nu(1 - \\alpha/2) = -t_\\nu(\\alpha/2)\\), our 89% CI is \\(\\hat\\theta \\pm se(\\hat\\theta)t_\\nu(5.5)\\).\n\n\nWhat is the Standard Error?\nIf we’re estimating the mean, \\(\\hat\\theta = (1/n)\\sum_{i=1}^nX_i\\), where we assume \\(X_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma)\\). \\[\nE(\\hat\\theta) = E\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n}\\sum_{i=1}^nE(X_i) = \\frac{n\\mu}{n} = \\mu\n\\] From this, we see that the mean is an unbiased estimator! (This is nice, but not required.)\n\\[\nV(\\hat\\theta) = V\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n^2}V\\left(\\sum_{i=1}^nX_i\\right) \\stackrel{indep}{=} \\frac{1}{n^2}\\sum_{i=1}^nV(X_i) = \\sigma^2/n \\stackrel{plug-in}{=} s^2/n\n\\] where \\(s^2\\) is the estimate variance since we cannot know the true mean. Note that \\(s^2\\) is a biased estimator for \\(\\sigma\\).\n\n\nCI for Variance\nFrom before: \\(\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_n\\).\nLet \\(\\chi^2_n(0.055)\\) be the lower 5.5% quantile, \\(\\chi^2_n(0.945)\\) be the upper.\nFor homework, find the CI from: \\[\n\\chi^2_n(0.055) \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi^2_n(0.945)\n\\] Note that \\(\\chi^2_n(0.055)\\ne-\\chi^2_n(0.945)\\).\n\n\nSummary\n\nDistributions exist and are important\n\nMost things will be normal, which leads to \\(t\\), \\(\\chi^2\\), and \\(F\\).\n\nCIs are all values that would not be rejected by a hypothesis test.\n\nThe null hypothesis determines the distribution of the test statistic, which allows us to find the CI.\n\n\nFor Next Class:\n\nRead through Ch01, especially procedure for least squares estimation.\n\n&lt;—"
  },
  {
    "objectID": "L01-Introduction.html#participation-questions",
    "href": "L01-Introduction.html#participation-questions",
    "title": "1  L01: Distributions",
    "section": "1.4 Participation Questions",
    "text": "1.4 Participation Questions\n\nQ1\nWhich of the following is a Normal distribution?\n\n\\(f(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}\\)\n\\(f(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-x^2/2\\right)\\)\n\\(f(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}\\)\n\\(f(x; k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2}\\)\n\n\n\nQ2\nIf you know \\(\\mu\\) and \\(\\sigma\\), then you know the exact shape of the normal distribution.\n\nTrue\nFalse\n\n\n\nQ3\nA confidence interval for \\(\\theta\\) contains all values \\(\\theta_0\\) that would not be rejected by a hypothesis test (assume that both are at the same significance level).\n\nTrue\nFalse\n\n\n\nQ4\nWhich of the following is the correct value for \\(E(\\hat\\theta)\\), where \\(\\hat\\theta = \\sum_{i=1}^n\\left(a + bX_i\\right)\\) and \\(E(X_i)=\\mu\\) for all \\(i\\)?\n\n\n\\(a + b\\mu\\)\n\\(b\\mu\\)\n\\(na + nb\\mu\\)\n\n\n\nQ5\nWhich of the following is the definition of an estimator?\n\nA value calculated from data.\nA function that returns the estimate for a parameter.\nAny function of the data.\nA person who estimates.\n\n\n\nQ6\nThe general approach to finding confidence intervals is to find a function of the statistic and the parameter it’s estimating that follows a known distribution and then solve for the unknown parameter.\n\n\nFalse\nTrue\n\n\n—&gt;"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#why-fit-models",
    "href": "L02-Fitting_Straight_Lines.html#why-fit-models",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.1 Why Fit Models?",
    "text": "2.1 Why Fit Models?\n\nThe Quote.\n“All models are wrong, some are useful.” - George Box\n\n\nAll Models Are Wrong, But Some Are Useful\nModel: A mathematical equation that explains something about the world.\n\nGravity: 9.8 \\(m/s^2\\) right?\n\nThis is a model - it’s a relationship that explains something in the world.\nVaries across the surface of the Earth.\nVaries according to air resistance.\n\nEvery additional cigarette decreases your lifespan by 11 minutes.\n\nVery, very much wrong.\nVery, very useful for communication.\n\n\n\n\nAll Linear Models are Wrong\n\n\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(palmerpenguins)\npenguins &lt;- penguins[complete.cases(penguins),]\n\nggplot(penguins) +\n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(title = \"A line fits reasonably well\",\n        x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\")\n\n\n\n\n\n\nggplot(mtcars) +\n    aes(x = disp, y = mpg) + \n    geom_point(size = 2) + \n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) + \n    geom_smooth(method = \"lm\", se = FALSE, formula = \"y~poly(x, 2)\", colour = 2) +\n    labs(title = \"A straight line misses the pattern\",\n        subtitle = \"A polynomial might fit better\",\n        x = \"Engine Displacement (1000 cu.in)\", \n        y = \"Fuel Efficiency (mpg)\")\n\n\n\n\n\n\n\n\nSome Linear Models are Useful\n\nxpred &lt;- 200\nypred &lt;- predict(\n    lm(body_mass_g ~ flipper_length_mm, data = penguins),\n        newdata = data.frame(flipper_length_mm = xpred))\n\nggplot(penguins) +\n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(title = \"The height of the line is an okay prediction for Body Mass\",\n        x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\") +\n    annotate(geom = \"point\", shape = \"*\", size = 30, colour = 2,\n        x = xpred, \n        y = ypred) + \n    annotate(geom = \"segment\", colour = 2, size = 2, \n        linetype = \"dashed\",\n        x = xpred, xend = xpred, \n        yend = -Inf, y = ypred) + \n    annotate(geom = \"segment\", colour = 2, size = 2, \n        linetype = \"dashed\",\n        x = -Inf, xend = xpred, \n        yend = ypred, y = ypred)\n\n\n\n\n\n\nA Model is an Equation\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\Leftrightarrow Y = X\\underline\\beta + \\underline\\epsilon \\Leftrightarrow E(Y|X) = X\\underline \\beta;\\; V(Y|X)=\\sigma^2\n\\]\n\nFor a one unit increase in \\(x_i\\), \\(y_i\\) increases by \\(\\beta_1\\).\n\nIf our model fits well and this is statistically significant, we can apply this to the population.\n\nThe estimated value of \\(y_i\\) when \\(x_i=0\\) is \\(\\beta_0\\).\n\nNot always interesting, but usually\\(^*\\) necessary.\n\n\nOur prediction for \\(y_i\\) at any given value of \\(x_i\\) is calculated as: \\[\n\\hat y_i = \\hat\\beta_0 + \\hat \\beta_1x_i\n\\] where the “hats” mean we’ve found estimates for the \\(\\beta\\) values.\n\n\nAside: My notation differs from the text\nI will make mistakes, but in general:\n\n\\(Y\\) is a random variable (usually a vector) representing the response.\n\n\\(Y_i\\) is also a random variable (not a vector)\n\n\\(\\underline y\\) is the response vector; the observed values of \\(Y\\)\n\\(\\underline x\\) is the vector of covariates in simple linear regression\n\\(X\\) is a matrix of covariates\n\nNot a random variable!!! Capital letters could be either random or matrix (or both). I will specify when necessary.\nI will avoid the notation \\(X_i\\).\nWhen necessary, the first column is all ones so that \\(X\\underline\\beta=\\beta_0 + \\beta_1\\underline x_1 + \\beta_1\\underline x_2 + ...\\).\n\nContext should make this clear.\n\n\n\n\n\nThe Mean of \\(Y\\) at any value of \\(X\\)\n\\[\nE(Y|X) = X\\underline\\beta\n\\]\n\n\n\nA Linear Model is Linear in the Parameters\nThe following are linear:\n\n\\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1\\sin\\left(x_i^{x_i}\\right) + \\beta_2x_i^{\\sin(x_i)} + \\epsilon_i\\)\n\nThe following are not linear:\n\n\\(y_i = \\beta_0 + \\beta_1^2x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_0\\beta_1x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\sin(\\beta_1)x_i +\\epsilon_i\\)"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#estimation",
    "href": "L02-Fitting_Straight_Lines.html#estimation",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.2 Estimation",
    "text": "2.2 Estimation\n\nGoal: Find \\(\\beta\\) values which minimize the error\nModel: \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\)\nError: \\(\\hat\\epsilon_i = y_i - \\hat y_i\\)\nWhy is this a bad way to do it?\n\n\nLeast Squares\nGoal: Minimize \\(\\sum_{i=1}^n\\hat\\epsilon_i^2=\\sum_{i=1}^n(y_i - \\hat y_i)^2\\), the sum of squared errors.\nThis ensures errors don’t cancel out. It also penalizes large errors more.\nCould we have used \\(|\\epsilon_i|\\), or some other function? \\(|ly|\\)!\n\n\nLeast Squares Estimates\nI assume that you can do the Least Squares estimate in your sleep (be ready for tests).\nThe final results are: \\[\\begin{align*}\n\\hat\\beta_0 &= \\bar y - \\hat\\beta_1\\bar x\\\\\n\\hat\\beta_1 &= \\frac{\\sum_{i-1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i-1}^n(x_i - \\bar x)^2}\\stackrel{def}{=}\\frac{S_{XY}}{S_{XX}}\n\\end{align*}\\]\nThe textbook was written in 1998 and gives formulas to make the calculation easier to do on pocket calculators. You will not need a pocket calculator for this course.\n\n\nLet’s Add Assumptions\nAssumptions allow great things, but only when they’re correct!\n\n\\(E(\\epsilon_i) = 0\\), \\(V(\\epsilon_i) = \\sigma^2\\).\n\\(cov(\\epsilon_i, \\epsilon_j) =0\\) when \\(i\\ne j\\).\n\nImplies that \\(E(y_i) = \\beta_0 + \\beta_1x_i\\) and \\(V(y_i) = \\sigma^2\\).\n\n\\(\\epsilon_i\\stackrel{iid}{\\sim} N(0,\\sigma^2)\\)\n\nThis is a strong assumption, but often works!\n\n\nInterpretation: the model looks like a line with completely random errors. (“Completely random” doesn’t mean “without structure”!)\n\n\nMean of \\(\\hat\\beta_1\\)\nIt is easy to show that \\[\n\\hat\\beta_1 = \\frac{\\sum_{i-1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i-1}^n(x_i - \\bar x)^2} = \\frac{\\sum_{i-1}^n(x_i - \\bar x)y_i}{\\sum_{i-1}^n(x_i - \\bar x)^2}\n\\] since \\(\\sum(x_i - \\bar x) = 0\\).\nHomework: show that \\(E(\\hat\\beta_1) = \\beta_1\\) (Hint: use the fact that \\(E(y_i) = \\beta_0 + \\beta_1x_i\\) and the “pocket calculator” expansions).\n\n\nVariance of \\(\\hat\\beta_1\\)\n\\[\n\\hat\\beta_1 = \\frac{\\sum_{i-1}^n(x_i - \\bar x)y_i}{\\sum_{i-1}^n(x_i - \\bar x)^2}\n\\] can be re-written as \\[\n\\hat\\beta_1 = \\sum_{i=1}^na_iy_i\\text{, where }a_i = \\frac{x_i - \\bar x}{\\sum_{i=1}^n(x_i - \\bar x)^2}\n\\]\nThus the variance of \\(\\hat\\beta_1\\) is \\[\nV(\\hat\\beta_1) = \\sum_{i=1}^na_i^2V(y_i) = ... = \\frac{\\sigma^2}{S_{XX}} \\stackrel{plug-in}{=} = \\frac{s^2}{S_{XX}}\n\\]\n\n\nConfidence Interval and Test Statistic for \\(\\hat\\beta_1\\)\nThe test statistic can be found as: \\[\nt = \\frac{\\hat\\beta_1 - \\beta_1}{se(\\hat\\beta_1)} = \\frac{(\\hat\\beta_1 - \\beta_1)\\sqrt{S_{XX}}}{s} \\sim t_\\nu\n\\]\nSince this follows a \\(t\\) distribution, we can get the CI: \\[\n\\hat\\beta_1 \\pm t_\\nu(\\alpha/2)\\sqrt{\\frac{s^2}{S_{XX}}}\n\\]"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#participation-questions",
    "href": "L02-Fitting_Straight_Lines.html#participation-questions",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.3 Participation Questions",
    "text": "2.3 Participation Questions\n\nQ1\nAll models are useful, but some are wrong.\n\nTrue\nFalse\n\n\n\nQ2\nFor a one unit increase in \\(x\\), \\(y\\) increases by\n\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(\\beta_1x\\)\n\\(\\beta_0 + \\beta_1x\\)\n\n\n\nQ3\nThe standard error of \\(\\beta_1\\) refers to:\n\nThe variance of the population slope\nThe amount by which the slope might be off\nThe variance of the estimated slope across different samples\nA big chicken. Not, like, worryingly big, but big enough that you’d be like, “Wow, that’s a big chicken!”\n\n\n\nQ4\nWhich is not an assumption that we usually make in linear regression?\n\n\\(V(\\epsilon_i) = \\sigma^2\\)\n\\(E(\\epsilon_i) = 0\\)\n\\(E(X) = 0\\)\n\\(\\epsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\)\n\n\n\nQ5\nWhich of the following is not a linear model?\n\n\\(y_i = \\beta_0 + \\beta_1^2x_i + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_0x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1\\sin\\left(x_i^{x_i}\\right) + \\beta_2x_i^{\\sin(x_i)} + \\epsilon_i\\)\n\n\n\nQ6\nThe sum of squared errors is the best way to estimate the model parameters.\n\nTrue\nFalse"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#analysis-of-variance",
    "href": "L02-Fitting_Straight_Lines.html#analysis-of-variance",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.4 Analysis of Variance",
    "text": "2.4 Analysis of Variance\n\nStatistics is the Study of Variance\n\nGiven a data set with variable \\(\\underline y\\), \\(V(\\underline y) = \\sigma^2_y\\).\n\nThis is just the variance of a single variable.\n\nOnce we’ve incorporated the linear relationship with \\(\\underline x\\), \\(V(\\hat\\beta \\underline x + \\underline\\epsilon)=0+V(\\underline\\epsilon) = \\sigma^2\\)\n\nMathematically, \\(\\sigma^2 \\le \\sigma_y^2\\).\n\n\nThe variance in \\(Y\\) is explained by \\(X\\)!\n\n\nA Useful Identity, and its Interpretations\n\\[\\begin{align*}\n\\hat\\epsilon_i &= y_i - \\hat y_i \\\\&= y_i - \\bar y - (\\hat y_i - \\bar y)\\\\\n\\implies \\sum_{i=1}^n\\hat\\epsilon_i^2 &= \\sum_{i=1}^n(y_i-\\bar y)^2 - \\sum_{i=1}^n(\\hat y_i - \\bar y)^2\n\\end{align*}\\] where we’ve simply added and subtracted \\(\\bar y\\). The final line skips a few steps (try them yourself)!\nThe last line is often written as: \\(SS_E = SS_T - SS_{Reg}\\)\n\n\nSums of Squares\n\\[\nSS_E = SS_T - SS_{Reg}\n\\]\n\n\\(SS_E\\): Sum of Squared Errors\n\\(SS_T\\): Sum of Squares Total (i.e., without \\(\\underline x\\))\n\\(SS_{Reg}\\): Sum of Squares due to the regression.\n\nIt’s the variance of the line (calculated at observed \\(\\underline x\\) values) around the mean of \\(\\underline y\\)???\n\nThis is incredibly useful, but weird.\n\nI use \\(SS_{Reg}\\) instead of \\(SS_R\\) because some textbooks use \\(SS_R\\) as the Sume of Squared Residuals, which is confusing.\n\n\n\n\nAside: Degrees of Freedom\nDef: The number of “pieces of information” from \\(y_1, y_2, ..., y_n\\) to construct a new number.\n\nIf I have \\(x = (1,3,2,1,3,???)\\) and I know that \\(\\bar x = 2\\), I can recover the missing piece.\n\nThe mean “uses” (accounts for) one degree of freedom\n\nIf I have \\(x = (1,2,3,1,???,???)\\) and I know \\(\\bar x = 2\\) and \\(s_x^2=1\\), I can recover the two missing pieces.\n\nThe variance accounts for two degrees of freedom.\n\nOne \\(df\\) is required to compute it.\n\n\n\nEstimating one parameter takes away one degree of freedom for the rest!\n\nCan find \\(\\bar x\\) when \\(x = (1)\\), but can’t find \\(s_x^2\\) because there aren’t enough \\(df\\)!\n\n\n\nSums of Squares\n\n\n\n\n\n\n\n\n\nSource of Variation\nDegress of Freedom \\(df\\)\nSum of Squares (\\(SS\\))\nMean Square (\\(MS\\))\n\n\n\n\nRegression\n1\n\\(\\sum_{i=1}^n(\\hat y_i - \\bar y)^2\\)\n\\(MS_{Reg}\\)\n\n\nError\n\\(n-2\\)\n\\(\\sum_{i=1}^n(y_i - \\hat y_i)^2\\)\n\\(MS_E=s^2\\)\n\n\nTotal\n\\(n-1\\)\n\\(\\sum_{i=1}^n(y_i - \\bar y)^2\\)\n\\(s_y^2\\)\n\n\n\n\nNotice that \\(SS_T = SS_{Reg} + SS_E\\), which is also true for the \\(df\\) (but not \\(MS\\)). \nWhy is \\(df_E = n-2\\)? What two parameters have we estimated?\n\n\\(df_{Reg}\\) is trickier to explain. It suffices to know that \\(df_{Reg} = df_T-df_E\\).\n\n\n\n\nUsing Sums/Means of Squares\n\nIf \\(\\hat y_i = \\bar y\\) for all \\(i\\), then we have a horizontal line!\n\nThat is, there is no relationship between \\(\\underline x\\) and $\\underline \\(y\\).\nIn this case, \\(SS_{reg} = \\sum_{i=1}^n(\\hat y_i - \\bar y)^2 = 0\\).\n\n\nOkay, so, just test for \\(SS_{Reg} = 0\\)?\nBut how??? We need some measure of how far from 0 is statistically significant!!!\nRecall that \\(SS_E = \\sum_{i=1}^n(y_i - \\hat y_i)^2\\).\n\nWe can compare the variation around the line to the variation of the line.\n\nThis is \\(MS_{Reg}/MS_E\\), and it follows an \\(F\\) distribution!!\n\n\n\n\nThe F-test for Significance of Regression\n\\[\nF_{df_{Reg}, df_E} = \\dfrac{MS_{Reg}}{MS_E} = \\dfrac{MS_{Reg}}{s^2}\n\\]\n\nRecall that \\(SS_{Reg} = SS_E + SS_T &gt; SS_E\\), but the \\(df\\) make a difference.\nFor homework, show that \\(E(MS_{Reg}) = \\sigma^2 + \\beta_1^2\\sum_{i=1}^n(x_i-\\bar x)^2\\)\nThis implies that \\(E(MS_{Reg}) &gt; E(MS_E) = \\sigma^2\\).\n\n\n\nExercise A from Chapter 3 (Ch03 Exercises cover Ch1-3)\n\n\nA study was made on the effect of temperature on the yield of a chemical process. The data are shown to the right.\n\nAssuming \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\), what are the least squares estimates of \\(\\beta_0\\) and \\(\\beta_1\\)?\nConstruct the analysis of variance table and test the hypothesis \\(H_0: \\beta_1=0\\) at the 0.05 level.\nWhat are the confidence limits (at = 0.05) for \\(\\beta_1\\)?\n\n\n\nlibrary(aprean3) # Data from textbook\nhead(dse03a) # Data Set Exercise Ch03A\n\n   x  y\n1 -5  1\n2 -4  5\n3 -3  4\n4 -2  7\n5 -1 10\n6  0  8\n\nnrow(dse03a)\n\n[1] 11\n\n\n\n\n\nGood textbook questions: A, K, O, P, T, U, X (using data(anscombe) in R), Z, AA, CC. Note that all data sets in the textbook are available in an R package found here.\nPlease see Lab 2."
  },
  {
    "objectID": "L03-Residuals.html#the-residual-whats-left-over",
    "href": "L03-Residuals.html#the-residual-whats-left-over",
    "title": "3  L03: Assessing Fit",
    "section": "3.1 The residual: what’s left over",
    "text": "3.1 The residual: what’s left over\n\n\\(R^2\\): percent of variance explained by the regression model\n\n\n\\[\nR^2 = \\frac{SSReg}{SST} = \\frac{S_{XY}^2}{S_{XX}S_{YY}}\n\\]\n\n\nlayout(mat = matrix(c(1,2,3), nrow = 1), widths = c(0.5,1,1))\nset.seed(18)\nx &lt;- runif(25, 0, 10)\ny &lt;- rnorm(25, 2 + 5*x, 6)\n\nplot(rep(1, 25), y, xlab = \"y\", ylab = \"y has variance\", xaxt = \"n\")\nabline(h = mean(y))\naxis(2, mean(y), bquote(bar(y)), las = 1)\n\nplot(x, y, ylab = \"There's variance around the line\")\nabline(lm(y~x))\nabline(h = mean(y))\naxis(2, mean(y), bquote(bar(y)), las = 1)\n\nmids &lt;- predict(lm(y~x))\nfor(i in seq_along(mids)){\n    lines(x = rep(x[i], 2), y = c(y[i], mids[i]), col = 1)\n}\n\nmids &lt;- predict(lm(y~x))\nplot(mids ~ x, type = \"n\", ylab = \"The line varies around the mean of y!\")\nfor(i in seq_along(mids)){\n    lines(x = rep(x[i], 2), y = c(mean(y), mids[i]))\n}\naxis(2, at = mean(y), labels = bquote(bar(y)), las = 1)\nabline(h = mean(y))\n\n\n\n\n\n\n\n\nResidual Assumptions\n\nResidual: what’s left over\n\n\\(\\hat\\epsilon_i = y_i - \\hat y_i\\)\n\nAssumptions (from before):\n\n\\(E(\\epsilon_i) = 0\\)\n\\(V(\\epsilon_i) = \\sigma^2\\)\n\\(\\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n\nWe must check our assumptions!\n\nThere are statistical tests, but they’ll never tell you as much as a plot!\n\n\n\nResiduals versus fitted values: \\(\\hat{\\underline\\epsilon}\\) versus \\(\\hat{\\underline{y}}\\)\n\n\nWhy \\(\\hat{\\underline{y}}\\) instead of \\(\\underline y\\)?\n\nSee text. Try a regression of \\(\\hat{\\underline\\epsilon}\\) versus \\(\\underline{y}\\) yourself (mathematically and with code).\n\nWhy not \\(\\underline x\\)?\n\nFor simple linear regression, \\(\\hat{\\underline{y}}\\) is like a unit change for \\(\\underline x\\), so it doesn’t matter.\n\nFor multiple linear regression, it’s easier to have one variable for the \\(x\\) axis.\n\n\n\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(patchwork)\nlibrary(broom)\nlibrary(palmerpenguins)\n\npenguins &lt;- penguins[complete.cases(penguins),]\n\ng1 &lt;- ggplot(penguins) + \n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\",\n        title = \"y versus x\")\n\nplm &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\np2 &lt;- augment(plm)\n\ng2 &lt;- ggplot(p2) + \n    aes(x = .fitted, y = .resid) + \n    geom_point() +\n    geom_hline(yintercept = 0, col = \"grey\") +\n    labs(x = \"Fitted\", y = \"Residuals\", title = \"Residuals versus Fitted\")\n\n\ng1 / g2\n\n\n\n\n\n\n\n\nResidual Plots and Assumption Checking\nMathematics is the process of making assumptions and seeing if we can break them.\n\n\\(E(\\epsilon_i) = 0\\) is a given since \\(\\sum_{i=1}^n\\hat\\epsilon_i=0\\).\n\\(V(\\epsilon_i) = \\sigma^2\\)\n\nCheck if the variance looks stable.\n\n\\(\\epsilon_i \\sim N(0,\\sigma^2)\\) is harder to see\n\nExpect more points close to 0, fewer further away, no outliers\n\n\n\n\nResidual plots: unstable error\n\nx &lt;- runif(200, 0, 10)\ny0 &lt;- 2 - 3*x\ny &lt;- y0 + rnorm(length(x), 0, 2 * x)\n\ng1 &lt;- ggplot() + \n    aes(x = x, y = y) + \n    geom_point() + \n    geom_smooth(se = FALSE, method = \"lm\", formula = y~x) +\n    labs(x = NULL, y = NULL, title = \"y versus x\")\n\nxydf &lt;- augment(lm(y ~ x))\n\ng2 &lt;- ggplot(xydf) +\n    aes(x = .fitted, y = .resid) + \n    geom_point() +\n    geom_hline(yintercept = 0, col = \"grey\") +\n    labs(x = \"Fitted\", y = \"Residuals\", title = \"Residuals versus Fitted\")\n\ng1 + g2\n\n\n\n\n\n\nResidual plots: non-linear trend?\n\n## fig-height: 4\n## fig-width: 8\ng1 &lt;- ggplot(mtcars) + \n    aes(x = disp, y = mpg) + \n    geom_point() + \n    geom_smooth(se = FALSE, method = \"lm\", formula = y~x) +\n    labs(x = \"Engine Displacement\", y = \"Miles per Gallon\", title = \"y versus x\")\n\nmtdf &lt;- augment(lm(mpg ~ disp, data = mtcars))\n\ng2 &lt;- ggplot(mtdf) +\n    aes(x = .fitted, y = .resid) + \n    geom_point() +\n    geom_smooth(se = FALSE) +\n    geom_hline(yintercept = 0, col = \"grey\") +\n    labs(x = \"Fitted\", y = \"Residuals\", title = \"Residuals versus Fitted - non-linear trend?\")\n\ng1 + g2\n\n\n\n\n\n\nTesting Normality: Quantile-Quantile Plots\n\n\nConsider the data (2,3,3,4,5,5,6).\n\n50% of the data is below the median.\n\nFor a \\(N(0,1)\\) distribution, 50% of the data is below 0.\nPut the median on the y axis, 0 on the x axis.\n\n25% of the data is below Q1.\n\nFor a \\(N(0,1)\\) distribution, 25% is below qnorm(0.25) = -0.67\nPut a point at x = -0.67, y = Q1.\n\n75% of the data is below Q3.\n\nFor a \\(N(0,1)\\) distribution, 75% is below qnorm(0.75) = 0.67\nPut a point at x = 0.67, y = Q3.\n\n… and so on for the rest of the quantiles\n\n\nIf perfectly normal, expect a straight line!\n\nmydata &lt;- c(2, 3, 3, 4, 5, 5, 6)\nquants &lt;- qnorm(c(\n    0.125, 0.25, 0.375, 0.5, \n    0.625, 0.75, 0.875))\nplot(mydata ~ quants)\n\n\n\n\n\n\n\n\nOther Residual Plots:\nScale-Location\n\nScale: Standardized residual\nLocation: Fitted value\nMore on standardized residuals in Ch08\n\nCook’s Distance\n\nBasically, an outlier detection method.\nMore in Ch08\n\nLeverage\n\nMore in Ch08"
  },
  {
    "objectID": "L03-Residuals.html#participation-quiz",
    "href": "L03-Residuals.html#participation-quiz",
    "title": "3  L03: Assessing Fit",
    "section": "3.2 Participation Quiz",
    "text": "3.2 Participation Quiz\n\nQ01\nWhich of the following is not an assumption of linear models?\n\n$$\nThe variance is constant across all values of \\(X\\).\nThe height of the line is the mean value of \\(Y\\) for a given \\(X\\).\nNone of the above.\n\n\n\nQ02\nWhich of the following is a confidence interval for \\(s\\)?\n\n\\(\\chi^2_n(0.055) \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi^2_n(0.945)\\)\n\\(\\chi^2_n(0.945) \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi^2_n(0.055)\\)\n\\(s^2 \\pm \\text{Critical Value } * \\text{ se}(s^2)\\)\n\\(s \\pm \\text{Critical Value } * \\text{ se}(s)\\)\nNone of the above\n\n\n\nQ03\nWhich of the following is not a random variable?\n\n\\(Y\\)\n\\(\\hat\\epsilon_i\\)\n\\(\\hat Y\\)\n\\(\\underline\\epsilon\\)\n\n\n\nQ04\n\\(F = t^2\\)\n\nTrue\nFalse\nSometimes true\n\n\n\nQ05\nWhich of the following is the definition of a residual?\n\n\\(y_i - \\hat y_i\\)\n\\((y_i - \\hat y_i)^2\\)\n\\(\\hat y_i - y_i\\)\n\\((\\hat y_i - y_i)^2\\)\n\n\n\nQ06\nWhich of the following statements about \\(R^2\\) is false?\n\n\\(R^2 = SSReg / SST\\)\n\\(R^2 = r^2\\), where \\(r\\) is the correlation between \\(\\underline x\\) and \\(\\underline y\\)\n\\(R^2\\) compares the variance of the line to the variance of \\(y\\) alone\n\\(R^2\\) is not a random variable"
  },
  {
    "objectID": "L03-Residuals.html#maximum-likelihood",
    "href": "L03-Residuals.html#maximum-likelihood",
    "title": "3  L03: Assessing Fit",
    "section": "3.3 Maximum Likelihood",
    "text": "3.3 Maximum Likelihood\n\nMain Idea\nFind the values \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\), and \\(\\hat\\sigma^2\\) that maximize the likelihood of seeing our data.\nUnder the assumptions that \\(X\\) is fixed, \\(Y = \\beta_0 + \\beta_1X + \\epsilon_i\\), and \\(\\epsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\), \\[\nY \\sim N(\\beta_0 + \\beta_1X, \\sigma^2)\n\\]\n\n\nThe Likelihood\nThe probability of observering a data point is: \\[\nf_Y(y_i|x_i, \\beta_0, \\beta_1, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1x_i)^2\\right)\n\\]\nThe likelihood of the parameters, given the data, is: \\[\nL(\\beta_0, \\beta_1, \\sigma^2|x_i, y_i) = \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1x_i)^2\\right)\n\\]\n\nIt’s just a shift in perspective!\n\n\n\nSimple Coin Flip Example\nSuppose we flipped 10 bottle caps and got 6 “crowns”. Assume the probability of “crown” (\\(C\\)) is unknown, labelled \\(p\\).\n\nThe probability of one cap flip is \\(P(C|p) = p\\).\nThe probability of this is \\(P(C = 6|p) = p^6(1-p)^4\\).\n\nThis is just \\(P(\\underline y|p) = \\prod_{i=1}^nP(Y = y_i)\\).\n\nThe likelihood is \\(L(p|\\underline y) = \\prod_{i=1}^nP(Y = y_i)\\).\n\n\n\nMaximizing the Likelihood in LM\n\\[\nL(\\beta_0, \\beta_, \\sigma^2) = \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1x_i)^2\\right)\n\\]\n\nTo maximize w.r.t \\(\\beta_0\\), we set the derivative w.r.t \\(\\beta_0\\) to 0 and solve for \\(\\beta_0\\).\n\n\\(\\frac{\\partial L(\\beta_0, \\beta_, \\sigma^2)}{\\partial \\beta_0} = 0\\).\n\nRepeat for \\(\\frac{\\partial L(\\beta_0, \\beta_, \\sigma^2)}{\\partial \\beta_1} = 0\\) and \\(\\frac{\\partial L(\\beta_0, \\beta_, \\sigma^2)}{\\partial \\sigma^2} = 0\\)\n\nHWK: Show that the estimates for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are the same as the OLS estimates. The estimate for \\(\\hat\\sigma^2\\) should come out to: \\[\n\\hat\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n\\left(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i\\right)^2\n\\]\n\n\nBias-Variance Decomposition\nOn the next slide, we’ll show that \\(E\\left((Y_i - \\hat Y_i)^2\\right)\\) can be decomposed into \\(V(\\epsilon_i)\\), squared bias, and the variance of the fitted model.\n\n\\(V(\\epsilon_i)\\) is the variance of the true errors.\nBias is the difference between the true model and the estimated one.\n\nSystematic difference, not just random errors\n\ne.g. fitting a linear model to a non-linear trend\n\n\nVariance of the fitted model across all possible samples\n\nNote that this is a slight deviation from how the text presents it.\n\n\nDerivation\nSuppose the true value is \\(Y_i = g(x_i) + \\epsilon_i\\) (not necessarily linear), where \\(E(\\epsilon_i) = 0\\), \\(E(Y_i) = g(x_i)\\), and \\(V(\\epsilon_i) = \\sigma^2\\). \\[\\begin{align*}\nE\\left((Y_i - \\hat Y_i)^2\\right) &= E(Y_i - 2Y_i\\hat Y_i + \\hat Y_i^2)\\\\\n&= E(Y_i^2) - 2E(Y_i\\hat Y_i) + E(\\hat Y_i^2)\\\\\n&= E((g(x_i) + \\epsilon)^2) - 2E(g(x_i) + \\epsilon_i)E(\\hat Y_i) + E(\\hat Y_i^2)\\\\\n&= g^2(x_i) + 2g(x_i)E(\\epsilon_i) + E(\\epsilon_i^2) - 2g(x_i)E(\\hat Y_i) + E(\\hat Y_i^2)\\\\\n&= g^2(x_i) + \\sigma^2 - 2g(x_i)E(\\hat Y_i) + E(\\hat Y_i^2) + E(\\hat Y_i)^2 - E(\\hat Y_i)^2\\\\\n&= \\sigma^2 + (g(x_i) - E(\\hat Y_i))^2 + V(\\hat Y_i)\\\\\n&= \\text{Error Variance} + \\text{Bias}^2 + \\text{Fitted Model Variance}\n\\end{align*}\\]\n\n\nNot Interpreting the MSE\n\\[\nE((Y_i - \\hat{Y_i})^2) = \\sigma^2 + (g(x_i) - E(\\hat Y_i))^2 + V(\\hat Y_i)\n\\]\n\nWe don’t know any of the numbers on the right!!!\n\nThe decomposition is theoretical, we can’t tease apart \\(s^2\\) into these terms.\n\n\nInterpreting the MSE\nThe basic question of statistics: “How big is this number?”\n\nCompare to previous studies - is MSE larger than \\(\\sigma^2\\)?\n\nImplies that Bias\\(^2\\) and Fitted Model Variance are larger than expected.\nF-test\n\nCompare to “pure error” - direct estimate of \\(\\sigma^2\\).\n\ni.e. the variance in repeated trials on the same covariate values\nTextbooks devotes a lot to this, but it’s often not plausible.\n\nWon’t be on tests!\n\n\nCompare to another model\n\nWe’ll focus on this (later)!\n\n\n\n\nCompare to Previous Studies\nHypothesis test for \\(\\sigma^2 = \\sigma_0^2\\) versus \\(\\sigma^2 &gt; \\sigma_0^2\\), where \\(\\sigma_0\\) is the value from a previous study.\n\nIf significant, some of your error is coming from the study design!\n\n\n\nCompare to other models\nIt can be shown that \\(E(MSReg) = \\sigma^2 + \\beta_1S_{XX}\\).\nConsider the Null hypothesis \\(\\beta_1 =0\\) (why is this a good null?).\n\nUnder this null, \\(\\frac{MSReg}{s^2}\\sim F_{1, n-2}\\).\n\nObvious CI from this. \n\nThis is equivalent to the t-test for \\(\\beta_1\\)! (See text.)\n\n\n\nMSE of a Parameter: Bias of \\(s^2\\)\nFrom a previous class, we know that \\[\n\\frac{(n-2)s^2}{\\sigma^2}\\sim\\chi^2_{n-2}\n\\]\nFrom wikipedia, we know that the mean of a \\(\\chi^2_k\\) distribution is \\(k\\). Therefore, \\[\nE\\left(\\frac{(n-2)s^2}{\\sigma^2}\\right) = n-2 \\Leftrightarrow E(s^2) = \\sigma^2\n\\] and thus \\(s^2\\) is unbiased.\nThis does not necessarily mean that \\(s^2\\) is the best estimator for \\(\\sigma^2\\)!\n\n\nMSE of a Parameter: Bias of \\(s\\)\nEven though \\(s^2\\) is an unbiased estimator, \\(s = \\sqrt{s^2}\\) is biased! Specifically, \\(E(s) &lt; \\sigma\\)\nTo see why, first note that \\[\nV(s) = E(s^2) - (E(s))^2 \\Leftrightarrow E(s) = \\sqrt{E(s^2) - V(s)}\n\\] since \\(V(s) &gt; 0\\), \\(E(s^2) - V(s) &lt; E(s^2)\\), and therefore \\[\nE(s) &lt; \\sqrt{E(s^2)} = \\sqrt{\\sigma^2} = \\sigma\n\\]"
  },
  {
    "objectID": "L04-Matrix_Form.html#chapter-04-summary",
    "href": "L04-Matrix_Form.html#chapter-04-summary",
    "title": "4  L04: Regression in Matrix Form",
    "section": "4.1 Chapter 04 Summary",
    "text": "4.1 Chapter 04 Summary\n\nMatrix Forms of Things We’ve Seen\nUsing\n\\[\nY = \\begin{bmatrix}Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_n\\end{bmatrix};\\quad \\underline y = \\begin{bmatrix}y_1\\\\ y_2\\\\ \\vdots\\\\ y_n\\end{bmatrix};\\quad X = \\begin{bmatrix}1 & x_1\\\\1 &x_2\\\\ \\vdots & \\vdots\\\\ 1 & x_n\\end{bmatrix};\\quad \\underline\\beta = \\begin{bmatrix}\\beta_0\\\\\\beta_1\\end{bmatrix};\\quad\\underline\\epsilon \\begin{bmatrix}\\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots\\\\ \\epsilon_n\\end{bmatrix}\n\\]\n\\(Y = X\\underline\\beta + \\underline\\epsilon\\) is the same as \\[\\begin{align*}\nY_1 &= \\beta_0 + \\beta_1x_1 + \\epsilon_1\\\\\nY_2 &= \\beta_0 + \\beta_1x_2 + \\epsilon_2\\\\\n&\\vdots\\\\\nY_n &= \\beta_0 + \\beta_1x_n + \\epsilon_n\\\\\n\\end{align*}\\]\n\n\nSome Fun Matrix Forms\n\\[\\begin{align*}\n\\underline\\epsilon^T\\underline\\epsilon &= \\epsilon_1^2 + \\epsilon_2^2 + ... + \\epsilon_n^2 = \\sum_{i=1}^n\\epsilon_i^2\\\\\nY^TY &= \\sum Y_i^2\\\\\n\\mathbf{1}^T\\underline y &= \\sum y_i = n\\bar y\\\\\nX^TX &= \\begin{bmatrix}n & \\sum x_i\\\\ \\sum x_i & \\sum x_i^2\\end{bmatrix}\\\\\n(X^TX)^{-1} &= \\frac{1}{nS_{XX}}\\begin{bmatrix}\\sum x_i^2 & -n\\bar x\\\\-n \\bar x & n\\end{bmatrix}\\\\\nX^TY &= \\begin{bmatrix}\\sum Y_i\\\\ \\sum x_iY_i\\end{bmatrix}\n\\end{align*}\\]\n\n\nThe “Normal” Equations\n\n\nCopied from previous slide: \\[\\begin{align*}\nX^TX &= \\begin{bmatrix}n & \\sum x_i\\\\ \\sum x_i & \\sum x_i^2\\end{bmatrix}\\\\\nX^T\\underline y &= \\begin{bmatrix}\\sum y_i\\\\ \\sum x_iy_i\\end{bmatrix}\n\\end{align*}\\]\n\nThe textbook included the following equations in Ch02. The estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) in OLS (same as MLE) are the solution to:\n\\[\\begin{align*}\n\\hat\\beta_0 + \\hat\\beta_1\\sum x_i &= \\sum y_i\\\\\n\\hat\\beta_0\\sum x_i + \\hat\\beta_1\\sum x_i^2 &= \\sum x_iy_i\\\\\n\\end{align*}\\]\n\n\nPutting these together: \\[\nX^TX\\hat{\\underline\\beta} = X^T\\underline y\\quad \\Leftrightarrow\\quad \\hat{\\underline\\beta} = (X^TX)^{-1}X^T\\underline y\n\\] Try this out using the definition of \\((X^TX)^{-1}\\) on the previous slide.\n\n\n(Corrected) ANOVA in Matrix Form\n\n\n\n\n\n\n\n\n\nSource\n\\(df\\)\n\\(SS\\)\n\\(MS\\)\n\n\n\n\nRegression (corrected)\n1\n\\(\\hat{\\underline\\beta}^TX^T\\underline y - n\\bar{\\underline y}^2\\)\n\\(SS/df\\)\n\n\nError\n\\(n-2\\)\n\\(\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y\\)\n\\(SS/df\\)\n\n\nTotal (corrected)\n\\(n - 1\\)\n\\(\\underline y^t\\underline y - n\\bar{\\underline y}^2\\)\n\\(SS/df\\)\n\n\n\n\nThe “corrected” ANOVA is the ANOVA table for comparing the errors due to \\(\\hat\\beta_1\\) to the errors due to \\(\\hat\\beta_0\\).\nThis is different from comparing a model with \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) to a model with neither parameter.\n\nThe value of the “correction” is based on \\(\\bar x\\). If the slope is 0, then the estimate for \\(\\hat\\beta_0\\) is \\(\\bar x\\)."
  },
  {
    "objectID": "L04-Matrix_Form.html#variances",
    "href": "L04-Matrix_Form.html#variances",
    "title": "4  L04: Regression in Matrix Form",
    "section": "4.2 Variances",
    "text": "4.2 Variances\n\nVariance of a Vector: the Variance-Covariance Matrix\nIn general, for vector-valued random variable \\(Z = (Z_1, Z_2, \\dots, Z_n)\\), \\[\nV(Z) = \\begin{bmatrix}\nV(Z_1) & cov(Z_1, Z_2) & \\cdots & cov(Z_1, Z_n) \\\\\ncov(Z_2, Z_1) & V(Z_2) & \\cdots &  cov(Z_2, Z_n)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\ncov(Z_n, Z_1) & cov(Z_n, Z_2) & \\cdots & V(Z_n)\n\\end{bmatrix}\n\\]\n\n\n\\(V(\\hat{\\underline\\beta})\\)\n\n\nLet’s start with the covariance of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\): \\[\\begin{align*}\ncov(\\hat\\beta_0, \\hat\\beta_1) &= cov(\\bar y - \\hat\\beta_1\\bar x, \\hat\\beta_1)\\\\\n&= -\\bar x cov(\\hat\\beta_1, \\hat\\beta_1)\\\\\n&= -\\bar x V(\\hat\\beta_1)\\\\\n&= \\frac{-\\sigma^2\\bar x}{S_{XX}}\n\\end{align*}\\]\n\nThe var-covar matrix is: \\[\\begin{align*}\nV(\\hat{\\underline\\beta}) &= \\begin{bmatrix}\nV(\\hat\\beta_0) & cov(\\hat\\beta_0, \\hat\\beta_1)\\\\\ncov(\\hat\\beta_0, \\hat\\beta_1) & V(\\hat\\beta_1)\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\frac{\\sigma^2\\sum x_i^2}{S_{XX}} & \\frac{-\\sigma^2\\bar x}{S_{XX}}\\\\\n\\frac{-\\sigma^2\\bar x}{nS_{XX}} & \\frac{\\sigma^2}{S_{XX}}\n\\end{bmatrix}\\\\\n&= \\frac{\\sigma^2}{nS_{XX}}\\begin{bmatrix}\n\\sum x_i^2 & -n\\bar x\\\\\n-n\\bar x & n\n\\end{bmatrix}\\\\\n&= (X^TX)^{-1}\\sigma^2\n\\end{align*}\\]\n\n\n\n\nVariance of \\(\\hat Y_0\\)\nLet \\(X_0 = [1, x_0]\\) be an arbitrary observation. Then the predicted value of the line at \\(X_0\\), labelled \\(\\hat Y_0\\), is: \\[\\begin{align*}\nV(\\hat Y_0) &= V(X_0\\hat{\\underline\\beta})\\\\\n&= X_0V(\\hat{\\underline\\beta})X_0^T\\\\\n&= \\sigma^2X_0(X^TX)^{-1}X_0\n\\end{align*}\\]\nHWK: Verify that \\(X_0(X^TX)^{-1}X_0\\) is a \\(1\\times 1\\) matrix.\n\n\nVariance of \\(\\hat Y_{n+1}\\)\nFor a new observation, we have the variance of the line plus a new unobserved error \\(\\epsilon_{n+1}\\). \\[\\begin{align*}\nV(\\hat Y_{n+1}) &= V(X_{n+1}\\hat{\\underline\\beta} + \\epsilon_{n+1})\\\\\n&= X_{n+1}V(\\hat{\\underline\\beta})X_{n+1}^T + V(\\epsilon_{n+1})\\\\\n&= \\sigma^2X_{n+1}(X^TX)^{-1}X_{n+1}^T + \\sigma^2\\\\\n&= \\sigma^2\\left(X_{n+1}(X^TX)^{-1}X_{n+1}^T + 1\\right)\n\\end{align*}\\]\nHWK: Verify (empirically or mathematically) that this is smallest when \\(x_0 = \\bar x\\).\n\n\nSummary\nWhen we have one predictor, it is clear that: \\[\\begin{align*}\nY &= X\\hat{\\underline\\beta} + \\underline\\epsilon\\\\\n\\hat{\\underline\\beta} &= (X^TX)^{-1}X^T\\underline y\\\\\nV(\\hat{\\underline\\beta}) &= (X^TX)^{-1}\\sigma^2\\\\\nV(\\hat Y_0) &= \\sigma^2X_0(X^TX)^{-1}X_0^T\\\\\nV(\\hat Y_{n+1}) &= \\sigma^2(X_{n+1}(X^TX)^{-1}X_{n+1}^T + 1)\\\\\n\\end{align*}\\]\nThis will not change when we add predictors!"
  },
  {
    "objectID": "L04-Matrix_Form.html#participation-questions",
    "href": "L04-Matrix_Form.html#participation-questions",
    "title": "4  L04: Regression in Matrix Form",
    "section": "4.3 Participation Questions",
    "text": "4.3 Participation Questions\n\nQ01\nWhich statement about matrix multiplication is false?\n\n\\((AB)^T = B^TA^T\\)\n\\((AB)^{-1} = B^{-1}A^{-1}\\)\n\\((A + B)^T = A^T + B^T\\)\n\\(A^{-1}A = AA^{-1}\\)\n\n\n\nQ02\nThe Normal Equations come from:\n\nSetting the partial derivatives of \\(\\underline\\epsilon^T\\underline\\epsilon\\) to 0.\nSetting the partial derivatives of \\(\\hat{\\underline\\epsilon}^T\\hat{\\underline\\epsilon}\\) to 0.\nRearranging the equation \\(\\sum_{i=1}^n(y_i - \\hat y_i)^2\\) for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)\nRearranging the equation \\(\\sum_{i=1}^n(y_i - \\beta_0 + \\beta_1x_i + \\epsilon_i)^2\\) for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)\n\n\n\nQ03\n\\(MS_E = MS_T - MS_Reg\\)\n\nTrue\nFalse\n\n\n\nQ04\nFor vector-valued r.v. \\(Z\\), \\(V(Z)\\) is a symmetric matrix.\n\nNo, since the \\(i,j\\) element is \\(cov(Z_i, Z_j)\\) and the \\(j,i\\) element is \\(cov(Z_j, Z_i)\\).\nNo, since the \\(i,i\\) element is \\(V(Z_i)\\), which is different for different \\(i\\).\nYes, because it is a scalar.\nYes, because \\(cov(Z_i, Z_j) = cov(Z_j, Z_i)\\)\n\n\n\nQ05\nFor scalar-valued random variables \\(X\\) and \\(Y\\), which statement is true?\n\n\\(cov(a + bX, Y) = b cov(X,Y)\\)\n\\(cov(a + bX, bY) = b cov(X,Y)\\)\n\\(cov(bX, X) = b^2V(X)\\)\n\\(cov(a + bX, a + bY) = a + b cov(X,Y)\\)\n\n\n\nQ06\nThe variance of the line evaluated at an existing point is smaller than the variance of the line evaluated at a new point, even if the new point is the same as an existing point.\n\nTrue\nFalse"
  },
  {
    "objectID": "L05-General_Regression.html#chapter-summary",
    "href": "L05-General_Regression.html#chapter-summary",
    "title": "5  L05: The General Regression Situation",
    "section": "5.1 Chapter Summary",
    "text": "5.1 Chapter Summary\n\nThe Normal Equations\n\\[\\begin{align*}\n\\underline\\epsilon^T\\underline\\epsilon &= (Y - X\\underline\\beta)^T(Y - X\\underline\\beta)\\\\\n&= ... = Y^TY - 2\\underline\\beta^TX^TY + \\underline\\beta^TX^TX\\underline\\beta\n\\end{align*}\\] We then take the derivative with respect to \\(\\underline\\beta\\). Note that \\(X^TX\\) is symmetric and \\(Y^TX\\underline\\beta\\) is a scalar.. \\[\\begin{align*}\n\\frac{\\partial}{\\partial\\underline\\beta}\\underline\\epsilon^T\\underline\\epsilon &= 0 - 2X^TY + 2X^TX\\underline\\beta\n\\end{align*}\\]\n\nFor the 1 predictor case, make sure the equations look the same!\n\nSetting to 0, rearranging, and plugging in our data gets us the Normal equations: \\[\\begin{align*}\nX^TX\\underline{\\hat\\beta} &= X^T\\underline y\n\\end{align*}\\]\n\n\nFacts\n\\[X^TX\\underline{\\hat\\beta} = X^T\\underline y\\]\n\nNo distributional assumptions.\nIf \\(X^TX\\) is invertible, \\(\\hat{\\underline\\beta} = (X^TX)^{-1}X^T\\underline y\\).\n\n\\(\\hat{\\underline\\beta}\\) is a linear transformation of \\(\\underline y\\)!\nThis is the same as the MLE.\n\n\\(E(\\hat{\\underline\\beta}) = \\underline\\beta\\) and \\(V(\\hat{\\underline\\beta}) = \\sigma^2(X^TX)^{-1}\\).\n\nThis is the smallest variance amongst all unbiased estimators of \\(\\underline\\beta\\).\n\n\n\n\nExample Proof Problems\n\nProve that \\(\\sum_{i=1}^n\\hat\\epsilon_i\\hat y_i = 0\\).\nProve that \\((1/n)\\sum_{i=1}^n\\hat\\epsilon_i = 0\\).\nProve that \\(X^TX\\) is symmetric. Is \\(A^TA\\) symmetric in general?\n\n\n## Demonstration that they're true (up to a rounding error)\nmylm &lt;- broom::augment(lm(mpg ~ disp, data = mtcars))\nsum(mylm$.resid * mylm$.fitted)\n\n[1] 4.241496e-12\n\nmean(mylm$.resid)\n\n[1] 6.77236e-15\n\nX &lt;- model.matrix(mpg ~ disp, data = mtcars)\nall.equal(t(X) %*% X, t(t(X) %*% X))\n\n[1] TRUE\n\n\n\n\nAnalysis of Variance (Corrected)\n\n\n\n\n\n\n\n\nSource\n\\(df\\)\n\\(SS\\)\n\n\n\n\nRegression (corrected)\n\\(p - 1\\)\n\\(\\underline{\\hat{\\beta}}^TX^T\\underline y - n\\bar{\\underline y}^2\\)\n\n\nError\n\\(n - p\\)\n\\(\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y\\)\n\n\nTotal (corrected)\n\\(n - 1\\)\n\\(\\underline y^t\\underline y - n\\bar{\\underline y}^2\\)\n\n\n\n\nNote that \\(p\\) is the number of parameters, not the index of the largest param.\n\n\\(\\underline\\beta = (\\beta_0, \\beta_1, ..., \\beta_{p-1})\\)\n\nWe’ll always be using corrected sum-of-squares.\n\nEspecially next chapter!\n\n\n\n\n\\(F\\)-test for overall significance\nIf SSReg is significantly larger than SSE, then fitting the model was worth it!\n\nThis is a test for \\(\\beta_1 = \\beta_2 = ... = \\beta_{p-1} = 0\\), versus any \\(\\beta_j\\ne 0\\).\n\nAs before, we find a quantity with a known distribution, then use it for hypothesis tests.\n\\[\nF = \\frac{MS(Reg|\\hat\\beta_0)}{MSE} = \\frac{SS(Reg|\\hat\\beta_0)/(p-1)}{SSE/(n-p)} \\sim F_{p-1, n-p}\n\\]\nAgain, note that a regression with no predictors always has \\(\\hat\\beta_0 = \\bar y\\).\n\n\nExample: Significance of disp\n\n\n\nanova(lm(mpg ~ disp, data = mtcars)) |&gt; \n    knitr::kable()\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\ndisp\n1\n808.8885\n808.88850\n76.51266\n0\n\n\nResiduals\n30\n317.1587\n10.57196\nNA\nNA\n\n\n\n\n\n\n\\(p = 2\\), \\(n = 32\\).\n\n\\(df_R' + df_E' = df_T'\\), where \\(df'\\) is the df for corrected SS.\n\nVerified these numbers in the last lecture\n\n\n\nplot(mpg ~ disp, data = mtcars)\nabline(lm(mpg ~ disp, data = mtcars))\n\n\n\n\n\n\n\n\nExample: \\(F_{1,p-1} = t^2_{p-1}\\)\n\nanova(lm(mpg ~ qsec, data = mtcars)) |&gt; \n    knitr::kable()\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nqsec\n1\n197.3919\n197.39193\n6.376702\n0.017082\n\n\nResiduals\n30\n928.6553\n30.95518\nNA\nNA\n\n\n\n\nsummary(lm(mpg ~ qsec, data = mtcars))$coef |&gt;\n    knitr::kable()\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-5.114038\n10.0295433\n-0.5098974\n0.6138544\n\n\nqsec\n1.412125\n0.5592101\n2.5252133\n0.0170820\n\n\n\n\n\nWhat do you notice about these two tables?\n\n\nExample: Significance of Regression (\\(F_{2,p-1} \\ne t^2_{p-1}\\))\n\nanova(lm(mpg ~ qsec + disp, data = mtcars)) |&gt; \n    knitr::kable()\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nqsec\n1\n197.3919\n197.39193\n18.25740\n0.0001898\n\n\ndisp\n1\n615.1185\n615.11850\n56.89424\n0.0000000\n\n\nResiduals\n29\n313.5368\n10.81161\nNA\nNA\n\n\n\n\nsummary(lm(mpg ~ qsec + disp, data = mtcars))$coef |&gt;\n    knitr::kable()\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n25.5045079\n7.1840940\n3.5501356\n0.0013359\n\n\nqsec\n0.2122880\n0.3667758\n0.5787951\n0.5671961\n\n\ndisp\n-0.0398877\n0.0052882\n-7.5428272\n0.0000000\n\n\n\n\n\nWe’ll learn more about the ANOVA table next lecture.\n\n\n\\(R^2\\) again\n\\[\nR^2 = \\frac{SS(Reg|\\hat\\beta_0)}{Y^TY - SS(\\beta_0)} = \\frac{\\sum(\\hat y_i - \\bar y)^2}{\\sum(y_i - \\bar y)^2}\n\\]\nWorks for multiple dimensions!… kinda.\n\n\n\\(R^2\\) is bad?\n\n\n\nnx &lt;- 10 # Number of uncorrelated predictores\nuncorr &lt;- matrix(rnorm(50*(nx + 1)), \n    nrow = 50, ncol = nx + 1)\n## First column is y, rest are x\ncolnames(uncorr) &lt;- c(\"y\", paste0(\"x\", 1:nx))\nuncorr &lt;- as.data.frame(uncorr)\n\nrsquares &lt;- NA\nfor (i in 2:(nx + 1)) {\n    rsquares &lt;- c(rsquares,\n        summary(lm(y ~ ., \n            data = uncorr[,1:i]))$r.squared)\n}\nplot(1:10, rsquares[-1], type = \"b\",\n    xlab = \"Number of Uncorrelated Predictors\",\n    ylab = \"R^2 Value\",\n    main = \"R^2 ALWAYS increases\")\n\n\n\n\n\n\n\n\n\n\n\nAdjusted (Multiple) \\(R^2\\)\n\\[\nR^2_a = 1 - (1 - R^2)\\left(\\frac{n-1}{n-p}\\right)\n\\]\n\nPenalizes added predictors - won’t always increase!\n\nStill might increase by chance alone!\n\nF-test\n\n\\(R^2_a = R^2\\) when \\(p=1\\) (intercept model)\n\nStill not perfect!\n\nWorks for comparing different models on same data\nWorks (poorly) for comparing different models on different data.\n\nIn general you should use \\(R^2_a\\), but always be careful."
  },
  {
    "objectID": "L05-General_Regression.html#prediction-and-confidence-intervals-again",
    "href": "L05-General_Regression.html#prediction-and-confidence-intervals-again",
    "title": "5  L05: The General Regression Situation",
    "section": "5.2 Prediction and Confidence Intervals (Again)",
    "text": "5.2 Prediction and Confidence Intervals (Again)\n\n\\(R^2\\) and \\(F\\)\nRecall that \\[\nF = \\frac{MS(Reg|\\hat\\beta_0)}{MSE} = \\frac{SS(Reg|\\hat\\beta_0)/(p-1)}{SSE/(n-p)} \\sim F_{p-1, n-p}\n\\]\nFrom the definition of \\(R^2\\), \\[\\begin{align*}\nR^2 &= \\frac{SS(Reg|\\hat\\beta_0)}{SST}\\\\\n&= \\frac{SS(Reg|\\hat\\beta_0)}{SS(Reg|\\hat\\beta_0) + SSE}\\\\\n&= \\frac{(p-1)F}{(p-1)F + (n-p)}\n\\end{align*}\\] Conclusion: Hypothesis tests/CIs for \\(R^2\\) aren’t useful. Just use \\(F\\)!\n\n\nCorrelation of \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\), \\(\\hat\\beta_2\\), etc.\nWith a different sample, we would have gotten slightly different numbers!\n\nIf the slope changed, the intercept must change to fit the data\n\n(and )\nThe parameter estimates are correlated!\n\nSimilar things happen with multiple predictors!\nThis correlation can be a problem for confidence regions\n\n\n\nUncorrelated \\(\\hat\\underline{\\beta}\\)\n\\[\nV(\\hat{\\underline\\beta}) = \\sigma^2(X^TX)^{-1}\n\\]\nIn simple linear regression, \\[\n(X^TX)^{-1} = \\frac{1}{nS_{XX}}\\begin{bmatrix}\\sum x_i^2 & -n\\bar x\\\\-n \\bar x & n\\end{bmatrix}\n\\]\nso the correlation is 0 when \\(\\bar x = 0\\)!\n\n\nPrediction and Confidence Intervals for \\(Y\\)\n\\(\\hat Y = X\\hat\\beta\\).\n\nA confidence interval around \\(\\hat Y\\) is based on the variance of \\(\\hat\\beta\\).\n\\(\\hat Y \\pm t * se(X\\hat\\beta)\\)\n\n\\(Y_{n+1} = X\\beta + \\epsilon_{n+1}\\)\n\nA prediction interval around \\(Y_{n+1}\\) is based on the variance of \\(\\hat\\beta\\) and \\(\\epsilon\\)!\n\\(\\hat Y_{n+1} \\pm t * se(X\\hat\\beta + \\epsilon_{n+1})\\)"
  },
  {
    "objectID": "L05-General_Regression.html#participation-questions",
    "href": "L05-General_Regression.html#participation-questions",
    "title": "5  L05: The General Regression Situation",
    "section": "5.3 Participation Questions",
    "text": "5.3 Participation Questions\n\nQ1\nWhich of the following are the Normal equations?\n\n\\(X^TX\\underline\\beta = X^T\\underline y\\)\n\\(X^TX\\underline{\\hat\\beta} = X^T\\underline y\\)\n\\(\\hat{\\underline\\beta} = (X^TX)^{-1}X^T\\underline y\\)\n\\(f(\\epsilon_i) = \\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp\\left(\\frac{-1}{2}\\epsilon_i^2\\right)\\)\n\n\n\nQ2\nWhen is \\(X^TX\\) not invertible?\n\nOne of the predictors can be written as a linear combination of the others.\nThere are more predictors than observations.\nOne of the predictors has 0 variance.\nAll of the above\n\n\n\nQ3\nWhat does a significant F-test for the overall regression mean?\n\nThe variance in the line is significantly larger than the variance in the data.\nThe estimate of \\(\\beta_1\\) is significantly different from \\(\\beta_0\\),\nThe variance of the line is significantly different from 0.\nAt least one of the predictors in the model will have significant \\(t\\)-test.\n\n\n\nQ4\n\\(R^2\\) is best used for:\n\nDetermining whether a new predictor is worth including.\nComparing models with different numbers of predictors.\nComparing models based on different data sets.\nNone of the above.\n\n\n\nQ5\nWhich of the following describes a Prediction Interval?\n\nThe CI for the predicted value of the line\nThe CI for the predicted value of the line, including unobserved error at an \\(X\\) value\nThe CI for the predicted value of the line, including unobserved error at an \\(X\\) value that was not observed in the data\nThe CI for the predicted value of the line, including unobserved error at an \\(X\\) value that was not observed in the data, using the true value of \\(\\sigma^2\\)\n\n\n\nQ6\nWhich ANOVA table does the anova() function calculate?\n\n\n\n\n\n\n\n\n\n\n\nSource\n\\(df\\)\n\\(SS\\)\n\n\n\n\nRegression\n1\n\\(\\hat{\\underline\\beta}^TX^T\\underline y - n\\bar{\\underline y}^2\\)\n\n\nError\n\\(n-2\\)\n\\(\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y\\)\n\n\nTotal\n\\(n - 1\\)\n\\(\\underline y^t\\underline y - n\\bar{\\underline y}^2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\n\\(df\\)\n\\(SS\\)\n\n\n\n\nRegression\n1\n\\(\\hat{\\underline\\beta}^TX^T\\underline y\\)\n\n\nError\n\\(n-2\\)\n\\(\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y\\)\n\n\nTotal\n\\(n - 1\\)\n\\(\\underline y^t\\underline y\\)"
  },
  {
    "objectID": "L06-Extra_Sums_of_Squares-1.html#introduction",
    "href": "L06-Extra_Sums_of_Squares-1.html#introduction",
    "title": "6  L06: Extra Sum-of-Squares (Part 1)",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\n\nToday’s Main Idea\nIf you add or remove predictors, the variance of the residuals changes!\n\nAs always, we ask if it’s a “big” change.\nDifferent predictors have a different effect on the residuals.\n\nWhich predictors have a meaningful (significant?) effect on the residuals?\n\n\nSum-of-Squares due to Regression\n\nSince SSE = SST - SSReg and SST never changes, we’re focusing on SSReg.\nRecall: SSReg is the variance of the line itself!\n\n\\[\nSSReg = \\sum_{i=1}^n(\\hat y_i - \\bar y)\n\\]\n\n\nSSReg in two different penguin models\nIn the penguins data, we’re determining which predictors are associated with body mass.\n\n\\(SS_1\\) = SSReg for model 1\n\n\\(\\texttt{body\\_mass\\_g} = \\beta_0 + \\beta_1 \\texttt{flipper\\_length\\_mm} + \\beta_2 \\texttt{bill\\_length\\_mm} + \\beta_3 \\texttt{bill\\_depth\\_mm}\\)\n\n\\(SS_2\\) = SSReg for model 2\n\n\\(\\texttt{body\\_mass\\_g} = \\beta_0 + \\beta_1 \\texttt{flipper\\_length\\_mm} + \\beta_2 \\texttt{bill\\_length\\_mm}\\)\n\n\nNote: M2 is nested in M1 - M1 has all the same predictors and then some.\n\n\n\n\n\n\nImportant\n\n\n\n\\(\\beta_1\\) in the first model is different from \\(\\beta_1\\) in the second model.\n\n\n\n\nExtra Sum-of-Squares\nIf M2 is nested within M1, :\n\nM1: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength} + \\beta_3 \\texttt{billdepth}\\)\nM2: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength}\\)\n\nThen the Extra sum of squares is defined as: \\[\nSS(\\hat\\beta_3 | \\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) = S_1 - S_2\n\\]\nConvince yourself that S1 &gt; S2.\n\n\nSpecial Case: Corrected Sum-of-Squares\nWe’ve already seen this notation: \\[\nSSReg(\\hat\\beta_0) = n\\bar{\\underline y}^2\n\\] and \\[\nSSReg(corrected) = \\hat{\\underline\\beta}^TX^T\\underline y - n\\bar{\\underline y}^2 = S_1 - S_2\n\\] where \\(S_2\\) is the sum-of-squares for the null model!\n\n\nUnspecial Case: Correction doesn’t matter!\nConsider \\(S_{1c}\\) and \\(S_{2c}\\), the corrected versions of \\(S_1\\) and \\(S_2\\). Then\n\\[\\begin{align*}\nS_{1c} - S_{2c} = (S_2 - n\\bar{\\underline y}^2) - (S_1 - n\\bar{\\underline y}^2) = S_1 - S_2\n\\end{align*}\\]\nIn other words, the correction term doesn’t matter.\nThis is useful because R outputs the corrected versions.\n\n\nUnspecial Case: SSReg versus SSE doesn’t matter!\nConsider \\(SSE_1\\) and \\(SSE_2\\). Since SST is the same for both models,\n\\[\\begin{align*}\nSSE_2 - SSE_1 = (SST - S_1) - (SST - S_2) = S_2 - S_1\n\\end{align*}\\]\nNotice that the order is switched, which is fine.\n\n\nANOVA Tests for ESS\nConsider the models:\n\nM1: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength} + \\beta_3 \\texttt{billdepth}\\)\n\n\\(df_1 = 4\\)\n\nM2: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength}\\)\n\n\\(df_2 = 3\\)\n\n\nIf we choose \\(H_0: \\beta_3 = 0\\) in model 1, then \\[\n\\frac{S_1 - S_2}{(4 - 3)s^2} \\sim F_{1,\\nu}\n\\]\nwhere \\(s^2\\) is the error variance (MSE) in the larger model with degress of freedom \\(\\nu = df_1\\).\nThis is almost identical to the F-test for only one predictor (with one important difference).\n\n\nIn General\nIf M1 has \\(p\\) df, M2 has \\(q\\) df, and one is nested in the other, then \\(\\nu = \\max(p, q)\\) and\n\\[\n\\frac{S_1 - S_2}{(p - q)s^2} \\sim F_{|p-q|,\\nu}\n\\]\nNote that it doesn’t matter which is nested: \\(S_1 - S_2\\) has the same sign as \\(p-q\\), so it’s always positive.\n\n\nOmnibus Tests for Multiple Predictors\nSuppose we want to test if any bill measurement is useful.\n\nBill length and depth are highly correlated - marginal CIs won’t be valid.\nConfidence Regions are hard (and only work in 2D)\n\nInstead, we can use the ESS to test for a subset of predictors!\n\nM1: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength} + \\beta_3 \\texttt{billdepth}\\)\nM2: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength}\\)\n\n\\(S_1 = S_2\\) is equivalent to \\(\\beta_2 = \\beta_3 = 0\\), and it accounts for their covariance!\nIf significant, then at least one of \\((\\beta_2, \\beta_3)\\) is not 0.\n\n\nIn R\n\nlibrary(palmerpenguins)\npeng &lt;- penguins[complete.cases(penguins),]\nm1 &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n    data = peng)\nm2 &lt;- lm(body_mass_g ~ flipper_length_mm,\n    data = peng)\nanova(m1, m2) |&gt; knitr::kable()\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n329\n50814912\nNA\nNA\nNA\nNA\n\n\n331\n51211963\n-2\n-397050.9\n1.285349\n0.2779392\n\n\n\n\n\n\n\nNext time\n\nWhen to check ESS\nHow to check all ESS\nWhat is R’s anova() function even doing??"
  },
  {
    "objectID": "L07-Exampless.html#review",
    "href": "L07-Exampless.html#review",
    "title": "7  L07: ESS Exampless",
    "section": "7.1 Review",
    "text": "7.1 Review\nFrom last time, we basically learned what the following means:\n\\[\n\\frac{SS(\\hat\\beta_{q+1}, ..., \\hat\\beta_p | \\hat\\beta_0, ... \\hat\\beta_q)}{(p-q)s^2} =\\frac{S_1 - S(\\hat\\beta_0) - (S_2 - S(\\hat\\beta_0))}{(p-q)s^2}\\sim F_{p-q, \\max(p, q)}\n\\] where \\(s^2\\) is the MSE calculated from the larger model.\nThis allows us to do a test for whether \\(\\beta_{q+1} = \\beta_{q+2} = ... = \\beta_p = 0\\).\nThe R code to do this test is as follows. In this code, we believe that the bill length and bill depth are strongly correlated, and thus we cannot trust the CIs that we get from summary(lm()) (we saw “Confidence Regions” in the slides and code for L05).\n\nnrow(peng)\n\n[1] 333\n\n\n\nlm1 &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n    data = peng)\nlm2 &lt;- lm(body_mass_g ~ flipper_length_mm,\n    data = peng)\nanova(lm2, lm1)\n\nAnalysis of Variance Table\n\nModel 1: body_mass_g ~ flipper_length_mm\nModel 2: body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm\n  Res.Df      RSS Df Sum of Sq      F Pr(&gt;F)\n1    331 51211963                           \n2    329 50814912  2    397051 1.2853 0.2779\n\n\nLet’s try and calculate these values ourselves in a couple different ways!\n\nanova(lm1)\n\nAnalysis of Variance Table\n\nResponse: body_mass_g\n                   Df    Sum Sq   Mean Sq   F value Pr(&gt;F)    \nflipper_length_mm   1 164047703 164047703 1062.1232 &lt;2e-16 ***\nbill_length_mm      1    140000    140000    0.9064 0.3418    \nbill_depth_mm       1    257051    257051    1.6643 0.1979    \nResiduals         329  50814912    154453                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom this model, SSE is 50814912 on 329 degrees of freedom. This is the same as the SSE in the output of anova(lm2, lm1).\n\nanova(lm2)\n\nAnalysis of Variance Table\n\nResponse: body_mass_g\n                   Df    Sum Sq   Mean Sq F value    Pr(&gt;F)    \nflipper_length_mm   1 164047703 164047703  1060.3 &lt; 2.2e-16 ***\nResiduals         331  51211963    154719                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAgain, the SSE of 51211963 matches what we saw in anova(lm2, lm1), and we have 331 degrees of freedom (as expected, the differences in degrees of freedom is 2).\nNote that the F-value in anova() is just the ratio of the MSEs, but this is not the case here. Instead, we need to calculate \\(s^2\\).\nWe can calculate \\(s^2\\) as the MSE for the larger model:\n\ns2 &lt;- 50814912/329\ns2\n\n[1] 154452.6\n\n\nAnd now we can calculate the F-value as expected:\n\n(51211963 - 50814912)/ (2 * s2)\n\n[1] 1.285349\n\n\n\nprint(1- pf(1.28539, 2, 329))\n\n[1] 0.2779278\n\n\nIt is left as an exercise to calculate these values based on matrix multiplication. I highly suggest trying it with and without correction factors to convince yourself that both of them work (and to convince yourself that you know what the correction factor is and why it’s necessary)."
  },
  {
    "objectID": "L07-Exampless.html#ess-algorithms",
    "href": "L07-Exampless.html#ess-algorithms",
    "title": "7  L07: ESS Exampless",
    "section": "7.2 ESS Algorithms",
    "text": "7.2 ESS Algorithms\nThe idea above is based on testing a subset of predictors for at least one significant coefficient. This is usually what we want.\nHowever, there are also times where we want to check all predictors one-by-one. This is much less common than the textbook may lead you to believe, but it still happens.\nThere are three ways to calculate the ESS for all predictors. They are very helpfully labelled Types I, II, and III.\n\nType I: Sequential Sum-of-Squares (with interactions)\n\nCheck \\(SS(\\hat\\beta_1|\\hat\\beta_0)\\)\nCheck \\(SS(\\hat\\beta_2|\\hat\\beta_0, \\hat\\beta_1)\\)\nCheck \\(SS(\\hat\\beta_2:\\hat\\beta_1|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\n\n\\(\\hat\\beta_2:\\hat\\beta_1\\) is an interaction term, which means we use a formula like y ~ x1 + x2 + x1*x2 (although we’ll learn why R uses different notation than this).\n\nCheck \\(SS(\\hat\\beta_3|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\nCheck all interactions between x1, x2, and x3,\n…\n\n\nThis will give us every possible sum-of-squares. This is very very dubious, and can lead to a multiple comparisons problem!\n\nType 2: Sequential Sum-of-Squares (R’s Default)\n\nCheck \\(SS(\\hat\\beta_1|\\hat\\beta_0)\\)\nCheck \\(SS(\\hat\\beta_2|\\hat\\beta_0, \\hat\\beta_1)\\)\nCheck \\(SS(\\hat\\beta_3|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\n…\n\n\nThis gives us an ordered sequence of “is it worth adding x1?”, “if we have x1, is it worth adding x2?”, etc. This is only meaningful if the predictors are naturally ordered (such as polynomial regression, see below).\n\nType 3: Last-entry sum-of-squares\n\nCheck \\(SS(\\hat\\beta_1|\\hat\\beta_0, \\hat\\beta_2, \\hat\\beta_3)\\)\nCheck \\(SS(\\hat\\beta_2|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_3)\\)\nCheck \\(SS(\\hat\\beta_3|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\n\n\nThis checks whether adding predictor \\(x_i\\) is worth it, considering all other predictors are already in the model.\n\nType 2 ANOVA (Sequental Sum-of-Squares)\nBy default, R does sequential sum-of-squares. This is a very important fact to know!\nIn Types I and II, the order of the predictors matters. In fact, you cannot make any conclusions about the significance that doesn’t make reference to this fact.\n\n## Try changing the order to see how the significance changes!\nmylm &lt;- lm(mpg ~ qsec + disp + wt, data = mtcars)\nanova(mylm)\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nqsec       1 197.39  197.39  28.276 1.165e-05 ***\ndisp       1 615.12  615.12  88.116 3.816e-10 ***\nwt         1 118.07  118.07  16.914 0.0003104 ***\nResiduals 28 195.46    6.98                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mylm)$coef # No obvious connection to anova\n\n                 Estimate Std. Error     t value     Pr(&gt;|t|)\n(Intercept) 19.7775575655 5.93828659  3.33051585 0.0024420674\nqsec         0.9266492353 0.34209668  2.70873496 0.0113897664\ndisp        -0.0001278962 0.01056025 -0.01211109 0.9904228666\nwt          -5.0344097167 1.22411993 -4.11267686 0.0003104157\n\n\nHowever, there is at least one case where we do care about the order of the predictors. Consider polynomial regression, which we will return to later. For now, it is sufficient to know that we’re dealing with the model: \\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + ... + \\beta_{p-1}x_i^{p-1} + \\epsilon_i\n\\]In this model, notice that we only have one predictor \\(x\\), but we have performed non-linear transformations (HMWK: why is it important that the transformations are non-linear?).\nIn this case, a sequential SS setup makes quite a bit of sense. Given we have a linear model, is it worth making it quadratic? Given that we have a quadratic model, is it worth making it cubic? Given that we have a cubic model…\nIn the code below, I use the I() function (the I means identity) to make the polynomial model. The “formula” notation in R, y ~ x + z, has a lot of options. Including x^2, rather than I(x^2), makes R think we want to do one of the more fancy things, but the I() tells it that we want to literally square it. In the future, we’ll use a better way of doing this.\n\nx &lt;- runif(600, 0, 20)\ny &lt;- 2 - 3*x + 3*x^2 - 0.3*x^3 + rnorm(600, 0, 100)\nplot(y ~ x)\n\n\n\nmylm &lt;- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))\nanova(mylm)\n\nAnalysis of Variance Table\n\nResponse: y\n           Df   Sum Sq  Mean Sq   F value Pr(&gt;F)    \nx           1 49560412 49560412 4726.8201 &lt;2e-16 ***\nI(x^2)      1 19661307 19661307 1875.1955 &lt;2e-16 ***\nI(x^3)      1  1150679  1150679  109.7459 &lt;2e-16 ***\nI(x^4)      1      661      661    0.0630 0.8018    \nI(x^5)      1      112      112    0.0107 0.9177    \nI(x^6)      1     6274     6274    0.5984 0.4395    \nResiduals 593  6217568    10485                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom the table above, we can clearly see that this should just be a cubic model (which is the true model that we generated). Try changing things around to see if, say, it will still detect an order 5 polynomial if if there’s no terms of order 3 or 4.\n\n\nA note on calculations\nTake a moment to consider the following. Suppose I checked the following two (Type II) ANOVA tables:\n\nanova(lm(mpg ~ disp, data = mtcars))\nanova(lm(mpg ~ disp + wt, data = mtcars))\n\nBoth tables will have the first row labelled “disp” and include its sum-of-squares along with the F-value. Do you expect these two rows to be the same?\nThink about it.\nThink a little more.\nWhat values do you expect to be used in the calculation?\nWhich sums-of-squares? Which variances?\nLet’s test it out:\n\nanova(lm(mpg ~ disp, data = mtcars))\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndisp       1 808.89  808.89  76.513 9.38e-10 ***\nResiduals 30 317.16   10.57                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm(mpg ~ disp + wt, data = mtcars))\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndisp       1 808.89  808.89 95.0929 1.164e-10 ***\nwt         1  70.48   70.48  8.2852  0.007431 ** \nResiduals 29 246.68    8.51                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThey’re different! As homework, find out where the F value for disp is coming from in both tables. (All required values are in the table, and the answer was stated earlier!)\nWith both the polynomial and the disp example, we see that the interpretation of the anova table is highly, extremely, extraordinarily dependent on which predictors we choose to include AND the order in which we choose to include them. So, yeah. Be careful.\n\n\nType III SS in R\nThere isn’t a built-in function to do this. To create this, we can either use our math (my preferred method) or test each one individually.\n\nanova(\n    lm(mpg ~ disp + wt, data = mtcars),\n    lm(mpg ~ disp + wt + qsec, data = mtcars)\n)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp + wt\nModel 2: mpg ~ disp + wt + qsec\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     29 246.68                              \n2     28 195.46  1     51.22 7.3372 0.01139 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(\n    lm(mpg ~ disp + qsec, data = mtcars),\n    lm(mpg ~ disp + wt + qsec, data = mtcars)\n)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp + qsec\nModel 2: mpg ~ disp + wt + qsec\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     29 313.54                                  \n2     28 195.46  1    118.07 16.914 0.0003104 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(\n    lm(mpg ~ wt + qsec, data = mtcars),\n    lm(mpg ~ disp + wt + qsec, data = mtcars)\n)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ wt + qsec\nModel 2: mpg ~ disp + wt + qsec\n  Res.Df    RSS Df Sum of Sq     F Pr(&gt;F)\n1     29 195.46                          \n2     28 195.46  1 0.0010239 1e-04 0.9904"
  },
  {
    "objectID": "L07-Exampless.html#modelling-strategies",
    "href": "L07-Exampless.html#modelling-strategies",
    "title": "7  L07: ESS Exampless",
    "section": "7.3 Modelling Strategies",
    "text": "7.3 Modelling Strategies\nIf you are in a situation where you think to yourself “my predictors are logically ordered and I want to check for the significance of all of them one-by-one”, you want Type II.\nIf you think “they’re not ordered but I want to check significance”, you might want to check the overall F test for all predictors and then check t-tests for individual parameters.\nIf you think “what would happen if each predictor were the last one I put in the model”, then you want Type III. I can’t think of a good situation for Type I - you’re pretty much guaranteed to have a multiple comparisons issue.\nI also want to call attention to the fact that all of these algorithms assume that you have a set of predictors that you already know you want to check. If you noticed, there are other predictors in the mtcars dataset that we did not consider!\nWe’ll slowly build up some intuition over time, but my advice for choosing which predictors to include is as follows:\n\nStart with a lot of plots.\nBased on the plots and your knowledge of the context, create a candidate set of predictors that you think will be the final model.\nCheck the model fit (p-values, residuals, etc).\nBased on your knowledge of the context, check significance of groups of predictors that you think are highly correlated.\nYour final model will be based on the tests for groups of (or individual) predictors that you suspect would be relevant.\n\nThe purpose of this method for selecting predictors is to minimize the number of p-values that you check. The ESS techniques that we learned today (especially for the bill length/depth, where our knowledge of the problem informed us of which predictors to check) are an important part of the modelling process, but there is more to learn!"
  },
  {
    "objectID": "L09-Serial_Correlation.html#serial-correlation-in-the-residuals",
    "href": "L09-Serial_Correlation.html#serial-correlation-in-the-residuals",
    "title": "8  L09: Cereal Correlation in the Residuals",
    "section": "8.1 Serial Correlation in the Residuals",
    "text": "8.1 Serial Correlation in the Residuals\n\nAssumptions and Definitions\nThe time of each observation is recorded, and they are equally spaced.\n\nIn other words, \\(x_1\\) is observed at time 1, \\(x_2\\) is observed at time 2, etc.\n\nSerial Correlation: \\(cor(\\epsilon_{t-1}, \\epsilon_t)\\ne 0\\)\n\nSerial Correlation is not causation.\n\nKnowledge of one gives you more knowledge of the other.\n\nSerial correlation can be negative\n\nExample: didn’t hit quota today, so big push tomorrow.\n\n\n\n\nVisualizing Serial Correlation in the Residuals\nR"
  },
  {
    "objectID": "L09-Serial_Correlation.html#durbin-watson",
    "href": "L09-Serial_Correlation.html#durbin-watson",
    "title": "8  L09: Cereal Correlation in the Residuals",
    "section": "8.2 Durbin-Watson",
    "text": "8.2 Durbin-Watson\n\nStrong Assumption about Correlation\nThe usual model: \\(Y = X\\underline{\\beta} + \\underline \\epsilon\\).\nAssume that \\(cor(\\epsilon_{t-1}, \\epsilon_t) = \\rho\\), \\(cor(\\epsilon_{t-2}, \\epsilon_t)= \\rho^2\\), \\(cor(\\epsilon_{t-3}, \\epsilon_t) = \\rho^3\\), etc.\n\nThe correlation is proportional to the distance in time.\n\nThis can be written as: \\[\n\\epsilon_t = \\rho\\epsilon_{t-1} + z_t\n\\] where \\(z_t\\sim N(0,\\sigma^2)\\). Note that \\(V(\\hat\\epsilon_t) = \\frac{\\sigma^2}{1 - \\rho^2}\\).\n\n\nThe Durbin-Watson test statistic\nAs usual, we find a quantity with a known distribution: \\[\nd = \\dfrac{\\sum_{s=2}^n(\\hat\\epsilon_s - \\hat\\epsilon_{s-1})^2}{\\sum_{s=1}^n\\hat\\epsilon_s^2}\\sim\\text{ some complicated distribution}\n\\]\n\nDistribution has a closed form, but I hate it.\n\\(d\\in [0, 4]\\), with \\(d=2\\) corresponding to the null.\nR will calculate the values for you.\n\nTextbook has pages and pages of tables. Textbook was written before iPhones existed.\n\n\nI’ll show an example of DW later.\n\n\nCautions with Durbin-Watson\n\nTests the hypotheses \\(H_a:\\;cor(\\epsilon_{t-s}, \\epsilon_t) = \\rho^s\\) versus not that.\n\nThere are many, many other \\(H_a\\). DW has low power for these situations.\n\nGraphical summaries will reveal strong patterns; patterns found by DW might not be worrisome. \nIt’s more p-values to look at. We want to minimize the number of p-values we look at."
  },
  {
    "objectID": "L09-Serial_Correlation.html#graphical-methods",
    "href": "L09-Serial_Correlation.html#graphical-methods",
    "title": "8  L09: Cereal Correlation in the Residuals",
    "section": "8.3 Graphical Methods",
    "text": "8.3 Graphical Methods\n\nEmpirical Autocorrelation\n\n\n\nWe can simply find the correlation between \\(\\hat \\epsilon_t\\) and \\(\\hat \\epsilon_{t-1}\\)!\nWe can do the same for \\(\\hat \\epsilon_t\\) and \\(\\hat \\epsilon_{t-2}\\).\n\netc.\n\n\n\n\n\n\n\n\n\n\n\n\nt\n\\(\\hat \\epsilon_t\\)\n\\(\\hat \\epsilon_{t-1}\\)\n\\(\\hat \\epsilon_{t-2}\\)\n\n\n\n\n2\n\\(\\hat \\epsilon_1\\)\nNA\nNA\n\n\n2\n\\(\\hat \\epsilon_2\\)\n\\(\\hat \\epsilon_1\\)\nNA\n\n\n3\n\\(\\hat \\epsilon_3\\)\n\\(\\hat \\epsilon_2\\)\n\\(\\hat \\epsilon_1\\)\n\n\n4\n\\(\\hat \\epsilon_4\\)\n\\(\\hat \\epsilon_3\\)\n\\(\\hat \\epsilon_2\\)\n\n\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\n\n\n\n\n\n\nThe ACF: Empirical Autocorrelations of lag \\(k\\)\n\n\nACF: AutoCorrelation Function\nThe x-axis shows the lag, the y axis shows the correlations\nThe plot on the right shows an example of time series data.\n\n\npar(mfrow = c(2, 1))\nplot(co2, main = \"Time series of CO2 data\")\nacf(co2, main = \"ACF of CO2 data (not residuals)\")\n\n\n\n\n\n\n\n\nACF isn’t ideal\nIn the model we saw for DW, \\[\n\\epsilon_t = \\rho\\epsilon_{t-1} + z_t\n\\] which means that \\[\n\\epsilon_t = \\rho(\\rho\\epsilon_{t-2} + z_{t-1}) + z_t\n\\]\nThe lag 2 correlation (the \\(\\rho^2\\) term) includes the lag 1 correlation!\n\n\nPartial Autocorrelations\n\n\nIf we extend the model to: \\[\n\\epsilon_t = \\rho_1\\epsilon_{t-1} + \\rho_2\\epsilon_{t-2} + z_t,\n\\] then \\(\\rho_2\\) is the correlation in the lag 2 terms, accounting for lag 1 terms!\nThis is the PACF, and it’s often much more useful.\nThe plot on the right shows a cyclic trend.\n\n\npar(mfrow = c(2, 1))\nplot(co2, main = \"Time series of CO2 data\")\npacf(co2, main = \"PACF of CO2 data (not residuals)\")\n\n\n\n\n\n\n\n\nDW, ACF, and PACF in practice\nMost of the time, just check the PACF.\n\nIf you see something, check ACF.\nIf you’re in a field that requires p-values, show them the DW statistic.\n\nOr use a non-parametric test…\n\n\n\n\nWhat to do if there is autocorrelation?\n\nCorrelation in the residuals might mean correlation in the \\(y_i\\)’s\n\nTry time series modelling!\n\nIf it’s simple (lag 1) autocorrlation, the data could potentially be transformed to remove the autocorrelation.\n\n\\(y_t - y_{t-1} = X\\underline \\beta\\).\nChange estimation to account for autocorrelation.\n\nIf it’s complicated, get a PhD student to do it for you."
  },
  {
    "objectID": "L09-Serial_Correlation.html#participation",
    "href": "L09-Serial_Correlation.html#participation",
    "title": "8  L09: Cereal Correlation in the Residuals",
    "section": "8.4 Participation",
    "text": "8.4 Participation\n\nQ1\nSerial correlation can be tested for any data set.\n\n\nTrue\nFalse\n\n\n\n\nQ2\nA non-significant result from the DW test means there is no autocorrelation in the residuals.\n\nTrue\n\n\nFalse\n\n\n\nQ3\nThe quantity \\(d = \\dfrac{\\sum_{s=2}^n(\\hat\\epsilon_s - \\hat\\epsilon_{s-1})^2}{\\sum_{s=1}^n\\hat\\epsilon_s^2}\\) is a statistic because:\n\nIt’s a value calculated from the data, possibly including information from outside the data.\nIt’s a value calculated from the data only, with no other information.\nIt’s a value calculated from the data only, with no other information, and has a known distribution.\nIt’s not a statistic since it’s not estimating a population parameter.\n\n\n\nQ4\nThe “partial” in PACF refers to:\n\nThe PACF plot only contains some of (partial) information.\nThe PACF evaluates the lag \\(k\\) correlation after controlling for lags 1 to \\(k-1\\).\nThe PACF is only evaluated for a portion of the data.\nThe PACF for a lag of \\(k\\) cannot use the first \\(k-1\\) data points (they are the NAs in the table).\n\n\n\nQ5\nWhich of the following is not an assumption of the DW test?\n\nThe time points are all equally spaced.\nThe correlation between two residuals is equal to \\(\\rho\\).\nThe further apart two residuals are, the less correlated they are.\nThere is no missing data.\n\n\n\nQ6\nAutocorrelation means that there are no possible insights into the data.\n\nTrue, the study was worthless.\nFalse, there are standard methods that will work for any situation.\nFalse, but we won’t get into the details in this course."
  },
  {
    "objectID": "L09-Serial_Correlation.html#non-parametric-test-for-autocorrelation",
    "href": "L09-Serial_Correlation.html#non-parametric-test-for-autocorrelation",
    "title": "8  L09: Cereal Correlation in the Residuals",
    "section": "8.5 Non-Parametric Test for Autocorrelation",
    "text": "8.5 Non-Parametric Test for Autocorrelation\n\nRuns\n\\(\\sum_{i=1}^n\\hat\\epsilon_i = 0\\), so some residuals are positive and some are negative.\nThe runs test just looks at the sign of the residuals. Consider the signs:\n+ + - + - - - - + + - + + + + \nThere are 7 runs in these residuals. Is this a lot of runs?\n\n\nDefining “A Lot Of Runs”\np-value: Probability of a result at least as extreme as the one obtained, under the null hypothesis.\n\nNull: random +’s and -’s.\n\nFor small numbers, we can look at all sequences of +’s and -’s and count the runs!\n\nP(7 or more runs) is an upper tailed test (-ive autocorrelation)\nP(7 or fewer runs) is a test for +ive autocorrelation\n\n\n\nLarge Numbers: Of course it’s Normal!\nGiven \\(n_1\\) +’s and \\(n_2\\) -’s, the mean and variance of the number of runs is: \\[\n\\mu = \\frac{2n_1n_2}{n_1 + n_2} + 1\\text{, and }\\sigma^2 = \\frac{2n_1n_2(2n_1n_2 - n_1 - n_2)}{(n_1+n_2)^2(n_1 + n_2 - 1)}\n\\]\nIn the actual distribution, \\(P(runs\\le \\mu) = P(runs\\le \\mu -1/2) = P(runs &lt; \\mu + 1/2)\\).\nIn the normal distribution this isn’t true, so we apply a correction factor:\n\nLower-tailed test: \\(runs\\sim N(\\mu + 1/2, \\sigma^2)\\)\nUpper-tailed test: \\(runs\\sim N(\\mu - 1/2, \\sigma^2)\\)\nTwo-tailed test: \\(runs\\sim N(\\mu, \\sigma^2)\\) and we hope it averages out.\n\n\n\nExample\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/SerialCorrelation\")"
  },
  {
    "objectID": "L10-Hat-Resid_Plots-Cook.html#le-chapeau",
    "href": "L10-Hat-Resid_Plots-Cook.html#le-chapeau",
    "title": "9  L10: The Hat Matrix",
    "section": "9.1 Le Chapeau",
    "text": "9.1 Le Chapeau\n\nThe Hat Matrix\n\\[\nH = X(X^TX)^{-1}X^T\n\\]\nRecall: The hat matrix projects \\(Y\\) onto \\(\\hat Y\\), based on \\(X\\).\n\n\\(\\hat Y = HY\\)\n\n\\(\\hat Y_i = h_{ii} Y_i + \\sum_{j\\ne i}h_{ij}Y_j\\)\n\n\n\n\nVariance-Covariance matrix of \\(\\hat{\\underline\\epsilon}\\)\nJust like \\(\\beta_0\\) and \\(\\beta_1\\), each sample results in different \\(\\underline{\\hat\\epsilon}\\).\nAcross samples, we have: \\[\n\\underline{\\hat\\epsilon} - E(\\underline{\\hat\\epsilon}) = (I-H)(Y-X\\underline{\\beta}) = (I-H)\\underline{\\epsilon}\n\\] and therefore: \\[\\begin{align*}\nV(\\underline{\\hat\\epsilon}) &= E([\\underline{\\hat\\epsilon} - E(\\underline{\\hat\\epsilon})][\\underline{\\hat\\epsilon} - E(\\underline{\\hat\\epsilon})]^T)\\\\\n&= [I-H]E(\\underline{\\epsilon}\\underline{\\epsilon}^T)[I-H]^T\\\\\n&= [I-H]\\sigma^2[I-H]\\\\\n&= [I-H]\\sigma^2\n\\end{align*}\\] where we used the idempotency and symmetry of \\(I-H\\).\n\n\nThe variance of a residual\nGiven that \\(V(\\underline{\\hat\\epsilon}) = (I-H)\\sigma^2\\), \\[\nV(\\hat\\epsilon_i) = (1-h_{ii})\\sigma^2\n\\]\n\nThe variance of the residual depends on how much \\(Y_i\\) influences it’s own estimate.\n\nHigh influence = low variance.\n\n\nThe correlation between residuals is: \\[\n\\rho_{ij} = \\frac{Cov(\\hat\\epsilon_i, \\hat\\epsilon_j)}{\\sqrt{V(\\hat\\epsilon_i)V(\\hat\\epsilon_j)}} = \\frac{-h_{ij}}{\\sqrt{(1-h_{ii})(1-h_{jj})}}\n\\]\n\nThe covariance is negative! A large residual tells us there are small residuals.\n\n“Large” and “small” are relative\n\n\n\n\nMore H Facts\n\n\\(SS(\\hat{\\underline\\beta}) = \\hat{\\underline\\beta}^TX^TY = \\hat Y^TY = Y^TH^TY = Y^TH^THY = \\hat Y^T\\hat Y\\)\n\nWe used the facts \\(H^T= H^TH\\) and \\(\\hat Y = HY\\).\n\n\\(\\sum_{i=1}^nV(\\hat Y_i) = trace(H\\sigma^2) = p\\sigma^2\\)\n\n\\(p\\) is the number of parameters.\nProof is part of the assignment\n\n\\(H1 = 1\\) if the model contains a \\(\\beta_0\\) term.\n\n\\(1\\) is a column of 1s, not identity matrix.\nProof on next slide.\n\\(h_{ii} = 1 - \\sum_{j\\ne i}h_{ij}\\)\n\nNote that \\(h_{ij}\\in[-1, 1]\\).\n\n\n\n\n\nProof that \\(H1 = 1\\)\nNote that \\(HX = X\\) (as proven on A1).\n\nThe first column of \\(X\\) is all ones (\\(\\beta_0\\) term).\n\n\\([X]_{i1} = 1\\)\n\nTherefore \\([HX]_{i1}\\) is a column of ones.\n\nEvery row of \\(H\\) times the column of 1s in \\(X\\) results in a column of ones.\n\n\\([HX]_{i1}\\) is every row of \\(H\\) times the first column of \\(X\\).\n\nThe first column of \\(X\\) is 1s, which is equal to the first column of \\(HX\\), which is \\(H\\) times a column of ones.\nIn other words, \\(H1 = 1\\)"
  },
  {
    "objectID": "L10-Hat-Resid_Plots-Cook.html#studentized-residuals",
    "href": "L10-Hat-Resid_Plots-Cook.html#studentized-residuals",
    "title": "9  L10: The Hat Matrix",
    "section": "9.2 Studentized Residuals",
    "text": "9.2 Studentized Residuals\n\nInternally Studentized (not ideal)\nHow do you measure the size a residual?\nDivide by the variance, of course!\nWe know that \\(V(\\hat \\epsilon_i) = (1-h_{ii})\\sigma^2\\), and \\[\ns^2 = \\frac{\\sum_{y=1}^n(y_i-\\hat y_i)^2}{n-p} = \\frac{SSE}{df_E} = MSE\n\\] is an estimate of \\(\\sigma^2\\). Then, \\[\nr_i = \\frac{\\hat\\epsilon_i}{\\sqrt{s^2(1-h_{ii})}}\n\\] is called the internally studentized residual.\n\n\n“Internally” “Studentized”\nNote that \\[\ns^2 = \\frac{\\sum_{i=1}^n(y_i-\\hat y_i)^2}{n-p} = \\frac{\\sum_{i=1}^n(\\hat\\epsilon_i)^2}{n-p}\n\\] and therefore \\[\nr_i = \\frac{\\hat\\epsilon_i}{\\sqrt{s^2(1-h_{ii})}} = \\frac{\\hat\\epsilon_i}{\\sqrt{(\\hat\\epsilon_i^2 + \\sum_{j\\ne i}\\hat\\epsilon_j^2)(1-h_{ii})/(n-p)}}\n\\]\n\nIf \\(\\hat\\epsilon_i\\) is large, then \\(s^2\\) is large.\n\nIf \\(s^2\\) is large, then \\(s_i\\) is small!\n“Internally”: the variance includes the residual of interest.\n\n“Studentized” because Student made it popular.\nOften called “standardized”.\n\n\n\nExternally Studentized Step 1\nLike adding/removing predictors and checking the cahnge in SS, we can add/remove points!\n\nCalculate SS\nRemove the first point. Estimate the model again and calculate new SS.\nAdd the first point back, remove the second. Estimate the model again and check the SS.\n\nFor each point, we have an estimate of the variance without itself.\n\n\nExternally Studentized Step 2\nSkipping the math, \\[\ns^2_{(i)} = \\frac{(n-p)s^2 - \\hat\\epsilon_i^2/(1-h_{ii})}{n-p-1}\n\\] is the variance of the residuals without observation \\(i\\).\n\nThe influence tells us how much a point influenced the model\n\nWe can see what happened without it\nNo need to re-estimate the model!!!\n\n\n\n\nExternally Studentized Residuals\nUse \\(s^2_{(i)}\\) in place of \\(s^2\\). \\[\nt_i = \\frac{\\hat\\epsilon_i}{\\sqrt{s_{(i)}^2(1-h_{ii})}} \\sim t_{n-p-1}\n\\]\n\nFollows a \\(t\\) distribution!\n\nLarger than 2 is suspect, 3 is definitely an outlier!\n\nA large \\(t_i\\) is large relative to the other residuals\nUsually just called “Studentized”\n\nMost software uses Studentized residuals for plots/diagnostics!"
  },
  {
    "objectID": "L10-Hat-Resid_Plots-Cook.html#cooks-distance",
    "href": "L10-Hat-Resid_Plots-Cook.html#cooks-distance",
    "title": "9  L10: The Hat Matrix",
    "section": "9.3 Cook’s Distance",
    "text": "9.3 Cook’s Distance\n\nBetter Measures of Influence\nThe hat matrix is intepreted as influence, but it has problems.\n\n\\(y_i\\)’s influence on it’s own prediction,\n\ngiven all other points.\n\n\\(0 \\le h_{ii} \\le 1\\)\n\nWhat is a “big” influence?\n\nHow do you explain \\(h_{ii}\\) to nonstatisticians?\n\nA better measure is how much the predicted value changes with/without the obs.\n\n\nCooks Distance: Change in \\(\\hat y_i\\).\n\\[\nD_i = \\frac{\\sum_{i=1}^n(\\hat y_i - \\hat y_{(i)})^2}{ps^2}\n\\]\n\n\\(\\hat y_i\\) is the predicted value of \\(y_i\\) when all data are considered.\n\\(\\hat y_{(i)}\\) is the predicted value of \\(y_i\\) when observation \\(i\\) is removed.\n\\(s^2\\) is the MSE of the model with all of the data.\n\\(p\\) is the number of parameters\n\n\\(D_i\\) decreases as \\(p\\) increases!\n\n\nAgain, this would involve re-fitting the model \\(n\\) time (one for each obs).\n\n\nCook’s Distance: Alternate Form\n\\[\nD_i = \\left[\\frac{\\hat \\epsilon_i}{\\sqrt{s^2(1-h_{ii})}}\\right]^2\\left[\\frac{h_{ii}}{1 - h_{ii}}\\right]\\frac{1}{p} = r_i^2\\frac{\\text{variance of $i$th predicted value}}{\\text{variance of $i$th residual}}\\frac{1}{p}\n\\]\n\nAgain, use \\(H\\) rather than re-fitting the model.\nCook’s distance is a modification of the internally studentized residual.\n\nVariances are based on same “deletion” idea as studentized.\n\nRatio of Variances!\n\n\\(F\\) distribution, mean approaches 1 for large values of \\(n\\)\n\nCooks Distance of larger than 1 is suspect.\n\n\n\n\n\nNext Class\nPlots!"
  },
  {
    "objectID": "L10-Hat-Resid_Plots-Cook.html#participation",
    "href": "L10-Hat-Resid_Plots-Cook.html#participation",
    "title": "9  L10: The Hat Matrix",
    "section": "9.4 Participation",
    "text": "9.4 Participation\n\nQ1\nWhich does not describe a residual?\n\nThe observed difference between the measured value \\(y_i\\) and the one we predict with \\(\\hat y_i = X\\hat{\\underline\\beta}\\).\nGiven the true relationship \\(Y = X\\underline\\beta\\), residuals are deviations that cannot be observed.\nErrors that should be fixed.\n\n\n\nQ2\nWhich of the following is not true about the hat matrix (\\(H = X(X^TX)^{-1}X^T\\))?\n\n\\((I-H)(I-H)^T = (I-H)\\)\n\\(\\hat{\\underline\\beta} = (X^TX)^{-1}X^TY = X^{-1}HY\\)\n\\(h_{ii} = 1 - \\sum_{j\\ne i}h_{ij}\\)\n\\(H(I-H) = 0\\)\n\n\n\nQ3\nSuppose that \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\).\nWhich of the following statements is false?\n\n\\(V(\\hat\\epsilon_i) = (1 - h_{ii})\\sigma^2\\)\n\\(V(\\epsilon_i) = \\sigma^2\\)\n\\(V(Y_i) = \\sigma^2\\)\n\\(V(\\hat Y_i) \\ge V(\\hat \\epsilon_i)\\)\n\n\n\nQ4\nThe difference between internally and externally studentized residuals is:\n\n“Internal” uses all of the observations, “external” uses all of the observations except \\(i\\).\n“External” uses all of the observations, “internal” uses all of the observations except \\(i\\).\n\n\n\nQ5\nInternally studentized residuals follow a \\(t\\) distribution.\n\nTrue - they are a normal r.v. divided by a chi-square r.v.\nFalse - they are a normal r.v. divided by a chi-square r.v., but the two are not independent.\nFalse - They include a normal and a chi-square, but the \\(h_{ii}\\) make this not follow distributional assumptions.\nTrue - internally studentized residuals are a small modification to externally studentized residuals, which follow a \\(t\\) distribution.\n\n\n\nQ6\nWhich of the following is not useful for detecting outliers?\n\nStandardized residuals\nStudentized residuals\nCook’s distance\n\\(h_{ii}\\)"
  },
  {
    "objectID": "L11-Admin_Slides.html#participation-questions",
    "href": "L11-Admin_Slides.html#participation-questions",
    "title": "10  L11: The Hat Matrix 2",
    "section": "10.1 Participation Questions",
    "text": "10.1 Participation Questions\n\nQ1\nWhich of these does not give the diagonal of the hat matrix?\n\nhatvalues(mylm)\ndiag(X %*% solve(t(X) %*% X) %*% t(X))\naugment(mylm)$hat\n\n\n\nQ2\nRemoving an outlier will not change:\n\nThe diagonal of the hat matrix.\nThe off-diagonal of the hat matrix.\nSST\nAll of the above will change if we remove an outlier.\n\n\n\nQ3\nA large residual means large influence.\n\nTrue\nFalse\n\n\n\nQ4\nAll entries in the hat matrix are between -1 and 1.\n\nTrue\nFalse\n\n\n\nQ5\nWhich plot corresponds to the model with the most predictors?\n(All models are nested.)\n\n\n\n\n\n\n\nQ6\nIn the output of augment() in the broom package, the .sigma column refers to:\n\nThe MSE of the model.\nThe MSE of the model if it were fit without the observation in that row.\nThe variance of that residual, as calculated by \\(V(\\hat\\epsilon_i) = (1-h_{ii})s^2\\).\nThe variance of that residual, as calculated by \\(V(\\hat\\epsilon_i) = (1-h_{ii})s_{(i)}^2\\)."
  },
  {
    "objectID": "L12-Extra_Topics.html#standardizing-x",
    "href": "L12-Extra_Topics.html#standardizing-x",
    "title": "11  L12: Extra Topics",
    "section": "11.1 Standardizing \\(X\\)",
    "text": "11.1 Standardizing \\(X\\)\n\nMean-Centering\nConsider \\(y_i = \\beta_0 + \\beta_1 x'_i\\), where \\(x'_i\\) are the “centered” versions of \\(x_i\\): \\[\nx'_i = x_i - \\bar x\n\\]\nThen \\(\\bar{x'} = 0\\) and the coefficient estimates become: \\[\\begin{align*}\n\\hat\\beta_0 &= \\bar y - \\hat\\beta_1\\bar{x'} = \\bar y\\\\\n&\\text{and}\\\\\n\\hat\\beta_1 &= \\frac{S_{XX}}{S_{YY}} = \\frac{\\sum_{i=1}^n(x_i' - \\bar{x'})^2}{\\sum_{i=1}^n(x_i' - \\bar{x'})(y_i - \\bar{y})} = \\frac{\\sum_{i=1}^nx_i'^2}{\\sum_{i=1}^nx_i'(y_i - \\bar{y})}\n\\end{align*}\\]\n\n\nMean-Centering and Covariance\n\n\nFor un-centered data: \\[\\begin{align*}\nV(\\hat{\\underline\\beta}) &= (X^TX)^{-1}\\sigma^2,\\\\\n\\text{where }(X^TX)^{-1} &= \\frac{1}{nS_{XX}}\\begin{bmatrix}\\sum x_i^2 & -n\\bar x\\\\-n \\bar x & n\\end{bmatrix}\n\\end{align*}\\] Note also that \\(S_{x'x'} = \\sum_{i=1}^n{x'}_i^2\\), so \\[\\begin{align*}\nV(\\hat{\\underline\\beta}^c) &= \\frac{\\sigma^2}{nS_{X'X'}}\\begin{bmatrix}\\sum {x'}_i^2 & 0\\\\0 & n\\end{bmatrix}\\\\\n& = \\sigma^2\\begin{bmatrix}n^{-1} & 0 \\\\ 0 & (\\sum_{i=1}^n{x'}_i^2)^{-1}\\end{bmatrix}\n\\end{align*}\\]\n\\(\\implies\\) no covariance!!!\n\nSimulations with same data, but one uses centered data (code in L02 Rmd).\n\n\n\n\n\n\n\n\n\nComments on \\(V(\\hat{\\underline\\beta})\\)\n\\[\nV(\\hat{\\underline\\beta}^c) = \\sigma^2\\begin{bmatrix}n^{-1} & 0 \\\\ 0 & (\\sum_{i=1}^n{x'}_i^2)^{-1}\\end{bmatrix}\n\\]\n\n\\(V(\\hat\\beta_0) = \\sigma^2/n \\implies sd(\\hat\\beta_0) = \\sigma/\\sqrt{n}\\).\n\nThe t-test for significance of \\(\\beta_0\\) is just a hypothesis test for \\(\\bar y = 0\\). \n\nNote that \\(\\underline y\\) hasn’t changed, so \\(\\hat{\\underline\\epsilon}\\) and \\(\\sigma^2\\) are unchanged.\n\\(V(\\hat\\beta_0) = \\sigma^2/\\sum_{i=1}^n{x'}_i^2\\) isn’t all that interesting…\n\n\n\nStandardizing \\(\\underline x\\)\nIn addition to mean-centering, divide by the sd of \\(\\underline x\\): \\[\nz_i = \\frac{x_i - \\bar x}{\\sqrt{S_{XX}/(n-1)}}\n\\]\nThen \\(\\bar z = 0\\) and \\(sd(z) = 0 \\implies S_{ZZ} = n-1\\).\nIt can be shown that: \\[\nV(\\hat{\\underline\\beta}^s) = \\sigma^2\\begin{bmatrix}n^{-1} & 0 \\\\ 0 & (n-1)^{-1}\\end{bmatrix}\n\\]\n\n\\(\\underline x\\) doesn’t matter!!!\n\n\n\nStandardizing in Multiple Linear Regression\nSuppose we standardize each column of \\(X\\) (except the first).\nSeveral things happen:\n\nAll predictors are now in units of standard deviations!!!\n\nCoefficients are directly comparable!\n\nCovariances disappear!!!\n\nStandardizing doesn’t hurt and can often help \\(\\implies\\) it’s almost always worth it!"
  },
  {
    "objectID": "L12-Extra_Topics.html#general-linear-hypothesis",
    "href": "L12-Extra_Topics.html#general-linear-hypothesis",
    "title": "11  L12: Extra Topics",
    "section": "11.2 General Linear Hypothesis",
    "text": "11.2 General Linear Hypothesis\n\nDiet vs. Exercise\nWhich is more important for weight loss?\nWe can set this up in a linear regression framework: \\[\n\\text{Loss}_i = \\beta_0 + \\beta_1\\text{CaloriesConsumed}_i + \\beta_2\\text{ExercisesMinutes}_i\n\\] where we assume CaloriesConsumed and ExerciseMinutes are standardized.\nOur question about the importance of diet versus exercise becomes a hypothesis test: \\[\nH_0:\\; \\beta_1 = \\beta_2\\text{ vs. }H_a:\\; \\beta_1 \\ne \\beta_2\n\\] Alternatively, the null can be written as \\(\\beta_1 - \\beta_2 = 0\\).\n\n\nLinearly Independent Hypotheses\nIn some cases, we might have a collection of hypotheses. For ANOVA: \\[\nH_0:\\; \\beta_2 - \\beta_1 = 0,\\; \\beta_3 - \\beta_2 = 0,\\; \\beta_4 - \\beta_3 = 0,\\...,\\; and\\; \\beta_{p-1} - \\beta_{p-2} = 0\n\\] These hypotheses are linearly indepenent. To see why, we can write them in matrix form: \\[\n\\begin{bmatrix}\n0 & -1 & 1 & 0 & 0 & ...\\\\\n0 & 0 & -1 & 1 & 0 & ...\\\\\n0 & 0 & 0 & -1 & 1 & ...\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & ...\\\\\n\\end{bmatrix}\\hat{\\underline\\beta} = \\underline 0\n\\] where none of the rows are linear combinations of the others.\nWe’ll use the notation \\(C\\underline\\beta = \\underline 0\\).\n\n\nLinearly Independent Hypotheses\nThe \\(C\\) matrix can be row-reduced to the hypotheses \\(\\beta_i=0\\;\\forall i&gt;0\\). In this case, our hypothesized model is: \\[\nY_i = \\beta_0 + \\underline\\epsilon\n\\]\nWe have reduced \\(Y = X\\underline\\beta + \\underline\\epsilon\\) to \\(Y = Z\\underline\\alpha + \\underline\\epsilon\\), where \\(\\underline\\alpha = (\\beta_0)\\) and \\(Z\\) is a column of ones.\n\n\nLinearly Dependent Hypotheses\nConsider the model \\(Y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{11}x_1^2 + \\underline\\epsilon\\) and the hypotheses: \\[\nH_0:\\; \\beta_{11} = 0,\\ \\beta_1 - \\beta_2 = 0,\\; \\beta_1 - \\beta_2 + \\beta_3 = 0,\\; 2\\beta_1 - 2\\beta_2 + 3\\beta_3 = 0\n\\] We can write this as: \\[\n\\begin{bmatrix}\n0 & 0 & 0  & 1\\\\\n0 & 1 & -1 & 0\\\\\n0 & 1 & -1 & 1\\\\\n0 & 2 & -2 & 3\n\\end{bmatrix}\\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\\\\\beta_{11}\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\\\0\\\\0\\end{bmatrix}\n\\] With a little work, we can show that this reduces to the model: \\[\nY = \\beta_0 + \\beta(x_1 + x_2) + \\underline\\epsilon \\Leftrightarrow Y = Z\\underline\\alpha + \\underline\\epsilon\n\\]\n\n\nTesting General Linear Hypotheses\nConsider an arbitrary matrix for \\(C\\) (not linearly dependent), such that we can row-reduce \\(C\\) to \\(q\\) linearly independent hypotheses.\n\nFull Model\n\n\\(SS_E = Y^TY - \\hat{\\underline\\beta}^TX^TY\\) on \\(n-p\\) df.\n\nHypothesized Model\n\n\\(SS_W = Y^TY - \\hat{\\underline\\alpha}^TZ^TY\\) on \\(n-p-q\\) df.\n\n\nFrom these, we get: \\[\n\\left(\\frac{SSW-SSE}{q}\\right)/\\left(\\frac{SSE}{n-p}\\right) \\sim F_{q, n-p}\n\\] In other words, we test whether the restrictions significantly change the \\(SS_E\\)!"
  },
  {
    "objectID": "L12-Extra_Topics.html#generalized-least-squares",
    "href": "L12-Extra_Topics.html#generalized-least-squares",
    "title": "11  L12: Extra Topics",
    "section": "11.3 Generalized Least Squares",
    "text": "11.3 Generalized Least Squares\n\nMain Idea\nWhat if the variance of \\(\\epsilon_i\\) isn’t the same for all \\(i\\)?\nIn other words, \\(V(\\underline\\epsilon) = V\\sigma^2\\) for some matrix \\(V\\).\n\nThe structure of \\(V\\) changes how we approach this.\n\nWeighted least squares: \\(V\\) is diagonal.\nGeneralized: \\(V\\) is symmetric and positive-definite, but otherwise arbitrary.\n\n\n\n\nTransforming the Instability Away\nIn the model \\(Y = X\\underline\\beta + \\underline\\epsilon\\), we want \\(V(Y) = I\\sigma^2\\), but we have \\(V(Y) = V\\sigma^2\\)\nSince \\(V\\) is symmetric and positive-definite, we can find a matrix \\(P\\) such that: \\[\nP^TP = PP = P^2 = V\n\\]\nWe can pre-multiply the model by \\(P^{-1}\\) so that \\(V(P^{-1}Y) = V^{-1}V\\sigma^2 = I\\sigma^2\\): \\[\nP^{-1}Y = P^{-1}X\\underline\\beta + P^{-1}\\underline\\epsilon \\Leftrightarrow Z = Q\\underline\\beta + \\underline f\n\\]\n\n\nGeneralized Least Squares Results\n\\[\\begin{align*}\n\\underline f^T\\underline f &= \\underline\\epsilon^TV^{-1}\\underline\\epsilon = (Y - X\\underline\\beta)^TV^{-1}(Y - X\\underline\\beta)\\\\\n\\hat{\\underline\\beta} &= (X^TV^{-1}X)^{-1}X^TV^{-1}Y\\\\\nSS_T &= \\hat{\\underline\\beta}^TQ^TZ = Y^TV^{-1}X(X^TV^{-1}X)^{-1}X^TV^{-1}Y\\\\\nSST &= Z^TZ = Y^TV^{-1}Y\\\\\n\\hat Y &= X\\hat{\\underline\\beta}\\\\\n\\hat{\\underline f} &= P^{-1}(Y-\\hat Y) = P^{-1}(I-X(X^TV^{-1}X)^{-1}X^TV^{-1})Y\n\\end{align*}\\]\nMost things are just switching \\(Y\\) with \\(P^{-1}Y\\), etc., except one…\n\n\nOLS when you should have used GLS\nSuppose the true model has \\(V(\\underline\\epsilon) = V\\sigma^2\\).\nLet \\(\\hat{\\underline\\beta}_O\\) be the estimate of \\(\\underline\\beta\\) if we were to fit with Ordinary Least Squares. Then:\n\n\\(E(\\hat{\\underline\\beta}_O) = \\underline\\beta\\)\n\\(V(\\hat{\\underline\\beta}_O) = (X^TX)^{-1}X^TVX(X^TX)^{-1}\\sigma^2\\)\n\nThe OLS estimate is unbiased, but has a much higher variance!\n\n\nChoosing \\(V\\)\n\nFor serially correlated data, \\(V_{ii} = 1\\) and \\(V_{ij} = \\rho^{|i-j|}\\)\n\nThis is choosing \\(V\\) based on model assumptions!\n\\(\\rho\\) must be estimated ahead of time.\n\nIf we have repeteated \\(x\\)-values, we can use the estimated variance from there.\n\nChoosing \\(V\\) based on the data\n\nIn a controlled experiment, where we have known weights for different \\(x\\)-values\n\nE.g., more skilled surgeons, machine age."
  },
  {
    "objectID": "L12-Extra_Topics.html#participation-questions",
    "href": "L12-Extra_Topics.html#participation-questions",
    "title": "11  L12: Extra Topics",
    "section": "11.4 Participation Questions",
    "text": "11.4 Participation Questions\n\nQ1\nWhich of the following will result in no correlation between \\(\\beta_0\\) and \\(\\beta_1\\)?\n\nCentering\nStandardizing\nBoth centering and standardizing\nNone of the above\n\n\n\nQ2\nWhat’s the primary reason for standardizing the predictors?\n\nRemove correlation between the \\(\\hat\\beta\\)s\nMake it so that the variance is not a function of the \\(X\\)-values.\nEnsure that the values of \\(\\hat\\beta\\) are comparable.\nMake it so that general linear hypotheses are possible.\n\n\n\nQ3\nIn a general linear hypothesis, \\(q\\) is the rank of the \\(C\\) matrix in \\(C\\underline\\beta = \\underline 0\\).\n\nTrue\nFalse\n\n\n\nQ4\nGeneralized Least Squares requires strong assumptions about the matrix \\(V\\).\n\nTrue\nFalse\n\n\n\nQ5\nIgnoring correlation/unequal variance in \\(\\underline\\epsilon\\) will lead to a biased estimate of \\(\\underline\\beta\\)\n\nTrue\nFalse\n\n\n\nQ6\nWhat do you expect the hat matrix to be for GLS?\n\n\\(X(X^TX)^{-1}X^TY\\)\n\\(X(X^TV^{-1}X)^{-1}X^TV^{-1}Y\\)\n\\(XV^{-1}(X^TV^{-1}X)^{-1}X^TV^{-1}Y\\)\n\\(XV^{-T}(X^TV^{-1}X)^{-1}X^TV^{-1}Y\\)"
  },
  {
    "objectID": "L13-Wrong_Model.html#the-wrong-model",
    "href": "L13-Wrong_Model.html#the-wrong-model",
    "title": "12  L13: Getting the Wrong Model",
    "section": "12.1 The Wrong Model",
    "text": "12.1 The Wrong Model\n\nThe Right Model?\nRecall: All models are wrong, some are useful!\nBut how wrong can a model be while still being useful?\n\nThis is an extraordinarily challenging philosophical question.\nWe will touch on a very small part of it\n\n\n\nThe Wrong Predictors\nSo far, we’ve talked about a model of the form \\(Y=X\\underline\\beta + \\underline\\epsilon\\).\n\n\\(E(\\hat{\\underline\\beta}) = E((X^TX)^{-1}X^TY) = (X^TX)^{-1}X^TX\\underline\\beta = \\underline\\beta\\)\n\nHowever, what if we are missing some predictors?\nWhat if the true model is \\(Y=X\\underline\\beta + X_2\\underline\\beta_2 + \\underline\\epsilon\\)? \\[\\begin{align*}\nE(\\hat{\\underline\\beta}) &= E((X^TX)^{-1}X^TY)\\\\\n& = (X^TX)^{-1}X^T(X\\underline\\beta + X_2\\underline\\beta_2) \\\\\n& = (X^TX)^{-1}X^TX\\underline\\beta + (X^TX)^{-1}X^TX_2\\underline\\beta_2 \\\\\n&= \\underline\\beta+ (X^TX)^{-1}X^TX_2\\underline\\beta_2\\\\\n&= \\underline\\beta + A\\underline\\beta_2\n\\end{align*}\\]\n\n\nBias due to wrong predictors\nThe bias of an estimator is: \\[\n\\text{Bias}(\\hat{\\underline\\beta}) = \\underline\\beta - E(\\hat{\\underline\\beta})\n\\]\nFor the case where \\(Y = X\\underline\\beta + X_2\\underline\\beta_2 +\\underline\\epsilon\\), \\[\n\\text{Bias}(\\hat{\\underline\\beta}) = \\underline\\beta - (\\underline\\beta + A\\underline\\beta_2) = A\\underline\\beta_2\n\\]\n\n\nExpected Mean Square\nSee text.\nUses the identity: For an \\(n\\times n\\) matrix \\(Q\\) and \\(n\\times 1\\) random vector \\(Y\\) with variance \\(V(Y)=\\Sigma\\), \\[\nE(Y^TQY) = (E(Y))^TQE(Y) + trace(Q\\Sigma)\n\\]\nThis may be useful for a future assignment question (will notify if you need it), but for now I’m going to explore this via simulation in the Rmd.\n\n\nSummary\n\nChoosing the wrong set of predictors can affect the model!"
  },
  {
    "objectID": "L13-Wrong_Model.html#participation-questions",
    "href": "L13-Wrong_Model.html#participation-questions",
    "title": "12  L13: Getting the Wrong Model",
    "section": "12.2 Participation Questions",
    "text": "12.2 Participation Questions\n\nQ1\nChoosing a model is easy.\n\nTrue\nFalse\n\n\n\nQ2\nWhich statement is false?\n\nIf you have the correct subset of predictors, you will have an unbiased model.\nIf you do not, your model is likely biased.\nIf you’re only interested in the estimate of one predictor, then it’s okay if the other estimates are biased.\nAll of the above are true.\n\n\n\nQ3\nProxy measures of important predictors help remove bias, but the coefficient has no relation to the data generating process.\n\nTrue\nFalse"
  },
  {
    "objectID": "Lb02-OLS_Estimates.html#analysis-of-variance",
    "href": "Lb02-OLS_Estimates.html#analysis-of-variance",
    "title": "13  Lab 2: OLS Estimates",
    "section": "13.1 Analysis of Variance",
    "text": "13.1 Analysis of Variance\nThe code below demonstrates how the ANOVA tables are calculated.\n\ny &lt;- mydata$y\nyhat &lt;- b0 + b1*mydata$x\nybar &lt;- mean(y)\n\nc(sum((yhat - ybar)^2), sum((y - yhat)^2), sum((y - ybar)^2))\n\n[1] 4809.33809   30.93852 4840.27661\n\nANOVA &lt;- data.frame(\n    Source = c(\"Regression\", \"Error\", \"Total (cor.)\"),\n    df = c(1, nrow(mydata) - 2, nrow(mydata)),\n    SS = c(sum((yhat - ybar)^2), sum((y - yhat)^2), sum((y - ybar)^2))\n)\nANOVA$MS &lt;- ANOVA$SS / ANOVA$df\nANOVA\n\n        Source df         SS          MS\n1   Regression  1 4809.33809 4809.338089\n2        Error 28   30.93852    1.104947\n3 Total (cor.) 30 4840.27661  161.342554\n\n\nThis is equivalent to what R’s built-in functions do!\n\nanova(lm(y ~ x, data = mydata))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 4809.3  4809.3  4352.5 &lt; 2.2e-16 ***\nResiduals 28   30.9     1.1                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Lb02-OLS_Estimates.html#dependence-and-centering",
    "href": "Lb02-OLS_Estimates.html#dependence-and-centering",
    "title": "13  Lab 2: OLS Estimates",
    "section": "13.2 Dependence and Centering",
    "text": "13.2 Dependence and Centering\nSomething not touched on in class is that \\(cov(\\hat\\beta_0, \\hat\\beta_1)\\) is not 0! This should be clear from the formula got \\(\\hat\\beta_0\\), which is \\(\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\\).\nThe code below repeats what we did before, but with higher variance to better demonstrate the problem.\nIt also records the estimates based on centering \\(\\underline x\\). Notice how the formula for \\(\\hat\\beta_0\\) is no longer dependent on \\(\\hat\\beta_1\\) if the mean of \\(\\underline x\\) is 0!\n\nb1s &lt;- double(R)\nb0s &lt;- double(R)\nb1cs &lt;- double(R)\nb0cs &lt;- double(R)\nn &lt;- 25\nx &lt;- runif(n, 0, 10)\nxc &lt;- x - mean(x) # centered\n\nfor (i in 1:R) {\n    y &lt;- 2 - 2 * x + rnorm(25, 0, 4)\n    b1 &lt;- Sdotdot(x, y) / Sdotdot(x, x)\n    b0 &lt;- mean(y) - b1 * mean(x)\n    b1s[i] &lt;- b1\n    b0s[i] &lt;- b0\n\n    # Centered\n    y &lt;- 2 - 2 * xc + rnorm(25, 0, 4)\n    b1c &lt;- Sdotdot(xc, y) / Sdotdot(xc, xc)\n    b1cs[i] &lt;- b1c\n    b0c &lt;- mean(y) - b1 * mean(xc)\n    b0cs[i] &lt;- b0c\n}\n\npar(mfrow = c(1,2))\nplot(b1s, b0s)\nplot(b1cs, b0cs)"
  },
  {
    "objectID": "Lb03-MSE.html#best-estimator-of-sigma2",
    "href": "Lb03-MSE.html#best-estimator-of-sigma2",
    "title": "14  Lab 3: Bias/variance of \\(\\sigma^2\\)",
    "section": "14.1 Best estimator of \\(\\sigma^2\\)",
    "text": "14.1 Best estimator of \\(\\sigma^2\\)\nWe saw in the notes that \\(E(s^2) = \\sigma^2\\). Let’s explore why this might not be the best estimator.\nWe define the MSE of an estimator \\(\\theta\\) as \\(E((\\theta - \\hat\\theta^2)^2)\\). For the variance, this is \\(E((\\sigma^2 - s^2)^2)\\).\nLet’s simulate a bunch of linear models, calculate the standard deviations, and then calculate this quantity.\nWe’re going to focus on multiplicative bias, and you’ll see why at the end. This means that we’ll focus on estimators of the form \\(as^2\\).\n\nset.seed(2112)\n\nn &lt;- 30\nx &lt;- runif(n, 0, 10)\nbeta0 &lt;- -3\nbeta1 &lt;- -4\nsigma &lt;- 3\n\nreps &lt;- 1000\nesst &lt;- double(reps)\n\nfor (i in 1:reps) {\n    y &lt;- beta0 + beta1*x + rnorm(n, 0, sigma)\n    beta1 &lt;- sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)\n    beta0 &lt;- mean(y) - beta1 * mean(x)\n    yhat &lt;- beta0 + beta1 * x\n    e &lt;- y - yhat\n    esst[i] &lt;- sum(e^2)\n}\n\na &lt;- (n-4):(n+4)\nmse &lt;- double(length(a))\nbias2 &lt;- double(length(a))\nfor (i in seq_along(a)) {\n    mse[i] &lt;- mean((sigma^2 - esst / a[i])^2)\n    bias2[i] &lt;- (sigma^2 - mean(esst / a[i]))^2\n}\n\npar(mfrow = c(1,2))\nplot(a - n, mse, main = \"MSE = Bias^2 + Variance\")\nplot(a - n, bias2, main = \"Bias^2\")\nabline(h = 0)\n\n\n\n\nFrom these plots, we see that the lowest MSE, i.e. the lowest value of \\(E((\\sigma^2 - as^2)^2)\\), is at \\(a = 1/n\\). Note that this corresponds to the MLE of \\(\\sigma^2\\). However, this is a biased estimate, and the unbiased estimate occurs at \\(a=1/(n-2)\\).\nWhat’s happening here? Shouldn’t unbiased be best? Well, yes, if our criteria is minimizing bias! If we want to minimize \\(E((\\sigma^2 - \\hat\\sigma^2)^2)\\), we have to account for the variance of the estimator across all possible samples as well!\nHWK: Modify the code to show that the bias of the constant model (\\(\\beta_1 = 0\\)) is minimized at \\(n-1\\), with the MSE being minimized at \\(n+1\\). It’s bizarre, but that’s how it works!"
  },
  {
    "objectID": "Lb03-MSE.html#residuals",
    "href": "Lb03-MSE.html#residuals",
    "title": "14  Lab 3: Bias/variance of \\(\\sigma^2\\)",
    "section": "14.2 Residuals",
    "text": "14.2 Residuals\nThe plot.lm() function makes most of the plots you’ll need.\n\nmylm &lt;- lm(mpg ~ disp, data = mtcars)\nplot(mylm, which=1:6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe broom package will be very useful in the future. In particular, the augment() function results in a tidy data frame with columns that are very relevant to our analyses.\n\nlibrary(broom)\n\nhead(augment(mylm))\n\n# A tibble: 6 × 9\n  .rownames           mpg  disp .fitted .resid   .hat .sigma .cooksd .std.resid\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1 Mazda RX4          21     160    23.0  -2.01 0.0418   3.29 0.00865     -0.630\n2 Mazda RX4 Wag      21     160    23.0  -2.01 0.0418   3.29 0.00865     -0.630\n3 Datsun 710         22.8   108    25.1  -2.35 0.0629   3.28 0.0187      -0.746\n4 Hornet 4 Drive     21.4   258    19.0   2.43 0.0328   3.27 0.00983      0.761\n5 Hornet Sportabout  18.7   360    14.8   3.94 0.0663   3.22 0.0558       1.25 \n6 Valiant            18.1   225    20.3  -2.23 0.0313   3.28 0.00782     -0.696\n\n\n\nglance(mylm)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1     0.718        0.709  3.25    76.5 9.38e-10     1  -82.1  170.  175.    317.\n# … with 2 more variables: df.residual &lt;int&gt;, nobs &lt;int&gt;, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\n\nanova(mylm)\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndisp       1 808.89  808.89  76.513 9.38e-10 ***\nResiduals 30 317.16   10.57                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nx &lt;- rnorm(1000); qqnorm(x); qqline(x)"
  },
  {
    "objectID": "Lb04-R_Matrix_Form.html#verifying-matrix-results",
    "href": "Lb04-R_Matrix_Form.html#verifying-matrix-results",
    "title": "15  Lab 4: Verifying Matrix Identities",
    "section": "15.1 Verifying Matrix Results",
    "text": "15.1 Verifying Matrix Results\nWe’ll use the mtcars data for this. Here’s what it looks like:\n\nx &lt;- mtcars$disp\ny &lt;- mtcars$mpg\n\nplot(y ~ x)\nabline(lm(y ~ x))\n\n\n\n\nIt looks like the slope is negative, and the intercept will be somewhere between 25 and 35.\nLet’s use the formulae from the previous course: \\(\\hat\\beta_1 = S_{XY}/S_{XX}\\) and \\(\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\\).\n\nb1 &lt;- sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)\nb0 &lt;- mean(y) - mean(x) * b1\n\nmatrix(c(b0, b1))\n\n            [,1]\n[1,] 29.59985476\n[2,] -0.04121512\n\n\nTo make the matrix multiplication to work, we need \\(X\\) to be a column of 1s and a column representing our covariate.\n\nX &lt;- cbind(1, x)\nhead(X)\n\n         x\n[1,] 1 160\n[2,] 1 160\n[3,] 1 108\n[4,] 1 258\n[5,] 1 360\n[6,] 1 225\n\n\nThe estimates should be \\((X^TX)^{-1}X^T\\underline y\\). In R, we find the transpose with the t() function and we find inverse with the solve() function.\n\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nbeta_hat\n\n         [,1]\n  29.59985476\nx -0.04121512\n\n\nIt works!\nNow let’s check the ANOVA table!\n\nn &lt;- length(x)\ndata.frame(source = c(\"Regression\", \"Error\", \"Total\"),\n    df = c(2, n-2, n),\n    SS = c(t(beta_hat) %*% t(X) %*% y, \n        t(y) %*% y - t(beta_hat) %*% t(X) %*% y,\n        t(y) %*% y)\n)\n\n      source df         SS\n1 Regression  2 13725.1513\n2      Error 30   317.1587\n3      Total 32 14042.3100\n\n\n\nanova(lm(y ~ x))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nx          1 808.89  808.89  76.513 9.38e-10 ***\nResiduals 30 317.16   10.57                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncolSums(anova(lm(y ~ x)))\n\n       Df    Sum Sq   Mean Sq   F value    Pr(&gt;F) \n  31.0000 1126.0472  819.4605        NA        NA \n\n\nThey’re slightly different? Why?\nBecause the equation in the textbook is for the uncorrected sum of squares, which basically means we’re looking at estimating both \\(\\beta_0\\) and \\(\\beta_1\\) at the same time (hence the df of \\(n-2\\)). The usual ANOVA table is the corrected sum of squares, which the textbook labels \\(SS(\\hat\\beta_1|\\hat\\beta_1)\\) to make it clear that it’s estimating \\(\\beta_1\\) only; \\(\\beta_0\\) has already been estimated.\n\nn &lt;- length(x)\ndata.frame(source = c(\"Regression\", \"Error\", \"Total\"),\n    df = c(1, n-1, n),\n    SS = c(t(beta_hat) %*% t(X) %*% y - n * mean(y)^2, \n        t(y) %*% y - t(beta_hat) %*% t(X) %*% y,\n        t(y) %*% y - n * mean(y)^2)\n)\n\n      source df        SS\n1 Regression  1  808.8885\n2      Error 31  317.1587\n3      Total 32 1126.0472\n\n\nThe matrix form for \\(R^2\\) is a little different from what you might expect. It uses this idea of “corrected” sum-of-squares as well. For homework, verify that the corrected sum-of-squares works out to the same formula.\nHere’s how to extract the \\(R^2\\) value from R (note that the programming language R has nothing to do with the \\(R^2\\); R is named after S, which was the programming language that came before it (both chronologically and alphabetically); you’ll still find references to S and S-Plus).\n\nsummary(lm(y ~ x))$r.squared\n\n[1] 0.7183433\n\n\nIn the textbook, the formula is given as: \\[\nR^2 = \\frac{\\hat{\\underline\\beta}^TX^T\\underline y - n\\bar y^2}{\\underline y^t\\underline y - n\\bar y^2}\n\\]\n\nnumerator &lt;- t(beta_hat) %*% t(X) %*% y - n * mean(y)^2\ndenominator &lt;- t(y) %*% y - n * mean(y)^2\nnumerator / denominator\n\n          [,1]\n[1,] 0.7183433"
  },
  {
    "objectID": "Lb04-R_Matrix_Form.html#section",
    "href": "Lb04-R_Matrix_Form.html#section",
    "title": "15  Lab 4: Verifying Matrix Identities",
    "section": "15.2 ",
    "text": "15.2"
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html#basic-anova",
    "href": "Lb05-ANOVA-R2-F_test-CI.html#basic-anova",
    "title": "16  Lab 5: ANOVA",
    "section": "16.1 Basic ANOVA",
    "text": "16.1 Basic ANOVA\n\nset.seed(2221)\nplot(mpg ~ disp, data = mtcars)\nabline(lm(mpg ~ disp, data = mtcars))\n\n\n\n\n\nanova(lm(mpg ~ qsec, data = mtcars))\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nqsec       1 197.39 197.392  6.3767 0.01708 *\nResiduals 30 928.66  30.955                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsummary(lm(mpg ~ qsec, data = mtcars))$coef\n\n             Estimate Std. Error    t value   Pr(&gt;|t|)\n(Intercept) -5.114038 10.0295433 -0.5098974 0.61385436\nqsec         1.412125  0.5592101  2.5252133 0.01708199\n\n\nNotice the p-values! Also notice that the \\(F\\)-value is the square of the \\(t\\)-value! It’s like magic! Math is cool."
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html#r2-always-increases-with-new-predictors",
    "href": "Lb05-ANOVA-R2-F_test-CI.html#r2-always-increases-with-new-predictors",
    "title": "16  Lab 5: ANOVA",
    "section": "16.2 \\(R^2\\) always increases with new predictors",
    "text": "16.2 \\(R^2\\) always increases with new predictors\n\nnx &lt;- 10 # Number of uncorrelated predictors\nuncorr &lt;- matrix(rnorm(50*(nx + 1)), \n    nrow = 50, ncol = nx + 1)\n## First column is y, rest are x\ncolnames(uncorr) &lt;- c(\"y\", paste0(\"x\", 1:nx))\nuncorr &lt;- as.data.frame(uncorr)\n\nrsquares &lt;- NA\nfor (i in 2:(nx + 1)) {\n    rsquares &lt;- c(rsquares,\n        summary(lm(y ~ ., data = uncorr[,1:i]))$r.squared)\n}\nplot(1:10, rsquares[-1], type = \"b\",\n    xlab = \"Number of Uncorrelated Predictors\",\n    ylab = \"R^2 Value\",\n    main = \"R^2 ALWAYS increases\")\n\n\n\n\nThe one exception is when one of the predictors is a linear combination of the previous predictors. In this case, \\(R^2\\) will not change!\n\nuncorr[, nx + 2] &lt;- uncorr[,2] + 3*uncorr[,3]\nrsquares &lt;- c(rsquares, summary(lm(y ~ ., data = uncorr))$r.squared)\nrsquares\n\n [1]          NA 0.003898013 0.056398466 0.056407225 0.069142075 0.073155890\n [7] 0.105824965 0.122449599 0.145307322 0.168746574 0.172758068 0.172758068\n\nplot(rsquares, type = \"b\")\n\n\n\n\n\nx &lt;- runif(20, 0, 10)\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nb0s &lt;- b1s &lt;- double(1000)\nplot(NA, pch = 0, \n    xlim = c(-2, 12), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nabline(h = b0 + b1*mean(x))\nabline(v = mean(x))\nfor (i in 1:1000) {\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    abline(lm(y ~ x), col = rgb(0,0,0,0.1))\n}\n\n\n\n\nLet’s do that again, but record the values and only show the 89% quantiles!\n\nx &lt;- runif(20, 0, 10)\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nall_lines &lt;- replicate(1000, {\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    predict(lm(y ~ x), newdata = list(x = seq(0, 10, 0.1)))\n})\n\neightnine &lt;- apply(all_lines, 1, quantile, probs = c(0.045, 0.945))\n\n\nplot(NA, pch = 0, \n    xlim = c(0, 10), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nlines(seq(0, 10, 0.1), eightnine[1, ])\nlines(seq(0, 10, 0.1), eightnine[2, ])\n\n\n\n\nNote that the theoretical calculation of these bounds is built into R:\n\nplot(NA, pch = 0, \n    xlim = c(0, 10), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nlines(seq(0, 10, 0.1), eightnine[1, ])\nlines(seq(0, 10, 0.1), eightnine[2, ])\n\n## Add the TRUE relationship\nxseq &lt;- seq(0, 10, 0.1)\nlines(xseq, b0 + b1*xseq, col = 3)\n\n## New sample from the data generating process\nx &lt;- runif(20, 0, 10)\ny &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n\n## Extract the CI\nmylm &lt;- lm(y ~ x)\nxbeta &lt;- predict(mylm, interval = \"confidence\",\n    newdata = list(x = xseq))\n#lines(xseq, xbeta[,\"fit\"], col = 4)\nlines(xseq, xbeta[,\"upr\"], col = 4)\nlines(xseq, xbeta[,\"lwr\"], col = 4)\n\n\n\n\nNote that the intervals won’t exactly align - the samples are going to be different each time! In 95% of the samples we collect from this data generating process, the CI we construct from the sample will contain the true (green) line. This is a basic definition for confidence intervals, but it’s neat to see it around a line.\nNotice how the CI is curved. This is completely, 100% expected. Recall that \\((\\bar x, \\bar y)\\) is always a point on the line. If \\(x\\) is the same for all samples, then the variance in the height at \\(\\bar y\\) is just the variance in \\(y\\). However, we can rotate the line around this point and still fit most of the data “pretty well”, which is where the curved nature of the line comes from!\n\nAside\nWhy did I use the same \\(x\\) values for all of the simulations? Because that’s part of the assumptions (this isn’t an important point to make). Again, notice how the point \\((\\bar x, \\bar y)\\) is always on the line, and how the variance at the point \\(\\bar x\\) is minimized. If \\(\\bar x\\) is randomly moved, then there’s extra variance in the line.\n\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nall_lines2 &lt;- replicate(1000, {\n    x &lt;- runif(20, 0, 10)\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    predict(lm(y ~ x), newdata = list(x = seq(0, 10, 0.1)))\n})\n\neightnine2 &lt;- apply(all_lines2, 1, quantile, probs = c(0.045, 0.945))\n\n\nplot(NA, pch = 0, \n    xlim = c(0, 10), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nlines(seq(0, 10, 0.1), eightnine[1, ])\nlines(seq(0, 10, 0.1), eightnine[2, ])\nlines(seq(0, 10, 0.1), eightnine2[1, ], col = 2)\nlines(seq(0, 10, 0.1), eightnine2[2, ], col = 2)\nlegend(\"topleft\", legend = c(\"non-random x\", \"random x\"), col = 1:2, lty = 1)\n\n\n\n\nThe textbook for this course also covers models that incorporate randomness in \\(X\\), but this is not covered in this course."
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html#covariance-of-the-parameters",
    "href": "Lb05-ANOVA-R2-F_test-CI.html#covariance-of-the-parameters",
    "title": "16  Lab 5: ANOVA",
    "section": "16.3 Covariance of the parameters",
    "text": "16.3 Covariance of the parameters\n\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nall_params &lt;- replicate(1000, {\n    x &lt;- runif(20, 0, 10)\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    coef(lm(y ~ x))\n})\n\n\nplot(t(all_params))\n\n\n\n\nIntuition check: were you expecting a negative slope? Does this make sense? If you increase \\(\\beta_0\\), why would \\(\\beta_1\\) decrease?\nFor homework, try a negative intercept and see what happens! What about a negative slope?"
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html#joint-normality-of-the-underlinehatbetas",
    "href": "Lb05-ANOVA-R2-F_test-CI.html#joint-normality-of-the-underlinehatbetas",
    "title": "16  Lab 5: ANOVA",
    "section": "16.4 Joint Normality of the \\(\\underline{\\hat\\beta}\\)s",
    "text": "16.4 Joint Normality of the \\(\\underline{\\hat\\beta}\\)s\nJoint normality leads to marginal normality! This means we can create a confidence interval based on the marginal. However, if the joint distribution has a strong correlation, the marginal confidence intervals might contain unlikely points!\n\npar(mfrow = c(1, 3))\n\n## Marginal distribution of beta_0\nplot(density(all_params[1,]),\n    main = \"Distribution of b_0\",\n    xlab = \"b_0\")\nabline(v = quantile(all_params[1,], c(0.055, 0.945)), col = 2)\nabline(v = 8, col = 3) # Hypothesized beta_0\n\n## Marginal distribution of beta_1\nplot(density(all_params[2,]),\n    main = \"Distribution of b_1\",\n    xlab = \"b_1\")\nabline(v = quantile(all_params[2,], c(0.055, 0.945)), col = 2)\nabline(v = 6, col = 3) # Hypothesized beta_1\n\n## Joint distribution\nplot(t(all_params), main = \"Joint Distribution\",\n    xlab = \"b_0\", ylab = \"b_1\")\nabline(v = quantile(all_params[1,], c(0.055, 0.945)), col = 2)\nabline(h = quantile(all_params[2,], c(0.055, 0.945)), col = 2)\npoints(x = 8, y = 6, col = 3, pch = 16, cex = 1.5)\n\n\n\n\nNotice how the rectangular confidence region in the joint distribution contains regions where there are no points! For example, a hypothesis test for whether \\(\\beta_0=8\\) and \\(\\beta_1 = 6\\) (the green lines/points) would not be rejected if we checked the two confidence intervals separately, but likely should be rejected given the joint distribution! This is exactly what happens when the F-test is significant but none of the t-tests for individual predictors is significant.\nIn general, the CIs for each individual \\(\\hat\\beta\\) are missing something - especially if there’s correlation in the predictors!\nIn these examples, we repeatedly sampled from the true relationship to obtain simulation-based confidence intervals. The normality assumption allows us to make inferences about the distribution of the parameters - including the joint distribution - from a single sample! Inference is powerful!"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#the-hat-matrix",
    "href": "Lb11-R_hat_resids_cook.html#the-hat-matrix",
    "title": "17  Lab 11: Hat Matrix and Residuals in R",
    "section": "17.1 The Hat Matrix",
    "text": "17.1 The Hat Matrix\n\\[\nH = X(X^TX)^{-1}X^T\n\\]\nIn R, we can calculate the diagonal of the hat matrix as follows:\n\nmylm &lt;- lm(mpg ~ disp + wt, data = mtcars)\nhatvalues(mylm) |&gt; unname()\n\n [1] 0.04339369 0.04550894 0.06309504 0.03877647 0.14078260 0.04406584\n [7] 0.11516157 0.09635365 0.09875274 0.11012510 0.11012510 0.08141444\n[13] 0.04168379 0.04521644 0.17206264 0.19889125 0.19275897 0.08015728\n[19] 0.12405357 0.09579747 0.05703451 0.06246825 0.05648077 0.06838477\n[25] 0.14119998 0.08720679 0.07149742 0.16032953 0.18794989 0.05044456\n[31] 0.04474121 0.07408572"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#extracting-the-diagonals-from-h",
    "href": "Lb11-R_hat_resids_cook.html#extracting-the-diagonals-from-h",
    "title": "17  Lab 11: Hat Matrix and Residuals in R",
    "section": "17.2 Extracting the Diagonals from H",
    "text": "17.2 Extracting the Diagonals from H\nThere isn’t a built-in function for the full hat matrix (the diagonals are usually all you’ll need). For demonstration, here are some demonstrations of the features of the hat matrix.\n\nX &lt;- model.matrix(mylm)\nH &lt;- X %*% solve(t(X) %*% X) %*% t(X)\nall.equal(diag(H), hatvalues(mylm))\n\n[1] TRUE"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#features-of-h",
    "href": "Lb11-R_hat_resids_cook.html#features-of-h",
    "title": "17  Lab 11: Hat Matrix and Residuals in R",
    "section": "17.3 Features of H",
    "text": "17.3 Features of H\nIn the code below, I use the unname() function because mtcars has rownames which make the output harder to see (this used to be the norm in R, but it’s fallen out of fashion).\n\ncolSums(H) |&gt; unname()\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\nrowSums(H) |&gt; unname()\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\nrange(H) # -1 &lt;= h_{ij} &lt;= 1\n\n[1] -0.1076609  0.1988913\n\nrange(diag(H)) # 0 &lt;= h_{ii} &lt;= 1\n\n[1] 0.03877647 0.19889125\n\nH %*% rep(1, ncol(H)) # H1 = 1\n\n                    [,1]\nMazda RX4              1\nMazda RX4 Wag          1\nDatsun 710             1\nHornet 4 Drive         1\nHornet Sportabout      1\nValiant                1\nDuster 360             1\nMerc 240D              1\nMerc 230               1\nMerc 280               1\nMerc 280C              1\nMerc 450SE             1\nMerc 450SL             1\nMerc 450SLC            1\nCadillac Fleetwood     1\nLincoln Continental    1\nChrysler Imperial      1\nFiat 128               1\nHonda Civic            1\nToyota Corolla         1\nToyota Corona          1\nDodge Challenger       1\nAMC Javelin            1\nCamaro Z28             1\nPontiac Firebird       1\nFiat X1-9              1\nPorsche 914-2          1\nLotus Europa           1\nFord Pantera L         1\nFerrari Dino           1\nMaserati Bora          1\nVolvo 142E             1"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#extracting-the-residuals",
    "href": "Lb11-R_hat_resids_cook.html#extracting-the-residuals",
    "title": "17  Lab 11: Hat Matrix and Residuals in R",
    "section": "17.4 Extracting the residuals",
    "text": "17.4 Extracting the residuals\nSee ?influence.measures.\n\n?rstandard\ncooks.distance(mylm)\n\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n       1.022220e-02        4.351414e-03        1.721743e-02        5.241995e-03 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n       2.027544e-02        3.089406e-03        3.094888e-02        3.443123e-02 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n       3.775416e-03        8.693734e-03        3.864780e-02        4.425052e-06 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n       1.330376e-04        9.458368e-03        1.920638e-02        3.794816e-02 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n       3.441118e-01        1.429900e-01        3.046404e-02        1.850525e-01 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n       2.372110e-02        1.146745e-02        2.036673e-02        2.070769e-02 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n       1.331763e-01        2.049673e-04        3.812246e-04        4.292901e-02 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n       5.996344e-02        2.547326e-02        1.362489e-02        1.494183e-02"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#the-broom-package",
    "href": "Lb11-R_hat_resids_cook.html#the-broom-package",
    "title": "17  Lab 11: Hat Matrix and Residuals in R",
    "section": "17.5 The broom package",
    "text": "17.5 The broom package\nThe broom package has a wonderful function called augment(). This function sets up our data so that it’s super easy to see what we need.\n\nlibrary(broom)\naugment(mylm)\n\n# A tibble: 32 × 10\n   .rownames        mpg  disp    wt .fitted .resid   .hat .sigma .cooksd .std.…¹\n   &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Mazda RX4       21    160   2.62    23.3 -2.35  0.0434   2.93 0.0102   -0.822\n 2 Mazda RX4 Wag   21    160   2.88    22.5 -1.49  0.0455   2.95 0.00435  -0.523\n 3 Datsun 710      22.8  108   2.32    25.3 -2.47  0.0631   2.93 0.0172   -0.876\n 4 Hornet 4 Drive  21.4  258   3.22    19.6  1.79  0.0388   2.95 0.00524   0.624\n 5 Hornet Sporta…  18.7  360   3.44    17.1  1.65  0.141    2.95 0.0203    0.609\n 6 Valiant         18.1  225   3.46    19.4 -1.28  0.0441   2.96 0.00309  -0.448\n 7 Duster 360      14.3  360   3.57    16.6 -2.32  0.115    2.93 0.0309   -0.845\n 8 Merc 240D       24.4  147.  3.19    21.7  2.73  0.0964   2.92 0.0344    0.984\n 9 Merc 230        22.8  141.  3.15    21.9  0.890 0.0988   2.96 0.00378   0.322\n10 Merc 280        19.2  168.  3.44    20.5 -1.26  0.110    2.96 0.00869  -0.459\n# … with 22 more rows, and abbreviated variable name ¹​.std.resid\n\n\nNotice that it includes:\n\n.fitted = \\(X\\underline{\\hat\\beta}\\)\n.resid = \\(\\hat{\\underline\\epsilon}\\)\n.hat = \\(diag(H)\\)\n.sigma = \\(s_{(i)}\\)\n.cooksd = D\n.std.resid = standardized or studentized residuals?"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#plotting-the-residuals",
    "href": "Lb11-R_hat_resids_cook.html#plotting-the-residuals",
    "title": "17  Lab 11: Hat Matrix and Residuals in R",
    "section": "17.6 Plotting the residuals",
    "text": "17.6 Plotting the residuals\nIf you’ve ever accidentally typed plot(mylm), you’ve seen some plots of the residuals.\n\npar(mfrow = c(2, 2))\nplot(mylm) # ?plot.lm\n\n\n\n\nYou can access individual plots with the which argument.\n\npar(mfrow = c(2, 3))\nplot(mylm, which = 1:6)\n\n\n\n\n99% of the time, the default plots are the ones you’ll want to look at. For teaching purposes, we’ll look at a few extra."
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#demonstrations",
    "href": "Lb11-R_hat_resids_cook.html#demonstrations",
    "title": "17  Lab 11: Hat Matrix and Residuals in R",
    "section": "17.7 Demonstrations",
    "text": "17.7 Demonstrations\nWe’ll use the Ozone data from the mlbench package.\n\nV4 is response (measurement of Ozone)\nV5 is atmospheric pressure\nV6 is wind speed\nV7 is humidity\nWe’ll ignore the rest.\n\n\nlibrary(mlbench)\ndata(Ozone)\nstr(Ozone) # V4 is response (measurement of Ozone)\n\n'data.frame':   366 obs. of  13 variables:\n $ V1 : Factor w/ 12 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ V2 : Factor w/ 31 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ V3 : Factor w/ 7 levels \"1\",\"2\",\"3\",\"4\",..: 4 5 6 7 1 2 3 4 5 6 ...\n $ V4 : num  3 3 3 5 5 6 4 4 6 7 ...\n $ V5 : num  5480 5660 5710 5700 5760 5720 5790 5790 5700 5700 ...\n $ V6 : num  8 6 4 3 3 4 6 3 3 3 ...\n $ V7 : num  20 NA 28 37 51 69 19 25 73 59 ...\n $ V8 : num  NA 38 40 45 54 35 45 55 41 44 ...\n $ V9 : num  NA NA NA NA 45.3 ...\n $ V10: num  5000 NA 2693 590 1450 ...\n $ V11: num  -15 -14 -25 -24 25 15 -33 -28 23 -2 ...\n $ V12: num  30.6 NA 47.7 55 57 ...\n $ V13: num  200 300 250 100 60 60 100 250 120 120 ...\n\nOzone &lt;- Ozone[complete.cases(Ozone), ]\n\nA small amount of exploration first:\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(patchwork)\ng5 &lt;- ggplot(Ozone) +\n    aes(x = V5, y = V4) +\n    geom_point()\ng6 &lt;- ggplot(Ozone) +\n    aes(x = V6, y = V4) +\n    geom_jitter() # deal with overlapping points\ng7 &lt;- ggplot(Ozone) +\n    aes(x = V7, y = V4) +\n    geom_point()\ng5 + g6 + g7\n\n\n\n\nNow let’s check some residuals!\n\nChange .resid to .std.resid.\n\nTry rstudent(olm) as well.\n\nChange .hat to .cooksd.\n\n\nlibrary(dplyr)\nolm &lt;- lm(V4 ~ V5 + V6 + V7, data = Ozone)\naugment(olm) %&gt;%\n    ggplot() +\n        aes(x = .fitted, y = .std.resid, col = .cooksd) +\n        scale_colour_viridis_c(option = 2, end = 0.7) +\n        theme(legend.position = \"bottom\") +\n        geom_point(size = 2) +\n        geom_hline(yintercept = 0, colour = \"grey\")\n\n\n\n\nWhich ones have a large hat value?\nThe plots below are the same as the ones above, but coloured according to the hat values.\n\ng5 &lt;- ggplot(Ozone) +\n    aes(x = V5, y = V4, col = hatvalues(olm)) +\n    geom_point(size = 2)\ng6 &lt;- ggplot(Ozone) +\n    aes(x = V6, y = V4, col = hatvalues(olm)) +\n    geom_jitter(size = 2) # deal with overlapping points\ng7 &lt;- ggplot(Ozone) +\n    aes(x = V7, y = V4, col = hatvalues(olm)) +\n    geom_point(size = 2)\n(g5 + g6 + g7) +\n    plot_layout(guides = \"collect\") &\n    scale_colour_viridis_c(option = 2, end = 0.8) &\n    theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#adding-an-outlier",
    "href": "Lb11-R_hat_resids_cook.html#adding-an-outlier",
    "title": "17  Lab 11: Hat Matrix and Residuals in R",
    "section": "17.8 Adding an Outlier",
    "text": "17.8 Adding an Outlier\nLet’s add an outlier to see what happens with these data.\n\nnewzone &lt;- Ozone[, c(4:7)]\nnewzone &lt;- rbind(newzone,\n    data.frame(V4 = 30, V5 = 5300, V6 = 5, V7 = 40))\nnewlm &lt;- augment(lm(V4 ~ ., data = newzone))\n\ng5 &lt;- ggplot(newlm) +\n    aes(x = V5, y = V4, col = .hat) +\n    geom_point(size = 2)\ng6 &lt;- ggplot(newlm) +\n    aes(x = V6, y = V4, col = .hat) +\n    geom_jitter(size = 2) # deal with overlapping points\ng7 &lt;- ggplot(newlm) +\n    aes(x = V7, y = V4, col = .hat) +\n    geom_point(size = 2)\n(g5 + g6 + g7) +\n    plot_layout(guides = \"collect\") &\n    scale_colour_viridis_c(option = 2, end = 0.8) &\n    theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#using-rs-built-in-diagnostics",
    "href": "Lb11-R_hat_resids_cook.html#using-rs-built-in-diagnostics",
    "title": "17  Lab 11: Hat Matrix and Residuals in R",
    "section": "17.9 Using R’s Built-In Diagnostics",
    "text": "17.9 Using R’s Built-In Diagnostics\n\npar(mfrow = c(2, 2), mar = rep(2, 4))\nplot(lm(V4 ~ ., data = newzone))\n\n\n\nnewzone[row.names(newzone) %in% c(1, 58, 243), ]\n\n    V4   V5 V6 V7\n58  23 5740  3 47\n243 38 5950  5 62\n1   30 5300  5 40\n\n\n\nHuge residual!\n\nThis plot also just has a bad pattern\n\nDeviates from normality!\n\nOtherwise this looks pretty good.\n\nLarge standardized residual\n\nClear pattern without the outlier\n\nCook’s distance is massive compared to the others\n\nPotentially some large \\(D_i\\)’s\n\nIn the corner\n\nOtherwise this looks okay-ish\n\nLast plot also shows it as something different (harder to interpret)"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#what-to-do-with-a-large-residual",
    "href": "Lb11-R_hat_resids_cook.html#what-to-do-with-a-large-residual",
    "title": "17  Lab 11: Hat Matrix and Residuals in R",
    "section": "17.10 What to do with a large residual?",
    "text": "17.10 What to do with a large residual?\n\nMisrecorded: remove or fix, if possible\n\nFires with negative lengths (MDY versus DMY)\nCO2 measured as -99 (code for NA in a system with no NA option)\nHeights measured in the wrong units\n\nReal, but large residual: Consider whether it’s actually part of the population of interest\n\nStudying heights and got a basketball player in your sample? That’s a real data point and your model should allow for it!\nStudying fish and a shark was included? That’s real, but maybe you should narrow your scope!\n\nMany large outliers: you may need to try more predictors or a non-linear model.\n\nDo NOT remove a point simply because it’s an outlier!"
  },
  {
    "objectID": "Lb12-Corr_in_Betas.html",
    "href": "Lb12-Corr_in_Betas.html",
    "title": "18  Lab 12: Extra Topics",
    "section": "",
    "text": "n_sim &lt;- 1000\nn &lt;- 30\nbetas &lt;- matrix(ncol = 3, nrow = n_sim)\nbetacs &lt;- betas\n\nfor (i in 1:n_sim) {\n    x1 &lt;- runif(n, 0, 10)\n    x2 &lt;- runif(n, 0, 10) + 2*x1\n    y &lt;- 3 - 8*x1 + 4*x2 + rnorm(n, 0, 4)\n    betas[i, ] &lt;- coef(lm(y ~ x1 + x2))\n\n    x1c &lt;- scale(x1)\n    x2c &lt;- scale(x2)\n    betacs[i, ] &lt;- coef(lm(y ~ x1c + x2c))\n}\n\npar(mfrow = c(2, 3))\nplot(betas[, c(1,3)])\nplot(betas[, c(1,3)])\nplot(betas[, c(2,3)])\nplot(betacs[, c(1,2)])\nplot(betacs[, c(1,3)])\nplot(betacs[, c(2,3)])"
  },
  {
    "objectID": "Lb13-Wrong_Model.html#missing-predictors",
    "href": "Lb13-Wrong_Model.html#missing-predictors",
    "title": "19  Lab 13: Missing/Extra Predictors",
    "section": "19.1 Missing Predictors",
    "text": "19.1 Missing Predictors\n\nTrue model: \\(Y = X\\underline\\beta + X_2\\underline\\beta_2 + \\underline\\epsilon\\)\nEstimated model: \\(Y = X\\underline\\beta + \\underline\\epsilon\\)\n\nWe’re going to do this a little differently than other days. Let’s look at the penguins data again:\n\nset.seed(2121)\nlibrary(palmerpenguins)\n## Remove NAs and take continuous variables\npenguins &lt;- subset(penguins, species == \"Chinstrap\")\npeng &lt;- penguins[complete.cases(penguins), c(3, 4, 5, 6)]\n## Standardize the x values\n#peng[, 1:3] &lt;- apply(peng[, 1:3], 2, scale)\nhead(peng)\n\n# A tibble: 6 × 4\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1           46.5          17.9               192        3500\n2           50            19.5               196        3900\n3           51.3          19.2               193        3650\n4           45.4          18.7               188        3525\n5           52.7          19.8               197        3725\n6           45.2          17.8               198        3950\n\n\nLet’s “““make”“” a true model:\n\npenglm &lt;- lm(body_mass_g ~ ., data = peng)\nbeta &lt;- coef(penglm)\nsigma &lt;- summary(penglm)$sigma\n\nbeta\n\n      (Intercept)    bill_length_mm     bill_depth_mm flipper_length_mm \n      -3157.53005          16.03916          91.51275          22.57975 \n\nsigma\n\n[1] 276.9998\n\n\nWe’ll use these values as if they are population values and forget that they were calculated from a sample.\n\nThe x-values will stay the same, we’ll simulate new \\(y\\) values according to this model.\nThe advantage of this approach is that the predictors retain any correlation that they had."
  },
  {
    "objectID": "Lb13-Wrong_Model.html#simulating-from-the-right-model",
    "href": "Lb13-Wrong_Model.html#simulating-from-the-right-model",
    "title": "19  Lab 13: Missing/Extra Predictors",
    "section": "19.2 Simulating from the “Right” model",
    "text": "19.2 Simulating from the “Right” model\nLet’s forget the actual values of body_mass_g, and pretend that this is the true relationship: \\[\nbodymass = 4207 + 18*billlength + 35*billdepth + 711.5*flipperlength + \\epsilon\n\\] where \\(\\epsilon_i \\sim N(0, 393)\\).\nWe can simulate from this as follows:\n\nX &lt;- cbind(1, as.matrix(peng[, 1:3]))\nn &lt;- nrow(X)\nbody_mass_g &lt;- X %*% beta + rnorm(n, 0, sigma)\n\nunname(beta)\n\n[1] -3157.53005    16.03916    91.51275    22.57975\n\nunname(coef(lm(body_mass_g ~ -1 + X)))\n\n[1] -3311.31867    16.27829   120.41292    20.77560\n\nunname(coef(lm(body_mass_g ~ X[, -1])))\n\n[1] -3311.31867    16.27829   120.41292    20.77560\n\n\nNow let’s do this 1000s of times!\n\nres &lt;- matrix(ncol = 4, nrow = 0)\nfor (i in 1:10000) {\n    body_mass_g &lt;- X %*% beta + rnorm(n, 0, sigma)\n    right_lm &lt;- lm(body_mass_g ~ -1 + X)\n    res &lt;- rbind(res, unname(coef(right_lm)))\n}\n\ndim(res)\n\n[1] 10000     4\n\n\n\npar(mfrow = c(2, 2))\nfor(i in 1:4) {\n    hist(beta[i] - res[, i], \n        main =paste0(\"Bias = \",round(beta[i], 2), \" - \", \n            round(mean(res[, i]), 2), \" = \", \n            round(beta[i] - mean(res[, i]), 2)))\n    abline(v = 0, col = 2, lwd = 2)\n}\n\n\n\n\nThis looks good- we simulated according to the values in beta, and we were able to recover them. We’ve also shown that the linear model is unbiased!"
  },
  {
    "objectID": "Lb13-Wrong_Model.html#estimating-the-wrong-model---too-few-predictors",
    "href": "Lb13-Wrong_Model.html#estimating-the-wrong-model---too-few-predictors",
    "title": "19  Lab 13: Missing/Extra Predictors",
    "section": "19.3 Estimating the Wrong Model - Too few predictors",
    "text": "19.3 Estimating the Wrong Model - Too few predictors\nIn the following code, I remove the “flipper_length_mm” (the third predictor) by only taking the first three columns of X, which includes the column of 1s.\nI then fit the model without flipper length, which we’ve seen before is an important predictor!\n\nres_reduced &lt;- matrix(ncol = 3, nrow = 0)\nX_reduced &lt;- X[, 1:3] # Still includes column of 1s\nbeta_reduced &lt;- beta[1:3]\nfor (i in 1:10000) {\n    # Simulate from the correct model\n    body_mass_g &lt;- X %*% beta + rnorm(n, 0, sigma)\n    # Only estimate beta 0-3 (not beta4)\n    wrong_lm &lt;- lm(body_mass_g ~ -1 + X_reduced)\n    res_reduced &lt;- rbind(res_reduced, unname(coef(wrong_lm)))\n}\n\ndim(res_reduced)\n\n[1] 10000     3\n\n\n\npar(mfrow = c(2, 2))\nfor(i in 1:3) {\n    bias &lt;- beta[i] - res_reduced[, i]\n    hist(bias, \n        main =paste0(\"Bias = \",round(beta[i], 2), \" - \", \n            round(mean(res_reduced[, i]), 2), \" = \", \n            round(mean(bias), 2)),\n        xlim = range(0, bias))\n    abline(v = 0, col = 2, lwd = 2)\n}\n\n\n\n\nEverything is biased! Since flipper_length_mm was an important predictor, the estimates from the other predictors are biased!\nHere’s how I like to think of this: the machine is trying to learn a pattern using the predictors we give it. These other predictors are trying to pick up on as much pattern as possible. Without the true pattern, they have to adjust.\nA big part of this comes from the fact that there’s correlation in the predictors. Since they’re correlated, if one is missing then the others can find the pattern through their correlation. Instead of flipper_length_mm causing a change in body mass, flipper_length_mm is correlated with bill_length_mm and bill_depth_mm, which then affect body_mass_g in place of flipper_length_m’s affect. In other words, they’re trying to make up for missing patterns through the correlation, like a game of telephone where information has been lost along the way."
  },
  {
    "objectID": "Lb13-Wrong_Model.html#too-many-predictors",
    "href": "Lb13-Wrong_Model.html#too-many-predictors",
    "title": "19  Lab 13: Missing/Extra Predictors",
    "section": "19.4 Too Many Predictors",
    "text": "19.4 Too Many Predictors\nWhat happens if we include predictors that aren’t correlated with the response?\nBefore we run this code, what do you expect?\nRecall our results when \\(Y = X\\underline\\beta + X_2\\underline\\beta_2 + \\underline\\epsilon\\):\n\\[\n\\begin{align*}\nE(\\hat{\\underline\\beta}) &= E((X^TX)^{-1}X^TY)\\\\\n& = (X^TX)^{-1}X^T(X\\underline\\beta + X_2\\underline\\beta_2) \\\\\n& = (X^TX)^{-1}X^TX\\underline\\beta + (X^TX)^{-1}X^TX_2\\underline\\beta_2 \\\\\n&= \\underline\\beta+ (X^TX)^{-1}X^TX_2\\underline\\beta_2\\\\\n&= \\underline\\beta + A\\underline\\beta_2\n\\end{align*}\n\\]\nThis isn’t directly applicable, but might help you think about what happens when \\(\\underline\\beta\\) is too big.\nSince we already have the objects created, let’s pretend that X_reduced is correct.\n\nres &lt;- matrix(ncol = 4, nrow = 0)\nX_reduced &lt;- X[, 1:3] # Still includes column of 1s\nbeta_reduced &lt;- beta[1:3]\nfor (i in 1:10000) {\n    # Simulate from the correct (smaller) model\n    body_mass_g &lt;- X_reduced %*% beta_reduced + rnorm(n, 0, sigma)\n    # Estimate the wrong model\n    wrong_lm &lt;- lm(body_mass_g ~ -1 + X)\n    res &lt;- rbind(res, unname(coef(wrong_lm)))\n\n}\n\npar(mfrow = c(2, 2))\nfor(i in 1:3) {\n    bias &lt;- beta_reduced[i] - res[, i]\n    hist(bias, \n        main =paste0(\"Bias = \",round(beta[i], 2), \" - \", \n            round(mean(res[, i]), 2), \" = \", \n            round(mean(bias), 2)),\n        xlim = range(0, bias))\n    abline(v = 0, col = 2, lwd = 2)\n}\n\n\n\n\nIt’s unbiased! In this case, the estimate of \\(\\beta\\) for flipper_length_mm is 0, and it’s successfully estimating this:\n\nhist(res[, 4])"
  },
  {
    "objectID": "Lb13-Wrong_Model.html#too-many-and-too-few",
    "href": "Lb13-Wrong_Model.html#too-many-and-too-few",
    "title": "19  Lab 13: Missing/Extra Predictors",
    "section": "19.5 Too many and too few",
    "text": "19.5 Too many and too few\nSo let’s get to the final case. As we know, bill length and bill depth are correlated:\n\ncor(X[, -1]) # correlation matrix, without column of 1s\n\n                  bill_length_mm bill_depth_mm flipper_length_mm\nbill_length_mm         1.0000000     0.6535362         0.4716073\nbill_depth_mm          0.6535362     1.0000000         0.5801429\nflipper_length_mm      0.4716073     0.5801429         1.0000000\n\n\nLet’s simulate with the coefficient for bill depth as 0, but include it in the model.\n\nprint(beta)\n\n      (Intercept)    bill_length_mm     bill_depth_mm flipper_length_mm \n      -3157.53005          16.03916          91.51275          22.57975 \n\nprint(head(X))\n\n       bill_length_mm bill_depth_mm flipper_length_mm\n[1,] 1           46.5          17.9               192\n[2,] 1           50.0          19.5               196\n[3,] 1           51.3          19.2               193\n[4,] 1           45.4          18.7               188\n[5,] 1           52.7          19.8               197\n[6,] 1           45.2          17.8               198\n\n\nTo be clear:\n\nData Generating Process: body mass = \\(\\beta_0\\) + \\(\\beta_1\\) bill length + \\(\\beta_3\\) flipper length\nEstimating: body mass = \\(\\beta_0\\) + \\(\\beta_2\\) bill depth + \\(\\beta_3\\) flipper length\n\n\nres &lt;- matrix(ncol = 3, nrow = 0)\nbeta_fewmany &lt;- beta\nbeta_fewmany[3] &lt;- 0 # True coefficient for depth is 0, length != 0\nX_fewmany &lt;- X[, c(1, 3, 4)] # estimating depth, not length\n\nfor (i in 1:10000) {\n    # Simulate from the correct (smaller) model\n    body_mass_g &lt;- X %*% beta_fewmany + rnorm(n, 0, sigma)\n    # Estimate the wrong model\n    wrong_lm &lt;- lm(body_mass_g ~ -1 + X_fewmany)\n    res &lt;- rbind(res, unname(coef(wrong_lm)))\n\n}\n\npar(mfrow = c(2, 2))\nhist(res[, 1],\n    main = paste0(\"bias=\", round(beta[1], 2),\n        \"-\", round(mean(res[, 1]), 2),\n        \"=\", round(beta[1] - mean(res[, 1]), 2)))\nabline(v = beta[1], col = 2, lwd = 2)\n\nhist(res[, 2])\nabline(v = 0, col = 2, lwd = 2)\n\nhist(res[, 3],\n    main = paste0(\"bias=\", round(beta[4], 2),\n        \"-\", round(mean(res[, 3]), 2),\n        \"=\", round(beta[4] - mean(res[, 3]), 2)))\nabline(v = beta[4], col = 2, lwd = 2)\n\n\n\n\n\nIt looks like flipper length is unbiased\n\nTechnically, it isn’t, but in this case it’s a small bias.\nIf we were primarily interested in flipper length, misspecifying bill length/depth isn’t so bad.\n\nThe estimate of bill_depth isn’t 0, but also doesn’t correspond to anything in the DGP!\n\nIt’s called a “proxy measure”, and the coefficient must be interpreted carefully."
  },
  {
    "objectID": "Lb13-Wrong_Model.html#summary",
    "href": "Lb13-Wrong_Model.html#summary",
    "title": "19  Lab 13: Missing/Extra Predictors",
    "section": "19.6 Summary",
    "text": "19.6 Summary\nChoosing the right subset of predictors can be HARD!\n\nMissing predictors means your estimates are biased\nToo many predictors isn’t as bad of an issue\n\nOverfitting!\n\nThe wrong subset means no relation to DGP\n\nCan still give (nearly) unbiased estimates for predictors of interest."
  }
]