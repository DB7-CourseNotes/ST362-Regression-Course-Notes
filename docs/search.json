[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Analysis",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "Regression Analysis",
    "section": "About This Course",
    "text": "About This Course\nThis book contains the course notes for the Spring 2023 offering of ST362 Regression Analysis, based on the following sources:\n\nApplied Regression Analysis, 3rd edition, by Draper and Smith\n\nA PDF of this textbook is available through the WLU library\n\nIntroduction to Linear Regression Anaysis, 2nd edition, by Montgomery and Peck\n\nThis textbook is excellent but expensive, and I am striving to use free and OER materials.\n\nThe online course notes from Stat 501 at Penn State."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Regression Analysis",
    "section": "About This Book",
    "text": "About This Book\nSome features:\n\nThe GitHub logo takes you to the repo for this book. Feel free to fork and adapt as you please (under the CC BY-SA 4.0 license).\nThe little toggle next to the logo puts this into night mode. Try it out!\nEach lecture had a “Jam”, where I played music at the start of class that related to a particular slide. When that slide showed up, a student would say “That’s my Jam!” and I would throw chocolate at them.\n\nThe jams are still there, and you may wish to listen to them while reading the slides!\n\n\nThis book is very much a work in progress. There are missing sections and typos in the slides. I am working on adding speaker notes to the slides, which will show up as text in this book.\nI am also working on a major re-write of the first few chapters to walk through the concepts in a better order. In particular, I would like to stay in simple linear regression for a lot longer, demonstrating correlation, Cook’s distance, correlation between \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\), etc., then moving into binary and categorical predictors as a first step into multiple regression, polynomial as a second step, then a lecture demonstrating that all of these concepts generalize into multiple dimensions.\nThis is a quarto book based on my lecture slides. The “Lectures” are quarto files that were rendered into beamer PDF slides. I have included the configs to render the slides. To re-create my slides, you can use the code:\nquarto render L01-Introduction.qmd --profile slides\nThe --profile argument tells Quarto to use the configuration in the file _quarto-slides.yml. I will add speaker notes in a notes environment, which means that the notes will appear in the book but not the pdf slides.\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 Unported License."
  },
  {
    "objectID": "L01-Introduction.html#introduction",
    "href": "L01-Introduction.html#introduction",
    "title": "1  L01: Distributions",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\n\nToday’s Learning Outcomes\n\nImportant facts about distributions.\nCIs and t-tests."
  },
  {
    "objectID": "L01-Introduction.html#distributions",
    "href": "L01-Introduction.html#distributions",
    "title": "1  L01: Distributions",
    "section": "1.2 Distributions",
    "text": "1.2 Distributions\n\nNormal\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{(x-\\mu)^2}{-2\\sigma^2}\\right);\\;-\\infty&lt;x&lt;\\infty\n\\]\nShiny: https://shiny.abdn.ac.uk/Stats/apps/app_normal/#section-theory\n\nCompletely determined by \\(\\mu\\) and \\(\\sigma\\).\nIf \\(X \\sim N(\\mu,\\sigma^2)\\), then \\(Z=\\frac{X-\\mu}{\\sigma} \\sim N(0,1)\\).\n\nOften called “standardizing”\n\nYou won’t need to memorize the pdf, but it’s useful!\n\n\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pnorm\")\n\nThe normal distribution is the foundation for pretty much everything that we’re going to do in this class. In general, things aren’t normally distributed, but the assumption is very robust and works out in a lot of cases.\nIn this lecture we’re going to build up an important result that we’ll use frequently. In particular, we want some background into why the F and \\(t\\) distributions show up so often!\n\n\n\nBut first, Gamma!\n\n\n\\(t\\) is based on the Gamma (\\(\\Gamma\\)) function:\n\\[\n\\Gamma(q) = \\int_0^\\infty e^{-t}t^{q-1}dt\n\\]\n\nInteresting property: \\(\\Gamma(k+1) = k!\\) for integer \\(k\\).\n\nIn general, \\(\\Gamma(q) = (q-1)\\Gamma(q-1)\\)\n\nAlso, \\(\\Gamma(1/2) = \\pi^{1/2}\\)\n\n\\(\\Gamma(3/2) = (3/2-1)\\gamma(1/2) = \\sqrt{\\pi}/2\\)\n\n\n\n\n\n\n\n\nThe \\(t\\) Distribution\n\\[\nf(x; \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu + 1}{2}\\right)}{\\sqrt{\\nu\\pi}\\Gamma(\\nu/2)}\\left(1 + \\frac{x^2}{\\nu}\\right)^{-(\\nu + 1)/2}\n\\]\n\nThe notation \\(f(x; \\nu)\\) means “function of \\(x\\), given a value of \\(\\nu\\).”\nCompletely determined by \\(\\nu\\)\nAs \\(\\nu\\rightarrow\\infty\\), this becomes \\(N(0,1)\\).\n\n\\(\\nu&gt;30\\) is pretty much normal already.\nFor \\(\\nu&lt;\\infty\\), \\(t\\) has wider tails than normal.\n\n\n\n\nThe \\(\\chi^2\\) distribution - variances\nIf \\(Z_1, Z_2, ..., Z_k\\) are iid \\(N(0,1)\\), then \\(\\chi^2_k = \\sum_{i=1}^kZ_i^2\\) has a chi-square distribution on \\(k\\) degrees of freedom.\n\\[\nf(x; k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2}\n\\]\n\nIf \\(X_i\\sim N(\\mu_i\\sigma_i)\\), then we can just standardize each first.\nAs \\(k\\rightarrow\\infty\\), \\((\\chi^2_k-k)/\\sqrt{2k}\\stackrel{d}{\\rightarrow} N(0,1)\\).\n\nThat is, for large \\(k\\) this can be approximated by a normal distribution.\n\nVery related to the variance\n\n\\((n-1)s^2/\\sigma^2\\) follows a \\(\\chi^2_n\\) distribution.\n\n\n\n\nThe \\(F\\) distribution - ratio of variances\nIf \\(S_1\\) and \\(S_2\\) are independent \\(\\chi^2\\) distributions with degrees of freedom \\(\\nu_1\\) and \\(\\nu_2\\), then \\(\\frac{S_1/\\nu_1}{S_2/\\nu_2}\\) follows an \\(F_{\\nu_1,\\nu_2}\\) distribution.\n\\[\nf(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}\n\\]\n\nIf \\(s_1^2\\) and \\(s_2^2\\) are the sample-based estimates of the true values \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\), then \\(\\frac{s_1^2/\\sigma_1^2}{s_2^2/\\sigma_2^2}\\) follows an \\(F_{\\nu_1, \\nu_2}\\) distribution.\n\nUnder \\(H_0\\), \\(\\sigma_1=\\sigma_2\\), so we don’t need the true values.\nNote: \\(s_1^2\\) is calculated from \\(S_1^2/\\nu_1\\)."
  },
  {
    "objectID": "L01-Introduction.html#confidence-intervals",
    "href": "L01-Introduction.html#confidence-intervals",
    "title": "1  L01: Distributions",
    "section": "1.3 Confidence Intervals",
    "text": "1.3 Confidence Intervals\n\nGeneral Idea: Terminology\nConsider the statistic \\(t = \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)}\\) (a statistic is any number that can be calculated from data alone).\n\n\\(\\theta\\) is the “““true”“” value of the parameter.\n\nUnknown, unknowable, not used in formula - hypothesize \\(\\theta = \\theta_0\\) instead.\n\n\\(\\hat\\theta\\) is our estimator of \\(\\theta\\).\n\nEstimator is a function, like \\(\\hat\\mu = \\bar X =(1/n)\\sum_{i=1}^nX_i\\).\n\n\\(X\\) is a random variable.\n\nEstimate is a number, like the calculated mean of a sample.\n\n\\(se(\\hat\\theta)\\) is the standard error of \\(\\hat\\theta\\).\n\nIf we had a different sample, the value of \\(\\hat\\theta\\) would be different. It has variance.\nStandard error: the standard deviation of the sampling distribution.\n\n\n\n\nGeneral Idea: Distributional Assumptions\nConsider the quantity (not statistic) \\(t = \\frac{\\hat\\theta - \\theta}{se(\\hat\\theta)}\\).\nIf we assume \\(X\\sim N(\\mu, \\sigma^2)\\) and \\(\\hat\\theta = \\bar X\\) is an unbiased estimate of \\(\\theta\\) on \\(\\nu=n-1\\) degrees of freedom, then \\(t \\sim t_\\nu\\)\nFrom this, we can find the lower and upper \\(\\alpha/2\\)% of the \\(t\\)$ curve.\n\n\\(t_\\nu(\\alpha/2)\\) is the lower \\(\\alpha/2\\)%.\n\ni.e., if \\(\\alpha = 0.11\\), then it’s \\(t(\\nu, 0.055)\\), the lower 5.5% area.\n\n\\(t_\\nu(1 - \\alpha/2)\\) is the upper \\(\\alpha/2\\)%.\nSince \\(t\\) is symmetric, \\(t_\\nu(1 - \\alpha/2) = -t_\\nu(\\alpha/2)\\).\n\n\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pvalues\")\n\n\n\n\nGeneral Idea: Distribution to Quantiles\nUnder the null hypothesis, \\(\\theta = \\theta_0\\), so \\[\nt = \\frac{\\hat\\theta - \\theta}{se(\\hat\\theta)} = \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)} \\sim t_\\nu\n\\]\nWe know that \\(\\bar X\\) is a random variable, and we want everything in between its 5.5% and 94.5% quantiles. \n\nThis is a confidence interval!\n\n\n\nGeneral Idea: Distribution to CI\nWe can do this easily for the \\(t_\\nu\\) distribution: we want all values \\(t_0\\) such that\n\\[\\begin{align*}\nt_\\nu(5.5) &\\le t_0 \\le t_\\nu(94.5)\\\\\nt_\\nu(5.5) &\\le \\frac{\\hat\\theta - \\theta_0}{se(\\hat\\theta)} \\le t_\\nu(94.5)\\\\\nse(\\hat\\theta)t_\\nu(5.5) &\\le \\hat\\theta - \\theta_0 \\le se(\\hat\\theta)t_\\nu(94.5)\\\\\n\\hat\\theta + se(\\hat\\theta)t_\\nu(5.5) &\\le \\theta_0 \\le \\hat\\theta + se(\\hat\\theta)t_\\nu(94.5)\n\\end{align*}\\]\nThe CI is all values \\(\\theta_0\\) that would not be rejected by the null hypothesis \\(\\theta = \\theta_0\\) at the \\(\\alpha\\)% level.\nSince \\(t_\\nu(1 - \\alpha/2) = -t_\\nu(\\alpha/2)\\), our 89% CI is \\(\\hat\\theta \\pm se(\\hat\\theta)t_\\nu(5.5)\\).\n\n\nWhat is the Standard Error?\nIf we’re estimating the mean, \\(\\hat\\theta = (1/n)\\sum_{i=1}^nX_i\\), where we assume \\(X_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma)\\). \\[\nE(\\hat\\theta) = E\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n}\\sum_{i=1}^nE(X_i) = \\frac{n\\mu}{n} = \\mu\n\\] From this, we see that the mean is an unbiased estimator! (This is nice, but not required.)\n\\[\nV(\\hat\\theta) = V\\left(\\frac{1}{n}\\sum_{i=1}^nX_i\\right) = \\frac{1}{n^2}V\\left(\\sum_{i=1}^nX_i\\right) \\stackrel{indep}{=} \\frac{1}{n^2}\\sum_{i=1}^nV(X_i) = \\sigma^2/n \\stackrel{plug-in}{=} s^2/n\n\\] where \\(s^2\\) is the estimate variance since we cannot know the true mean. Note that \\(s^2\\) is a biased estimator for \\(\\sigma\\).\n\n\nCI for Variance\nFrom before: \\(\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_n\\).\nLet \\(\\chi^2_n(0.055)\\) be the lower 5.5% quantile, \\(\\chi^2_n(0.945)\\) be the upper.\nFor homework, find the CI from: \\[\n\\chi^2_n(0.055) \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi^2_n(0.945)\n\\] Note that \\(\\chi^2_n(0.055)\\ne-\\chi^2_n(0.945)\\).\n\n\nSummary\n\nDistributions exist and are important\n\nMost things will be normal, which leads to \\(t\\), \\(\\chi^2\\), and \\(F\\).\n\nCIs are all values that would not be rejected by a hypothesis test.\n\nThe null hypothesis determines the distribution of the test statistic, which allows us to find the CI.\n\n\nFor Next Class:\n\nRead through Ch01, especially procedure for least squares estimation.\n\n&lt;—"
  },
  {
    "objectID": "L01-Introduction.html#participation-questions",
    "href": "L01-Introduction.html#participation-questions",
    "title": "1  L01: Distributions",
    "section": "1.4 Participation Questions",
    "text": "1.4 Participation Questions\n\nQ1\nWhich of the following is a Normal distribution?\n\n\\(f(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}\\)\n\\(f(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-x^2/2\\right)\\)\n\\(f(x; \\nu_1, \\nu_2) = \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right)\\left(\\frac{\\nu_1}{\\nu_2}\\right)^{\\nu_1/2}}{\\Gamma(\\nu_1/2)\\Gamma(\\nu_2/2)}\\frac{x^{\\nu_1/2-1}}{(1 + x\\nu_1/\\nu_2)^{(\\nu_1+\\nu_2)/2}}\\)\n\\(f(x; k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-x/2}\\)\n\n\n\nQ2\nIf you know \\(\\mu\\) and \\(\\sigma\\), then you know the exact shape of the normal distribution.\n\nTrue\nFalse\n\n\n\nQ3\nA confidence interval for \\(\\theta\\) contains all values \\(\\theta_0\\) that would not be rejected by a hypothesis test (assume that both are at the same significance level).\n\nTrue\nFalse\n\n\n\nQ4\nWhich of the following is the correct value for \\(E(\\hat\\theta)\\), where \\(\\hat\\theta = \\sum_{i=1}^n\\left(a + bX_i\\right)\\) and \\(E(X_i)=\\mu\\) for all \\(i\\)?\n\n\n\\(a + b\\mu\\)\n\\(b\\mu\\)\n\\(na + nb\\mu\\)\n\n\n\nQ5\nWhich of the following is the definition of an estimator?\n\nA value calculated from data.\nA function that returns the estimate for a parameter.\nAny function of the data.\nA person who estimates.\n\n\n\nQ6\nThe general approach to finding confidence intervals is to find a function of the statistic and the parameter it’s estimating that follows a known distribution and then solve for the unknown parameter.\n\n\nFalse\nTrue\n\n\n—&gt;"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#why-fit-models",
    "href": "L02-Fitting_Straight_Lines.html#why-fit-models",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.1 Why Fit Models?",
    "text": "2.1 Why Fit Models?\n\nThe Quote.\n“All models are wrong, some are useful.” - George Box\n\n\nAll Models Are Wrong, But Some Are Useful\nModel: A mathematical equation that explains something about the world.\n\nGravity: 9.8 \\(m/s^2\\) right?\n\nThis is a model - it’s a relationship that explains something in the world.\nVaries across the surface of the Earth.\nVaries according to air resistance.\n\nEvery additional cigarette decreases your lifespan by 11 minutes.\n\nVery, very much wrong.\nVery, very useful for communication.\n\n\n\n\nAll Linear Models are Wrong\n\n\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(palmerpenguins)\npenguins &lt;- penguins[complete.cases(penguins),]\n\nggplot(penguins) +\n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(title = \"A line fits reasonably well\",\n        x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\")\n\n\n\n\n\n\nggplot(mtcars) +\n    aes(x = disp, y = mpg) + \n    geom_point(size = 2) + \n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) + \n    geom_smooth(method = \"lm\", se = FALSE, formula = \"y~poly(x, 2)\", colour = 2) +\n    labs(title = \"A straight line misses the pattern\",\n        subtitle = \"A polynomial might fit better\",\n        x = \"Engine Displacement (1000 cu.in)\", \n        y = \"Fuel Efficiency (mpg)\")\n\n\n\n\n\n\n\n\nSome Linear Models are Useful\n\nxpred &lt;- 200\nypred &lt;- predict(\n    lm(body_mass_g ~ flipper_length_mm, data = penguins),\n        newdata = data.frame(flipper_length_mm = xpred))\n\nggplot(penguins) +\n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(title = \"The height of the line is an okay prediction for Body Mass\",\n        x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\") +\n    annotate(geom = \"point\", shape = \"*\", size = 30, colour = 2,\n        x = xpred, \n        y = ypred) + \n    annotate(geom = \"segment\", colour = 2, size = 2, \n        linetype = \"dashed\",\n        x = xpred, xend = xpred, \n        yend = -Inf, y = ypred) + \n    annotate(geom = \"segment\", colour = 2, size = 2, \n        linetype = \"dashed\",\n        x = -Inf, xend = xpred, \n        yend = ypred, y = ypred)\n\n\n\n\n\n\nA Model is an Equation\n\\[\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\Leftrightarrow Y = X\\underline\\beta + \\underline\\epsilon \\Leftrightarrow E(Y|X) = X\\underline \\beta;\\; V(Y|X)=\\sigma^2\n\\]\n\nFor a one unit increase in \\(x_i\\), \\(y_i\\) increases by \\(\\beta_1\\).\n\nIf our model fits well and this is statistically significant, we can apply this to the population.\n\nThe estimated value of \\(y_i\\) when \\(x_i=0\\) is \\(\\beta_0\\).\n\nNot always interesting, but usually\\(^*\\) necessary.\n\n\nOur prediction for \\(y_i\\) at any given value of \\(x_i\\) is calculated as: \\[\n\\hat y_i = \\hat\\beta_0 + \\hat \\beta_1x_i\n\\] where the “hats” mean we’ve found estimates for the \\(\\beta\\) values.\n\n\nAside: My notation differs from the text\nI will make mistakes, but in general:\n\n\\(Y\\) is a random variable (usually a vector) representing the response.\n\n\\(Y_i\\) is also a random variable (not a vector)\n\n\\(\\underline y\\) is the response vector; the observed values of \\(Y\\)\n\\(\\underline x\\) is the vector of covariates in simple linear regression\n\\(X\\) is a matrix of covariates\n\nNot a random variable!!! Capital letters could be either random or matrix (or both). I will specify when necessary.\nI will avoid the notation \\(X_i\\).\nWhen necessary, the first column is all ones so that \\(X\\underline\\beta=\\beta_0 + \\beta_1\\underline x_1 + \\beta_1\\underline x_2 + ...\\).\n\nContext should make this clear.\n\n\n\n\n\nThe Mean of \\(Y\\) at any value of \\(X\\)\n\\[\nE(Y|X) = X\\underline\\beta\n\\]\n\n\n\nA Linear Model is Linear in the Parameters\nThe following are linear:\n\n\\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1\\sin\\left(x_i^{x_i}\\right) + \\beta_2x_i^{\\sin(x_i)} + \\epsilon_i\\)\n\nThe following are not linear:\n\n\\(y_i = \\beta_0 + \\beta_1^2x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_0\\beta_1x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\sin(\\beta_1)x_i +\\epsilon_i\\)"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#estimation",
    "href": "L02-Fitting_Straight_Lines.html#estimation",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.2 Estimation",
    "text": "2.2 Estimation\n\nGoal: Find \\(\\beta\\) values which minimize the error\nModel: \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\)\nError: \\(\\hat\\epsilon_i = y_i - \\hat y_i\\)\nWhy is this a bad way to do it?\n\n\nLeast Squares\nGoal: Minimize \\(\\sum_{i=1}^n\\hat\\epsilon_i^2=\\sum_{i=1}^n(y_i - \\hat y_i)^2\\), the sum of squared errors.\nThis ensures errors don’t cancel out. It also penalizes large errors more.\nCould we have used \\(|\\epsilon_i|\\), or some other function? \\(|ly|\\)!\n\n\nLeast Squares Estimates\nI assume that you can do the Least Squares estimate in your sleep (be ready for tests).\nThe final results are: \\[\\begin{align*}\n\\hat\\beta_0 &= \\bar y - \\hat\\beta_1\\bar x\\\\\n\\hat\\beta_1 &= \\frac{\\sum_{i-1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i-1}^n(x_i - \\bar x)^2}\\stackrel{def}{=}\\frac{S_{XY}}{S_{XX}}\n\\end{align*}\\]\nThe textbook was written in 1998 and gives formulas to make the calculation easier to do on pocket calculators. You will not need a pocket calculator for this course.\n\n\nLet’s Add Assumptions\nAssumptions allow great things, but only when they’re correct!\n\n\\(E(\\epsilon_i) = 0\\), \\(V(\\epsilon_i) = \\sigma^2\\).\n\\(cov(\\epsilon_i, \\epsilon_j) =0\\) when \\(i\\ne j\\).\n\nImplies that \\(E(y_i) = \\beta_0 + \\beta_1x_i\\) and \\(V(y_i) = \\sigma^2\\).\n\n\\(\\epsilon_i\\stackrel{iid}{\\sim} N(0,\\sigma^2)\\)\n\nThis is a strong assumption, but often works!\n\n\nInterpretation: the model looks like a line with completely random errors. (“Completely random” doesn’t mean “without structure”!)\n\n\nMean of \\(\\hat\\beta_1\\)\nIt is easy to show that \\[\n\\hat\\beta_1 = \\frac{\\sum_{i-1}^n(x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i-1}^n(x_i - \\bar x)^2} = \\frac{\\sum_{i-1}^n(x_i - \\bar x)y_i}{\\sum_{i-1}^n(x_i - \\bar x)^2}\n\\] since \\(\\sum(x_i - \\bar x) = 0\\).\nHomework: show that \\(E(\\hat\\beta_1) = \\beta_1\\) (Hint: use the fact that \\(E(y_i) = \\beta_0 + \\beta_1x_i\\) and the “pocket calculator” expansions).\n\n\nVariance of \\(\\hat\\beta_1\\)\n\\[\n\\hat\\beta_1 = \\frac{\\sum_{i-1}^n(x_i - \\bar x)y_i}{\\sum_{i-1}^n(x_i - \\bar x)^2}\n\\] can be re-written as \\[\n\\hat\\beta_1 = \\sum_{i=1}^na_iy_i\\text{, where }a_i = \\frac{x_i - \\bar x}{\\sum_{i=1}^n(x_i - \\bar x)^2}\n\\]\nThus the variance of \\(\\hat\\beta_1\\) is \\[\nV(\\hat\\beta_1) = \\sum_{i=1}^na_i^2V(y_i) = ... = \\frac{\\sigma^2}{S_{XX}} \\stackrel{plug-in}{=} = \\frac{s^2}{S_{XX}}\n\\]\n\n\nConfidence Interval and Test Statistic for \\(\\hat\\beta_1\\)\nThe test statistic can be found as: \\[\nt = \\frac{\\hat\\beta_1 - \\beta_1}{se(\\hat\\beta_1)} = \\frac{(\\hat\\beta_1 - \\beta_1)\\sqrt{S_{XX}}}{s} \\sim t_\\nu\n\\]\nSince this follows a \\(t\\) distribution, we can get the CI: \\[\n\\hat\\beta_1 \\pm t_\\nu(\\alpha/2)\\sqrt{\\frac{s^2}{S_{XX}}}\n\\]"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#participation-questions",
    "href": "L02-Fitting_Straight_Lines.html#participation-questions",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.3 Participation Questions",
    "text": "2.3 Participation Questions\n\nQ1\nAll models are useful, but some are wrong.\n\nTrue\nFalse\n\n\n\nQ2\nFor a one unit increase in \\(x\\), \\(y\\) increases by\n\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(\\beta_1x\\)\n\\(\\beta_0 + \\beta_1x\\)\n\n\n\nQ3\nThe standard error of \\(\\beta_1\\) refers to:\n\nThe variance of the population slope\nThe amount by which the slope might be off\nThe variance of the estimated slope across different samples\nA big chicken. Not, like, worryingly big, but big enough that you’d be like, “Wow, that’s a big chicken!”\n\n\n\nQ4\nWhich is not an assumption that we usually make in linear regression?\n\n\\(V(\\epsilon_i) = \\sigma^2\\)\n\\(E(\\epsilon_i) = 0\\)\n\\(E(X) = 0\\)\n\\(\\epsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\)\n\n\n\nQ5\nWhich of the following is not a linear model?\n\n\\(y_i = \\beta_0 + \\beta_1^2x_i + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_0x_i +\\epsilon_i\\)\n\\(y_i = \\beta_0 + \\beta_1\\sin\\left(x_i^{x_i}\\right) + \\beta_2x_i^{\\sin(x_i)} + \\epsilon_i\\)\n\n\n\nQ6\nThe sum of squared errors is the best way to estimate the model parameters.\n\nTrue\nFalse"
  },
  {
    "objectID": "L02-Fitting_Straight_Lines.html#analysis-of-variance",
    "href": "L02-Fitting_Straight_Lines.html#analysis-of-variance",
    "title": "2  L02: Straight Line Fitting",
    "section": "2.4 Analysis of Variance",
    "text": "2.4 Analysis of Variance\n\nStatistics is the Study of Variance\n\nGiven a data set with variable \\(\\underline y\\), \\(V(\\underline y) = \\sigma^2_y\\).\n\nThis is just the variance of a single variable.\n\nOnce we’ve incorporated the linear relationship with \\(\\underline x\\), \\(V(\\hat\\beta \\underline x + \\underline\\epsilon)=0+V(\\underline\\epsilon) = \\sigma^2\\)\n\nMathematically, \\(\\sigma^2 \\le \\sigma_y^2\\).\n\n\nThe variance in \\(Y\\) is explained by \\(X\\)!\n\n\nA Useful Identity, and its Interpretations\n\\[\\begin{align*}\n\\hat\\epsilon_i &= y_i - \\hat y_i \\\\&= y_i - \\bar y - (\\hat y_i - \\bar y)\\\\\n\\implies \\sum_{i=1}^n\\hat\\epsilon_i^2 &= \\sum_{i=1}^n(y_i-\\bar y)^2 - \\sum_{i=1}^n(\\hat y_i - \\bar y)^2\n\\end{align*}\\] where we’ve simply added and subtracted \\(\\bar y\\). The final line skips a few steps (try them yourself)!\nThe last line is often written as: \\(SS_E = SS_T - SS_{Reg}\\)\n\n\nSums of Squares\n\\[\nSS_E = SS_T - SS_{Reg}\n\\]\n\n\\(SS_E\\): Sum of Squared Errors\n\\(SS_T\\): Sum of Squares Total (i.e., without \\(\\underline x\\))\n\\(SS_{Reg}\\): Sum of Squares due to the regression.\n\nIt’s the variance of the line (calculated at observed \\(\\underline x\\) values) around the mean of \\(\\underline y\\)???\n\nThis is incredibly useful, but weird.\n\nI use \\(SS_{Reg}\\) instead of \\(SS_R\\) because some textbooks use \\(SS_R\\) as the Sume of Squared Residuals, which is confusing.\n\n\n\n\nAside: Degrees of Freedom\nDef: The number of “pieces of information” from \\(y_1, y_2, ..., y_n\\) to construct a new number.\n\nIf I have \\(x = (1,3,2,1,3,???)\\) and I know that \\(\\bar x = 2\\), I can recover the missing piece.\n\nThe mean “uses” (accounts for) one degree of freedom\n\nIf I have \\(x = (1,2,3,1,???,???)\\) and I know \\(\\bar x = 2\\) and \\(s_x^2=1\\), I can recover the two missing pieces.\n\nThe variance accounts for two degrees of freedom.\n\nOne \\(df\\) is required to compute it.\n\n\n\nEstimating one parameter takes away one degree of freedom for the rest!\n\nCan find \\(\\bar x\\) when \\(x = (1)\\), but can’t find \\(s_x^2\\) because there aren’t enough \\(df\\)!\n\n\n\nSums of Squares\n\n\n\n\n\n\n\n\n\nSource of Variation\nDegress of Freedom \\(df\\)\nSum of Squares (\\(SS\\))\nMean Square (\\(MS\\))\n\n\n\n\nRegression\n1\n\\(\\sum_{i=1}^n(\\hat y_i - \\bar y)^2\\)\n\\(MS_{Reg}\\)\n\n\nError\n\\(n-2\\)\n\\(\\sum_{i=1}^n(y_i - \\hat y_i)^2\\)\n\\(MS_E=s^2\\)\n\n\nTotal\n\\(n-1\\)\n\\(\\sum_{i=1}^n(y_i - \\bar y)^2\\)\n\\(s_y^2\\)\n\n\n\n\nNotice that \\(SS_T = SS_{Reg} + SS_E\\), which is also true for the \\(df\\) (but not \\(MS\\)). \nWhy is \\(df_E = n-2\\)? What two parameters have we estimated?\n\n\\(df_{Reg}\\) is trickier to explain. It suffices to know that \\(df_{Reg} = df_T-df_E\\).\n\n\n\n\nUsing Sums/Means of Squares\n\nIf \\(\\hat y_i = \\bar y\\) for all \\(i\\), then we have a horizontal line!\n\nThat is, there is no relationship between \\(\\underline x\\) and $\\underline \\(y\\).\nIn this case, \\(SS_{reg} = \\sum_{i=1}^n(\\hat y_i - \\bar y)^2 = 0\\).\n\n\nOkay, so, just test for \\(SS_{Reg} = 0\\)?\nBut how??? We need some measure of how far from 0 is statistically significant!!!\nRecall that \\(SS_E = \\sum_{i=1}^n(y_i - \\hat y_i)^2\\).\n\nWe can compare the variation around the line to the variation of the line.\n\nThis is \\(MS_{Reg}/MS_E\\), and it follows an \\(F\\) distribution!!\n\n\n\n\nThe F-test for Significance of Regression\n\\[\nF_{df_{Reg}, df_E} = \\dfrac{MS_{Reg}}{MS_E} = \\dfrac{MS_{Reg}}{s^2}\n\\]\n\nRecall that \\(SS_{Reg} = SS_E + SS_T &gt; SS_E\\), but the \\(df\\) make a difference.\nFor homework, show that \\(E(MS_{Reg}) = \\sigma^2 + \\beta_1^2\\sum_{i=1}^n(x_i-\\bar x)^2\\)\nThis implies that \\(E(MS_{Reg}) &gt; E(MS_E) = \\sigma^2\\).\n\n\n\nExercise A from Chapter 3 (Ch03 Exercises cover Ch1-3)\n\n\nA study was made on the effect of temperature on the yield of a chemical process. The data are shown to the right.\n\nAssuming \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\), what are the least squares estimates of \\(\\beta_0\\) and \\(\\beta_1\\)?\nConstruct the analysis of variance table and test the hypothesis \\(H_0: \\beta_1=0\\) at the 0.05 level.\nWhat are the confidence limits (at = 0.05) for \\(\\beta_1\\)?\n\n\n\nlibrary(aprean3) # Data from textbook\nhead(dse03a) # Data Set Exercise Ch03A\n\n   x  y\n1 -5  1\n2 -4  5\n3 -3  4\n4 -2  7\n5 -1 10\n6  0  8\n\nnrow(dse03a)\n\n[1] 11\n\n\n\n\n\nGood textbook questions: A, K, O, P, T, U, X (using data(anscombe) in R), Z, AA, CC. Note that all data sets in the textbook are available in an R package found here.\nPlease see Lab 2."
  },
  {
    "objectID": "L03-Residuals.html#the-residual-whats-left-over",
    "href": "L03-Residuals.html#the-residual-whats-left-over",
    "title": "3  L03: Assessing Fit",
    "section": "3.1 The residual: what’s left over",
    "text": "3.1 The residual: what’s left over\n\n\\(R^2\\): percent of variance explained by the regression model\n\n\n\\[\nR^2 = \\frac{SSReg}{SST} = \\frac{S_{XY}^2}{S_{XX}S_{YY}}\n\\]\n\n\nlayout(mat = matrix(c(1,2,3), nrow = 1), widths = c(0.5,1,1))\nset.seed(18)\nx &lt;- runif(25, 0, 10)\ny &lt;- rnorm(25, 2 + 5*x, 6)\n\nplot(rep(1, 25), y, xlab = \"y\", ylab = \"y has variance\", xaxt = \"n\")\nabline(h = mean(y))\naxis(2, mean(y), bquote(bar(y)), las = 1)\n\nplot(x, y, ylab = \"There's variance around the line\")\nabline(lm(y~x))\nabline(h = mean(y))\naxis(2, mean(y), bquote(bar(y)), las = 1)\n\nmids &lt;- predict(lm(y~x))\nfor(i in seq_along(mids)){\n    lines(x = rep(x[i], 2), y = c(y[i], mids[i]), col = 1)\n}\n\nmids &lt;- predict(lm(y~x))\nplot(mids ~ x, type = \"n\", ylab = \"The line varies around the mean of y!\")\nfor(i in seq_along(mids)){\n    lines(x = rep(x[i], 2), y = c(mean(y), mids[i]))\n}\naxis(2, at = mean(y), labels = bquote(bar(y)), las = 1)\nabline(h = mean(y))\n\n\n\n\n\n\n\n\nResidual Assumptions\n\nResidual: what’s left over\n\n\\(\\hat\\epsilon_i = y_i - \\hat y_i\\)\n\nAssumptions (from before):\n\n\\(E(\\epsilon_i) = 0\\)\n\\(V(\\epsilon_i) = \\sigma^2\\)\n\\(\\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n\nWe must check our assumptions!\n\nThere are statistical tests, but they’ll never tell you as much as a plot!\n\n\n\nResiduals versus fitted values: \\(\\hat{\\underline\\epsilon}\\) versus \\(\\hat{\\underline{y}}\\)\n\n\nWhy \\(\\hat{\\underline{y}}\\) instead of \\(\\underline y\\)?\n\nSee text. Try a regression of \\(\\hat{\\underline\\epsilon}\\) versus \\(\\underline{y}\\) yourself (mathematically and with code).\n\nWhy not \\(\\underline x\\)?\n\nFor simple linear regression, \\(\\hat{\\underline{y}}\\) is like a unit change for \\(\\underline x\\), so it doesn’t matter.\n\nFor multiple linear regression, it’s easier to have one variable for the \\(x\\) axis.\n\n\n\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(patchwork)\nlibrary(broom)\nlibrary(palmerpenguins)\n\npenguins &lt;- penguins[complete.cases(penguins),]\n\ng1 &lt;- ggplot(penguins) + \n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = y~x) +\n    labs(x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\",\n        title = \"y versus x\")\n\nplm &lt;- lm(body_mass_g ~ flipper_length_mm, data = penguins)\n\np2 &lt;- augment(plm)\n\ng2 &lt;- ggplot(p2) + \n    aes(x = .fitted, y = .resid) + \n    geom_point() +\n    geom_hline(yintercept = 0, col = \"grey\") +\n    labs(x = \"Fitted\", y = \"Residuals\", title = \"Residuals versus Fitted\")\n\n\ng1 / g2\n\n\n\n\n\n\n\n\nResidual Plots and Assumption Checking\nMathematics is the process of making assumptions and seeing if we can break them.\n\n\\(E(\\epsilon_i) = 0\\) is a given since \\(\\sum_{i=1}^n\\hat\\epsilon_i=0\\).\n\\(V(\\epsilon_i) = \\sigma^2\\)\n\nCheck if the variance looks stable.\n\n\\(\\epsilon_i \\sim N(0,\\sigma^2)\\) is harder to see\n\nExpect more points close to 0, fewer further away, no outliers\n\n\n\n\nResidual plots: unstable error\n\nx &lt;- runif(200, 0, 10)\ny0 &lt;- 2 - 3*x\ny &lt;- y0 + rnorm(length(x), 0, 2 * x)\n\ng1 &lt;- ggplot() + \n    aes(x = x, y = y) + \n    geom_point() + \n    geom_smooth(se = FALSE, method = \"lm\", formula = y~x) +\n    labs(x = NULL, y = NULL, title = \"y versus x\")\n\nxydf &lt;- augment(lm(y ~ x))\n\ng2 &lt;- ggplot(xydf) +\n    aes(x = .fitted, y = .resid) + \n    geom_point() +\n    geom_hline(yintercept = 0, col = \"grey\") +\n    labs(x = \"Fitted\", y = \"Residuals\", title = \"Residuals versus Fitted\")\n\ng1 + g2\n\n\n\n\n\n\nResidual plots: non-linear trend?\n\n## fig-height: 4\n## fig-width: 8\ng1 &lt;- ggplot(mtcars) + \n    aes(x = disp, y = mpg) + \n    geom_point() + \n    geom_smooth(se = FALSE, method = \"lm\", formula = y~x) +\n    labs(x = \"Engine Displacement\", y = \"Miles per Gallon\", title = \"y versus x\")\n\nmtdf &lt;- augment(lm(mpg ~ disp, data = mtcars))\n\ng2 &lt;- ggplot(mtdf) +\n    aes(x = .fitted, y = .resid) + \n    geom_point() +\n    geom_smooth(se = FALSE) +\n    geom_hline(yintercept = 0, col = \"grey\") +\n    labs(x = \"Fitted\", y = \"Residuals\", title = \"Residuals versus Fitted - non-linear trend?\")\n\ng1 + g2\n\n\n\n\n\n\nTesting Normality: Quantile-Quantile Plots\n\n\nConsider the data (2,3,3,4,5,5,6).\n\n50% of the data is below the median.\n\nFor a \\(N(0,1)\\) distribution, 50% of the data is below 0.\nPut the median on the y axis, 0 on the x axis.\n\n25% of the data is below Q1.\n\nFor a \\(N(0,1)\\) distribution, 25% is below qnorm(0.25) = -0.67\nPut a point at x = -0.67, y = Q1.\n\n75% of the data is below Q3.\n\nFor a \\(N(0,1)\\) distribution, 75% is below qnorm(0.75) = 0.67\nPut a point at x = 0.67, y = Q3.\n\n… and so on for the rest of the quantiles\n\n\nIf perfectly normal, expect a straight line!\n\nmydata &lt;- c(2, 3, 3, 4, 5, 5, 6)\nquants &lt;- qnorm(c(\n    0.125, 0.25, 0.375, 0.5, \n    0.625, 0.75, 0.875))\nplot(mydata ~ quants)\n\n\n\n\n\n\n\n\nOther Residual Plots:\nScale-Location\n\nScale: Standardized residual\nLocation: Fitted value\nMore on standardized residuals in Ch08\n\nCook’s Distance\n\nBasically, an outlier detection method.\nMore in Ch08\n\nLeverage\n\nMore in Ch08"
  },
  {
    "objectID": "L03-Residuals.html#participation-quiz",
    "href": "L03-Residuals.html#participation-quiz",
    "title": "3  L03: Assessing Fit",
    "section": "3.2 Participation Quiz",
    "text": "3.2 Participation Quiz\n\nQ01\nWhich of the following is not an assumption of linear models?\n\n$$\nThe variance is constant across all values of \\(X\\).\nThe height of the line is the mean value of \\(Y\\) for a given \\(X\\).\nNone of the above.\n\n\n\nQ02\nWhich of the following is a confidence interval for \\(s\\)?\n\n\\(\\chi^2_n(0.055) \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi^2_n(0.945)\\)\n\\(\\chi^2_n(0.945) \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi^2_n(0.055)\\)\n\\(s^2 \\pm \\text{Critical Value } * \\text{ se}(s^2)\\)\n\\(s \\pm \\text{Critical Value } * \\text{ se}(s)\\)\nNone of the above\n\n\n\nQ03\nWhich of the following is not a random variable?\n\n\\(Y\\)\n\\(\\hat\\epsilon_i\\)\n\\(\\hat Y\\)\n\\(\\underline\\epsilon\\)\n\n\n\nQ04\n\\(F = t^2\\)\n\nTrue\nFalse\nSometimes true\n\n\n\nQ05\nWhich of the following is the definition of a residual?\n\n\\(y_i - \\hat y_i\\)\n\\((y_i - \\hat y_i)^2\\)\n\\(\\hat y_i - y_i\\)\n\\((\\hat y_i - y_i)^2\\)\n\n\n\nQ06\nWhich of the following statements about \\(R^2\\) is false?\n\n\\(R^2 = SSReg / SST\\)\n\\(R^2 = r^2\\), where \\(r\\) is the correlation between \\(\\underline x\\) and \\(\\underline y\\)\n\\(R^2\\) compares the variance of the line to the variance of \\(y\\) alone\n\\(R^2\\) is not a random variable"
  },
  {
    "objectID": "L03-Residuals.html#maximum-likelihood",
    "href": "L03-Residuals.html#maximum-likelihood",
    "title": "3  L03: Assessing Fit",
    "section": "3.3 Maximum Likelihood",
    "text": "3.3 Maximum Likelihood\n\nMain Idea\nFind the values \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\), and \\(\\hat\\sigma^2\\) that maximize the likelihood of seeing our data.\nUnder the assumptions that \\(X\\) is fixed, \\(Y = \\beta_0 + \\beta_1X + \\epsilon_i\\), and \\(\\epsilon_i\\stackrel{iid}{\\sim}N(0,\\sigma^2)\\), \\[\nY \\sim N(\\beta_0 + \\beta_1X, \\sigma^2)\n\\]\n\n\nThe Likelihood\nThe probability of observering a data point is: \\[\nf_Y(y_i|x_i, \\beta_0, \\beta_1, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1x_i)^2\\right)\n\\]\nThe likelihood of the parameters, given the data, is: \\[\nL(\\beta_0, \\beta_1, \\sigma^2|x_i, y_i) = \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1x_i)^2\\right)\n\\]\n\nIt’s just a shift in perspective!\n\n\n\nSimple Coin Flip Example\nSuppose we flipped 10 bottle caps and got 6 “crowns”. Assume the probability of “crown” (\\(C\\)) is unknown, labelled \\(p\\).\n\nThe probability of one cap flip is \\(P(C|p) = p\\).\nThe probability of this is \\(P(C = 6|p) = p^6(1-p)^4\\).\n\nThis is just \\(P(\\underline y|p) = \\prod_{i=1}^nP(Y = y_i)\\).\n\nThe likelihood is \\(L(p|\\underline y) = \\prod_{i=1}^nP(Y = y_i)\\).\n\n\n\nMaximizing the Likelihood in LM\n\\[\nL(\\beta_0, \\beta_, \\sigma^2) = \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-1}{2\\sigma^2}(y_i - \\beta_0 - \\beta_1x_i)^2\\right)\n\\]\n\nTo maximize w.r.t \\(\\beta_0\\), we set the derivative w.r.t \\(\\beta_0\\) to 0 and solve for \\(\\beta_0\\).\n\n\\(\\frac{\\partial L(\\beta_0, \\beta_, \\sigma^2)}{\\partial \\beta_0} = 0\\).\n\nRepeat for \\(\\frac{\\partial L(\\beta_0, \\beta_, \\sigma^2)}{\\partial \\beta_1} = 0\\) and \\(\\frac{\\partial L(\\beta_0, \\beta_, \\sigma^2)}{\\partial \\sigma^2} = 0\\)\n\nHWK: Show that the estimates for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are the same as the OLS estimates. The estimate for \\(\\hat\\sigma^2\\) should come out to: \\[\n\\hat\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n\\left(y_i - \\hat\\beta_0 - \\hat\\beta_1x_i\\right)^2\n\\]\n\n\nBias-Variance Decomposition\nOn the next slide, we’ll show that \\(E\\left((Y_i - \\hat Y_i)^2\\right)\\) can be decomposed into \\(V(\\epsilon_i)\\), squared bias, and the variance of the fitted model.\n\n\\(V(\\epsilon_i)\\) is the variance of the true errors.\nBias is the difference between the true model and the estimated one.\n\nSystematic difference, not just random errors\n\ne.g. fitting a linear model to a non-linear trend\n\n\nVariance of the fitted model across all possible samples\n\nNote that this is a slight deviation from how the text presents it.\n\n\nDerivation\nSuppose the true value is \\(Y_i = g(x_i) + \\epsilon_i\\) (not necessarily linear), where \\(E(\\epsilon_i) = 0\\), \\(E(Y_i) = g(x_i)\\), and \\(V(\\epsilon_i) = \\sigma^2\\). \\[\\begin{align*}\nE\\left((Y_i - \\hat Y_i)^2\\right) &= E(Y_i - 2Y_i\\hat Y_i + \\hat Y_i^2)\\\\\n&= E(Y_i^2) - 2E(Y_i\\hat Y_i) + E(\\hat Y_i^2)\\\\\n&= E((g(x_i) + \\epsilon)^2) - 2E(g(x_i) + \\epsilon_i)E(\\hat Y_i) + E(\\hat Y_i^2)\\\\\n&= g^2(x_i) + 2g(x_i)E(\\epsilon_i) + E(\\epsilon_i^2) - 2g(x_i)E(\\hat Y_i) + E(\\hat Y_i^2)\\\\\n&= g^2(x_i) + \\sigma^2 - 2g(x_i)E(\\hat Y_i) + E(\\hat Y_i^2) + E(\\hat Y_i)^2 - E(\\hat Y_i)^2\\\\\n&= \\sigma^2 + (g(x_i) - E(\\hat Y_i))^2 + V(\\hat Y_i)\\\\\n&= \\text{Error Variance} + \\text{Bias}^2 + \\text{Fitted Model Variance}\n\\end{align*}\\]\n\n\nNot Interpreting the MSE\n\\[\nE((Y_i - \\hat{Y_i})^2) = \\sigma^2 + (g(x_i) - E(\\hat Y_i))^2 + V(\\hat Y_i)\n\\]\n\nWe don’t know any of the numbers on the right!!!\n\nThe decomposition is theoretical, we can’t tease apart \\(s^2\\) into these terms.\n\n\nInterpreting the MSE\nThe basic question of statistics: “How big is this number?”\n\nCompare to previous studies - is MSE larger than \\(\\sigma^2\\)?\n\nImplies that Bias\\(^2\\) and Fitted Model Variance are larger than expected.\nF-test\n\nCompare to “pure error” - direct estimate of \\(\\sigma^2\\).\n\ni.e. the variance in repeated trials on the same covariate values\nTextbooks devotes a lot to this, but it’s often not plausible.\n\nWon’t be on tests!\n\n\nCompare to another model\n\nWe’ll focus on this (later)!\n\n\n\n\nCompare to Previous Studies\nHypothesis test for \\(\\sigma^2 = \\sigma_0^2\\) versus \\(\\sigma^2 &gt; \\sigma_0^2\\), where \\(\\sigma_0\\) is the value from a previous study.\n\nIf significant, some of your error is coming from the study design!\n\n\n\nCompare to other models\nIt can be shown that \\(E(MSReg) = \\sigma^2 + \\beta_1S_{XX}\\).\nConsider the Null hypothesis \\(\\beta_1 =0\\) (why is this a good null?).\n\nUnder this null, \\(\\frac{MSReg}{s^2}\\sim F_{1, n-2}\\).\n\nObvious CI from this. \n\nThis is equivalent to the t-test for \\(\\beta_1\\)! (See text.)\n\n\n\nMSE of a Parameter: Bias of \\(s^2\\)\nFrom a previous class, we know that \\[\n\\frac{(n-2)s^2}{\\sigma^2}\\sim\\chi^2_{n-2}\n\\]\nFrom wikipedia, we know that the mean of a \\(\\chi^2_k\\) distribution is \\(k\\). Therefore, \\[\nE\\left(\\frac{(n-2)s^2}{\\sigma^2}\\right) = n-2 \\Leftrightarrow E(s^2) = \\sigma^2\n\\] and thus \\(s^2\\) is unbiased.\nThis does not necessarily mean that \\(s^2\\) is the best estimator for \\(\\sigma^2\\)!\n\n\nMSE of a Parameter: Bias of \\(s\\)\nEven though \\(s^2\\) is an unbiased estimator, \\(s = \\sqrt{s^2}\\) is biased! Specifically, \\(E(s) &lt; \\sigma\\)\nTo see why, first note that \\[\nV(s) = E(s^2) - (E(s))^2 \\Leftrightarrow E(s) = \\sqrt{E(s^2) - V(s)}\n\\] since \\(V(s) &gt; 0\\), \\(E(s^2) - V(s) &lt; E(s^2)\\), and therefore \\[\nE(s) &lt; \\sqrt{E(s^2)} = \\sqrt{\\sigma^2} = \\sigma\n\\]"
  },
  {
    "objectID": "L04-Matrix_Form.html#chapter-04-summary",
    "href": "L04-Matrix_Form.html#chapter-04-summary",
    "title": "4  L04: Regression in Matrix Form",
    "section": "4.1 Chapter 04 Summary",
    "text": "4.1 Chapter 04 Summary\n\nMatrix Forms of Things We’ve Seen\nUsing\n\\[\nY = \\begin{bmatrix}Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_n\\end{bmatrix};\\quad \\underline y = \\begin{bmatrix}y_1\\\\ y_2\\\\ \\vdots\\\\ y_n\\end{bmatrix};\\quad X = \\begin{bmatrix}1 & x_1\\\\1 &x_2\\\\ \\vdots & \\vdots\\\\ 1 & x_n\\end{bmatrix};\\quad \\underline\\beta = \\begin{bmatrix}\\beta_0\\\\\\beta_1\\end{bmatrix};\\quad\\underline\\epsilon \\begin{bmatrix}\\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots\\\\ \\epsilon_n\\end{bmatrix}\n\\]\n\\(Y = X\\underline\\beta + \\underline\\epsilon\\) is the same as \\[\\begin{align*}\nY_1 &= \\beta_0 + \\beta_1x_1 + \\epsilon_1\\\\\nY_2 &= \\beta_0 + \\beta_1x_2 + \\epsilon_2\\\\\n&\\vdots\\\\\nY_n &= \\beta_0 + \\beta_1x_n + \\epsilon_n\\\\\n\\end{align*}\\]\n\n\nSome Fun Matrix Forms\n\\[\\begin{align*}\n\\underline\\epsilon^T\\underline\\epsilon &= \\epsilon_1^2 + \\epsilon_2^2 + ... + \\epsilon_n^2 = \\sum_{i=1}^n\\epsilon_i^2\\\\\nY^TY &= \\sum Y_i^2\\\\\n\\mathbf{1}^T\\underline y &= \\sum y_i = n\\bar y\\\\\nX^TX &= \\begin{bmatrix}n & \\sum x_i\\\\ \\sum x_i & \\sum x_i^2\\end{bmatrix}\\\\\n(X^TX)^{-1} &= \\frac{1}{nS_{XX}}\\begin{bmatrix}\\sum x_i^2 & -n\\bar x\\\\-n \\bar x & n\\end{bmatrix}\\\\\nX^TY &= \\begin{bmatrix}\\sum Y_i\\\\ \\sum x_iY_i\\end{bmatrix}\n\\end{align*}\\]\n\n\nThe “Normal” Equations\n\n\nCopied from previous slide: \\[\\begin{align*}\nX^TX &= \\begin{bmatrix}n & \\sum x_i\\\\ \\sum x_i & \\sum x_i^2\\end{bmatrix}\\\\\nX^T\\underline y &= \\begin{bmatrix}\\sum y_i\\\\ \\sum x_iy_i\\end{bmatrix}\n\\end{align*}\\]\n\nThe textbook included the following equations in Ch02. The estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) in OLS (same as MLE) are the solution to:\n\\[\\begin{align*}\n\\hat\\beta_0 + \\hat\\beta_1\\sum x_i &= \\sum y_i\\\\\n\\hat\\beta_0\\sum x_i + \\hat\\beta_1\\sum x_i^2 &= \\sum x_iy_i\\\\\n\\end{align*}\\]\n\n\nPutting these together: \\[\nX^TX\\hat{\\underline\\beta} = X^T\\underline y\\quad \\Leftrightarrow\\quad \\hat{\\underline\\beta} = (X^TX)^{-1}X^T\\underline y\n\\] Try this out using the definition of \\((X^TX)^{-1}\\) on the previous slide.\n\n\n(Corrected) ANOVA in Matrix Form\n\n\n\n\n\n\n\n\n\nSource\n\\(df\\)\n\\(SS\\)\n\\(MS\\)\n\n\n\n\nRegression (corrected)\n1\n\\(\\hat{\\underline\\beta}^TX^T\\underline y - n\\bar{\\underline y}^2\\)\n\\(SS/df\\)\n\n\nError\n\\(n-2\\)\n\\(\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y\\)\n\\(SS/df\\)\n\n\nTotal (corrected)\n\\(n - 1\\)\n\\(\\underline y^t\\underline y - n\\bar{\\underline y}^2\\)\n\\(SS/df\\)\n\n\n\n\nThe “corrected” ANOVA is the ANOVA table for comparing the errors due to \\(\\hat\\beta_1\\) to the errors due to \\(\\hat\\beta_0\\).\nThis is different from comparing a model with \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) to a model with neither parameter.\n\nThe value of the “correction” is based on \\(\\bar x\\). If the slope is 0, then the estimate for \\(\\hat\\beta_0\\) is \\(\\bar x\\)."
  },
  {
    "objectID": "L04-Matrix_Form.html#variances",
    "href": "L04-Matrix_Form.html#variances",
    "title": "4  L04: Regression in Matrix Form",
    "section": "4.2 Variances",
    "text": "4.2 Variances\n\nVariance of a Vector: the Variance-Covariance Matrix\nIn general, for vector-valued random variable \\(Z = (Z_1, Z_2, \\dots, Z_n)\\), \\[\nV(Z) = \\begin{bmatrix}\nV(Z_1) & cov(Z_1, Z_2) & \\cdots & cov(Z_1, Z_n) \\\\\ncov(Z_2, Z_1) & V(Z_2) & \\cdots &  cov(Z_2, Z_n)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\ncov(Z_n, Z_1) & cov(Z_n, Z_2) & \\cdots & V(Z_n)\n\\end{bmatrix}\n\\]\n\n\n\\(V(\\hat{\\underline\\beta})\\)\n\n\nLet’s start with the covariance of \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\): \\[\\begin{align*}\ncov(\\hat\\beta_0, \\hat\\beta_1) &= cov(\\bar y - \\hat\\beta_1\\bar x, \\hat\\beta_1)\\\\\n&= -\\bar x cov(\\hat\\beta_1, \\hat\\beta_1)\\\\\n&= -\\bar x V(\\hat\\beta_1)\\\\\n&= \\frac{-\\sigma^2\\bar x}{S_{XX}}\n\\end{align*}\\]\n\nThe var-covar matrix is: \\[\\begin{align*}\nV(\\hat{\\underline\\beta}) &= \\begin{bmatrix}\nV(\\hat\\beta_0) & cov(\\hat\\beta_0, \\hat\\beta_1)\\\\\ncov(\\hat\\beta_0, \\hat\\beta_1) & V(\\hat\\beta_1)\n\\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n\\frac{\\sigma^2\\sum x_i^2}{S_{XX}} & \\frac{-\\sigma^2\\bar x}{S_{XX}}\\\\\n\\frac{-\\sigma^2\\bar x}{nS_{XX}} & \\frac{\\sigma^2}{S_{XX}}\n\\end{bmatrix}\\\\\n&= \\frac{\\sigma^2}{nS_{XX}}\\begin{bmatrix}\n\\sum x_i^2 & -n\\bar x\\\\\n-n\\bar x & n\n\\end{bmatrix}\\\\\n&= (X^TX)^{-1}\\sigma^2\n\\end{align*}\\]\n\n\n\n\nVariance of \\(\\hat Y_0\\)\nLet \\(X_0 = [1, x_0]\\) be an arbitrary observation. Then the predicted value of the line at \\(X_0\\), labelled \\(\\hat Y_0\\), is: \\[\\begin{align*}\nV(\\hat Y_0) &= V(X_0\\hat{\\underline\\beta})\\\\\n&= X_0V(\\hat{\\underline\\beta})X_0^T\\\\\n&= \\sigma^2X_0(X^TX)^{-1}X_0\n\\end{align*}\\]\nHWK: Verify that \\(X_0(X^TX)^{-1}X_0\\) is a \\(1\\times 1\\) matrix.\n\n\nVariance of \\(\\hat Y_{n+1}\\)\nFor a new observation, we have the variance of the line plus a new unobserved error \\(\\epsilon_{n+1}\\). \\[\\begin{align*}\nV(\\hat Y_{n+1}) &= V(X_{n+1}\\hat{\\underline\\beta} + \\epsilon_{n+1})\\\\\n&= X_{n+1}V(\\hat{\\underline\\beta})X_{n+1}^T + V(\\epsilon_{n+1})\\\\\n&= \\sigma^2X_{n+1}(X^TX)^{-1}X_{n+1}^T + \\sigma^2\\\\\n&= \\sigma^2\\left(X_{n+1}(X^TX)^{-1}X_{n+1}^T + 1\\right)\n\\end{align*}\\]\nHWK: Verify (empirically or mathematically) that this is smallest when \\(x_0 = \\bar x\\).\n\n\nSummary\nWhen we have one predictor, it is clear that: \\[\\begin{align*}\nY &= X\\hat{\\underline\\beta} + \\underline\\epsilon\\\\\n\\hat{\\underline\\beta} &= (X^TX)^{-1}X^T\\underline y\\\\\nV(\\hat{\\underline\\beta}) &= (X^TX)^{-1}\\sigma^2\\\\\nV(\\hat Y_0) &= \\sigma^2X_0(X^TX)^{-1}X_0^T\\\\\nV(\\hat Y_{n+1}) &= \\sigma^2(X_{n+1}(X^TX)^{-1}X_{n+1}^T + 1)\\\\\n\\end{align*}\\]\nThis will not change when we add predictors!"
  },
  {
    "objectID": "L04-Matrix_Form.html#participation-questions",
    "href": "L04-Matrix_Form.html#participation-questions",
    "title": "4  L04: Regression in Matrix Form",
    "section": "4.3 Participation Questions",
    "text": "4.3 Participation Questions\n\nQ01\nWhich statement about matrix multiplication is false?\n\n\\((AB)^T = B^TA^T\\)\n\\((AB)^{-1} = B^{-1}A^{-1}\\)\n\\((A + B)^T = A^T + B^T\\)\n\\(A^{-1}A = AA^{-1}\\)\n\n\n\nQ02\nThe Normal Equations come from:\n\nSetting the partial derivatives of \\(\\underline\\epsilon^T\\underline\\epsilon\\) to 0.\nSetting the partial derivatives of \\(\\hat{\\underline\\epsilon}^T\\hat{\\underline\\epsilon}\\) to 0.\nRearranging the equation \\(\\sum_{i=1}^n(y_i - \\hat y_i)^2\\) for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)\nRearranging the equation \\(\\sum_{i=1}^n(y_i - \\beta_0 + \\beta_1x_i + \\epsilon_i)^2\\) for \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\)\n\n\n\nQ03\n\\(MS_E = MS_T - MS_Reg\\)\n\nTrue\nFalse\n\n\n\nQ04\nFor vector-valued r.v. \\(Z\\), \\(V(Z)\\) is a symmetric matrix.\n\nNo, since the \\(i,j\\) element is \\(cov(Z_i, Z_j)\\) and the \\(j,i\\) element is \\(cov(Z_j, Z_i)\\).\nNo, since the \\(i,i\\) element is \\(V(Z_i)\\), which is different for different \\(i\\).\nYes, because it is a scalar.\nYes, because \\(cov(Z_i, Z_j) = cov(Z_j, Z_i)\\)\n\n\n\nQ05\nFor scalar-valued random variables \\(X\\) and \\(Y\\), which statement is true?\n\n\\(cov(a + bX, Y) = b cov(X,Y)\\)\n\\(cov(a + bX, bY) = b cov(X,Y)\\)\n\\(cov(bX, X) = b^2V(X)\\)\n\\(cov(a + bX, a + bY) = a + b cov(X,Y)\\)\n\n\n\nQ06\nThe variance of the line evaluated at an existing point is smaller than the variance of the line evaluated at a new point, even if the new point is the same as an existing point.\n\nTrue\nFalse"
  },
  {
    "objectID": "L05-General_Regression.html#chapter-summary",
    "href": "L05-General_Regression.html#chapter-summary",
    "title": "5  L05: The General Regression Situation",
    "section": "5.1 Chapter Summary",
    "text": "5.1 Chapter Summary\n\nThe Normal Equations\n\\[\\begin{align*}\n\\underline\\epsilon^T\\underline\\epsilon &= (Y - X\\underline\\beta)^T(Y - X\\underline\\beta)\\\\\n&= ... = Y^TY - 2\\underline\\beta^TX^TY + \\underline\\beta^TX^TX\\underline\\beta\n\\end{align*}\\] We then take the derivative with respect to \\(\\underline\\beta\\). Note that \\(X^TX\\) is symmetric and \\(Y^TX\\underline\\beta\\) is a scalar.. \\[\\begin{align*}\n\\frac{\\partial}{\\partial\\underline\\beta}\\underline\\epsilon^T\\underline\\epsilon &= 0 - 2X^TY + 2X^TX\\underline\\beta\n\\end{align*}\\]\n\nFor the 1 predictor case, make sure the equations look the same!\n\nSetting to 0, rearranging, and plugging in our data gets us the Normal equations: \\[\\begin{align*}\nX^TX\\underline{\\hat\\beta} &= X^T\\underline y\n\\end{align*}\\]\n\n\nFacts\n\\[X^TX\\underline{\\hat\\beta} = X^T\\underline y\\]\n\nNo distributional assumptions.\nIf \\(X^TX\\) is invertible, \\(\\hat{\\underline\\beta} = (X^TX)^{-1}X^T\\underline y\\).\n\n\\(\\hat{\\underline\\beta}\\) is a linear transformation of \\(\\underline y\\)!\nThis is the same as the MLE.\n\n\\(E(\\hat{\\underline\\beta}) = \\underline\\beta\\) and \\(V(\\hat{\\underline\\beta}) = \\sigma^2(X^TX)^{-1}\\).\n\nThis is the smallest variance amongst all unbiased estimators of \\(\\underline\\beta\\).\n\n\n\n\nExample Proof Problems\n\nProve that \\(\\sum_{i=1}^n\\hat\\epsilon_i\\hat y_i = 0\\).\nProve that \\((1/n)\\sum_{i=1}^n\\hat\\epsilon_i = 0\\).\nProve that \\(X^TX\\) is symmetric. Is \\(A^TA\\) symmetric in general?\n\n\n## Demonstration that they're true (up to a rounding error)\nmylm &lt;- broom::augment(lm(mpg ~ disp, data = mtcars))\nsum(mylm$.resid * mylm$.fitted)\n\n[1] 4.241496e-12\n\nmean(mylm$.resid)\n\n[1] 6.77236e-15\n\nX &lt;- model.matrix(mpg ~ disp, data = mtcars)\nall.equal(t(X) %*% X, t(t(X) %*% X))\n\n[1] TRUE\n\n\n\n\nAnalysis of Variance (Corrected)\n\n\n\n\n\n\n\n\nSource\n\\(df\\)\n\\(SS\\)\n\n\n\n\nRegression (corrected)\n\\(p - 1\\)\n\\(\\underline{\\hat{\\beta}}^TX^T\\underline y - n\\bar{\\underline y}^2\\)\n\n\nError\n\\(n - p\\)\n\\(\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y\\)\n\n\nTotal (corrected)\n\\(n - 1\\)\n\\(\\underline y^t\\underline y - n\\bar{\\underline y}^2\\)\n\n\n\n\nNote that \\(p\\) is the number of parameters, not the index of the largest param.\n\n\\(\\underline\\beta = (\\beta_0, \\beta_1, ..., \\beta_{p-1})\\)\n\nWe’ll always be using corrected sum-of-squares.\n\nEspecially next chapter!\n\n\n\n\n\\(F\\)-test for overall significance\nIf SSReg is significantly larger than SSE, then fitting the model was worth it!\n\nThis is a test for \\(\\beta_1 = \\beta_2 = ... = \\beta_{p-1} = 0\\), versus any \\(\\beta_j\\ne 0\\).\n\nAs before, we find a quantity with a known distribution, then use it for hypothesis tests.\n\\[\nF = \\frac{MS(Reg|\\hat\\beta_0)}{MSE} = \\frac{SS(Reg|\\hat\\beta_0)/(p-1)}{SSE/(n-p)} \\sim F_{p-1, n-p}\n\\]\nAgain, note that a regression with no predictors always has \\(\\hat\\beta_0 = \\bar y\\).\n\n\nExample: Significance of disp\n\n\n\nanova(lm(mpg ~ disp, data = mtcars)) |&gt; \n    knitr::kable()\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\ndisp\n1\n808.8885\n808.88850\n76.51266\n0\n\n\nResiduals\n30\n317.1587\n10.57196\nNA\nNA\n\n\n\n\n\n\n\\(p = 2\\), \\(n = 32\\).\n\n\\(df_R' + df_E' = df_T'\\), where \\(df'\\) is the df for corrected SS.\n\nVerified these numbers in the last lecture\n\n\n\nplot(mpg ~ disp, data = mtcars)\nabline(lm(mpg ~ disp, data = mtcars))\n\n\n\n\n\n\n\n\nExample: \\(F_{1,p-1} = t^2_{p-1}\\)\n\nanova(lm(mpg ~ qsec, data = mtcars)) |&gt; \n    knitr::kable()\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nqsec\n1\n197.3919\n197.39193\n6.376702\n0.017082\n\n\nResiduals\n30\n928.6553\n30.95518\nNA\nNA\n\n\n\n\nsummary(lm(mpg ~ qsec, data = mtcars))$coef |&gt;\n    knitr::kable()\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-5.114038\n10.0295433\n-0.5098974\n0.6138544\n\n\nqsec\n1.412125\n0.5592101\n2.5252133\n0.0170820\n\n\n\n\n\nWhat do you notice about these two tables?\n\n\nExample: Significance of Regression (\\(F_{2,p-1} \\ne t^2_{p-1}\\))\n\nanova(lm(mpg ~ qsec + disp, data = mtcars)) |&gt; \n    knitr::kable()\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nqsec\n1\n197.3919\n197.39193\n18.25740\n0.0001898\n\n\ndisp\n1\n615.1185\n615.11850\n56.89424\n0.0000000\n\n\nResiduals\n29\n313.5368\n10.81161\nNA\nNA\n\n\n\n\nsummary(lm(mpg ~ qsec + disp, data = mtcars))$coef |&gt;\n    knitr::kable()\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n25.5045079\n7.1840940\n3.5501356\n0.0013359\n\n\nqsec\n0.2122880\n0.3667758\n0.5787951\n0.5671961\n\n\ndisp\n-0.0398877\n0.0052882\n-7.5428272\n0.0000000\n\n\n\n\n\nWe’ll learn more about the ANOVA table next lecture.\n\n\n\\(R^2\\) again\n\\[\nR^2 = \\frac{SS(Reg|\\hat\\beta_0)}{Y^TY - SS(\\beta_0)} = \\frac{\\sum(\\hat y_i - \\bar y)^2}{\\sum(y_i - \\bar y)^2}\n\\]\nWorks for multiple dimensions!… kinda.\n\n\n\\(R^2\\) is bad?\n\n\n\nnx &lt;- 10 # Number of uncorrelated predictores\nuncorr &lt;- matrix(rnorm(50*(nx + 1)), \n    nrow = 50, ncol = nx + 1)\n## First column is y, rest are x\ncolnames(uncorr) &lt;- c(\"y\", paste0(\"x\", 1:nx))\nuncorr &lt;- as.data.frame(uncorr)\n\nrsquares &lt;- NA\nfor (i in 2:(nx + 1)) {\n    rsquares &lt;- c(rsquares,\n        summary(lm(y ~ ., \n            data = uncorr[,1:i]))$r.squared)\n}\nplot(1:10, rsquares[-1], type = \"b\",\n    xlab = \"Number of Uncorrelated Predictors\",\n    ylab = \"R^2 Value\",\n    main = \"R^2 ALWAYS increases\")\n\n\n\n\n\n\n\n\n\n\n\nAdjusted (Multiple) \\(R^2\\)\n\\[\nR^2_a = 1 - (1 - R^2)\\left(\\frac{n-1}{n-p}\\right)\n\\]\n\nPenalizes added predictors - won’t always increase!\n\nStill might increase by chance alone!\n\nF-test\n\n\\(R^2_a = R^2\\) when \\(p=1\\) (intercept model)\n\nStill not perfect!\n\nWorks for comparing different models on same data\nWorks (poorly) for comparing different models on different data.\n\nIn general you should use \\(R^2_a\\), but always be careful."
  },
  {
    "objectID": "L05-General_Regression.html#prediction-and-confidence-intervals-again",
    "href": "L05-General_Regression.html#prediction-and-confidence-intervals-again",
    "title": "5  L05: The General Regression Situation",
    "section": "5.2 Prediction and Confidence Intervals (Again)",
    "text": "5.2 Prediction and Confidence Intervals (Again)\n\n\\(R^2\\) and \\(F\\)\nRecall that \\[\nF = \\frac{MS(Reg|\\hat\\beta_0)}{MSE} = \\frac{SS(Reg|\\hat\\beta_0)/(p-1)}{SSE/(n-p)} \\sim F_{p-1, n-p}\n\\]\nFrom the definition of \\(R^2\\), \\[\\begin{align*}\nR^2 &= \\frac{SS(Reg|\\hat\\beta_0)}{SST}\\\\\n&= \\frac{SS(Reg|\\hat\\beta_0)}{SS(Reg|\\hat\\beta_0) + SSE}\\\\\n&= \\frac{(p-1)F}{(p-1)F + (n-p)}\n\\end{align*}\\] Conclusion: Hypothesis tests/CIs for \\(R^2\\) aren’t useful. Just use \\(F\\)!\n\n\nCorrelation of \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\), \\(\\hat\\beta_2\\), etc.\nWith a different sample, we would have gotten slightly different numbers!\n\nIf the slope changed, the intercept must change to fit the data\n\n(and )\nThe parameter estimates are correlated!\n\nSimilar things happen with multiple predictors!\nThis correlation can be a problem for confidence regions\n\n\n\nUncorrelated \\(\\hat\\underline{\\beta}\\)\n\\[\nV(\\hat{\\underline\\beta}) = \\sigma^2(X^TX)^{-1}\n\\]\nIn simple linear regression, \\[\n(X^TX)^{-1} = \\frac{1}{nS_{XX}}\\begin{bmatrix}\\sum x_i^2 & -n\\bar x\\\\-n \\bar x & n\\end{bmatrix}\n\\]\nso the correlation is 0 when \\(\\bar x = 0\\)!\n\n\nPrediction and Confidence Intervals for \\(Y\\)\n\\(\\hat Y = X\\hat\\beta\\).\n\nA confidence interval around \\(\\hat Y\\) is based on the variance of \\(\\hat\\beta\\).\n\\(\\hat Y \\pm t * se(X\\hat\\beta)\\)\n\n\\(Y_{n+1} = X\\beta + \\epsilon_{n+1}\\)\n\nA prediction interval around \\(Y_{n+1}\\) is based on the variance of \\(\\hat\\beta\\) and \\(\\epsilon\\)!\n\\(\\hat Y_{n+1} \\pm t * se(X\\hat\\beta + \\epsilon_{n+1})\\)"
  },
  {
    "objectID": "L05-General_Regression.html#participation-questions",
    "href": "L05-General_Regression.html#participation-questions",
    "title": "5  L05: The General Regression Situation",
    "section": "5.3 Participation Questions",
    "text": "5.3 Participation Questions\n\nQ1\nWhich of the following are the Normal equations?\n\n\\(X^TX\\underline\\beta = X^T\\underline y\\)\n\\(X^TX\\underline{\\hat\\beta} = X^T\\underline y\\)\n\\(\\hat{\\underline\\beta} = (X^TX)^{-1}X^T\\underline y\\)\n\\(f(\\epsilon_i) = \\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp\\left(\\frac{-1}{2}\\epsilon_i^2\\right)\\)\n\n\n\nQ2\nWhen is \\(X^TX\\) not invertible?\n\nOne of the predictors can be written as a linear combination of the others.\nThere are more predictors than observations.\nOne of the predictors has 0 variance.\nAll of the above\n\n\n\nQ3\nWhat does a significant F-test for the overall regression mean?\n\nThe variance in the line is significantly larger than the variance in the data.\nThe estimate of \\(\\beta_1\\) is significantly different from \\(\\beta_0\\),\nThe variance of the line is significantly different from 0.\nAt least one of the predictors in the model will have significant \\(t\\)-test.\n\n\n\nQ4\n\\(R^2\\) is best used for:\n\nDetermining whether a new predictor is worth including.\nComparing models with different numbers of predictors.\nComparing models based on different data sets.\nNone of the above.\n\n\n\nQ5\nWhich of the following describes a Prediction Interval?\n\nThe CI for the predicted value of the line\nThe CI for the predicted value of the line, including unobserved error at an \\(X\\) value\nThe CI for the predicted value of the line, including unobserved error at an \\(X\\) value that was not observed in the data\nThe CI for the predicted value of the line, including unobserved error at an \\(X\\) value that was not observed in the data, using the true value of \\(\\sigma^2\\)\n\n\n\nQ6\nWhich ANOVA table does the anova() function calculate?\n\n\n\n\n\n\n\n\n\n\n\nSource\n\\(df\\)\n\\(SS\\)\n\n\n\n\nRegression\n1\n\\(\\hat{\\underline\\beta}^TX^T\\underline y - n\\bar{\\underline y}^2\\)\n\n\nError\n\\(n-2\\)\n\\(\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y\\)\n\n\nTotal\n\\(n - 1\\)\n\\(\\underline y^t\\underline y - n\\bar{\\underline y}^2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\n\\(df\\)\n\\(SS\\)\n\n\n\n\nRegression\n1\n\\(\\hat{\\underline\\beta}^TX^T\\underline y\\)\n\n\nError\n\\(n-2\\)\n\\(\\underline y^t\\underline y- \\hat{\\underline\\beta}^TX^T\\underline y\\)\n\n\nTotal\n\\(n - 1\\)\n\\(\\underline y^t\\underline y\\)"
  },
  {
    "objectID": "L06-Extra_Sums_of_Squares-1.html#introduction",
    "href": "L06-Extra_Sums_of_Squares-1.html#introduction",
    "title": "6  L06: Extra Sum-of-Squares (Part 1)",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\n\nToday’s Main Idea\nIf you add or remove predictors, the variance of the residuals changes!\n\nAs always, we ask if it’s a “big” change.\nDifferent predictors have a different effect on the residuals.\n\nWhich predictors have a meaningful (significant?) effect on the residuals?\n\n\nSum-of-Squares due to Regression\n\nSince SSE = SST - SSReg and SST never changes, we’re focusing on SSReg.\nRecall: SSReg is the variance of the line itself!\n\n\\[\nSSReg = \\sum_{i=1}^n(\\hat y_i - \\bar y)\n\\]\n\n\nSSReg in two different penguin models\nIn the penguins data, we’re determining which predictors are associated with body mass.\n\n\\(SS_1\\) = SSReg for model 1\n\n\\(\\texttt{body\\_mass\\_g} = \\beta_0 + \\beta_1 \\texttt{flipper\\_length\\_mm} + \\beta_2 \\texttt{bill\\_length\\_mm} + \\beta_3 \\texttt{bill\\_depth\\_mm}\\)\n\n\\(SS_2\\) = SSReg for model 2\n\n\\(\\texttt{body\\_mass\\_g} = \\beta_0 + \\beta_1 \\texttt{flipper\\_length\\_mm} + \\beta_2 \\texttt{bill\\_length\\_mm}\\)\n\n\nNote: M2 is nested in M1 - M1 has all the same predictors and then some.\n\n\n\n\n\n\nImportant\n\n\n\n\\(\\beta_1\\) in the first model is different from \\(\\beta_1\\) in the second model.\n\n\n\n\nExtra Sum-of-Squares\nIf M2 is nested within M1, :\n\nM1: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength} + \\beta_3 \\texttt{billdepth}\\)\nM2: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength}\\)\n\nThen the Extra sum of squares is defined as: \\[\nSS(\\hat\\beta_3 | \\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2) = S_1 - S_2\n\\]\nConvince yourself that S1 &gt; S2.\n\n\nSpecial Case: Corrected Sum-of-Squares\nWe’ve already seen this notation: \\[\nSSReg(\\hat\\beta_0) = n\\bar{\\underline y}^2\n\\] and \\[\nSSReg(corrected) = \\hat{\\underline\\beta}^TX^T\\underline y - n\\bar{\\underline y}^2 = S_1 - S_2\n\\] where \\(S_2\\) is the sum-of-squares for the null model!\n\n\nUnspecial Case: Correction doesn’t matter!\nConsider \\(S_{1c}\\) and \\(S_{2c}\\), the corrected versions of \\(S_1\\) and \\(S_2\\). Then\n\\[\\begin{align*}\nS_{1c} - S_{2c} = (S_2 - n\\bar{\\underline y}^2) - (S_1 - n\\bar{\\underline y}^2) = S_1 - S_2\n\\end{align*}\\]\nIn other words, the correction term doesn’t matter.\nThis is useful because R outputs the corrected versions.\n\n\nUnspecial Case: SSReg versus SSE doesn’t matter!\nConsider \\(SSE_1\\) and \\(SSE_2\\). Since SST is the same for both models,\n\\[\\begin{align*}\nSSE_2 - SSE_1 = (SST - S_1) - (SST - S_2) = S_2 - S_1\n\\end{align*}\\]\nNotice that the order is switched, which is fine.\n\n\nANOVA Tests for ESS\nConsider the models:\n\nM1: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength} + \\beta_3 \\texttt{billdepth}\\)\n\n\\(df_1 = 4\\)\n\nM2: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength}\\)\n\n\\(df_2 = 3\\)\n\n\nIf we choose \\(H_0: \\beta_3 = 0\\) in model 1, then \\[\n\\frac{S_1 - S_2}{(4 - 3)s^2} \\sim F_{1,\\nu}\n\\]\nwhere \\(s^2\\) is the error variance (MSE) in the larger model with degress of freedom \\(\\nu = df_1\\).\nThis is almost identical to the F-test for only one predictor (with one important difference).\n\n\nIn General\nIf M1 has \\(p\\) df, M2 has \\(q\\) df, and one is nested in the other, then \\(\\nu = \\max(p, q)\\) and\n\\[\n\\frac{S_1 - S_2}{(p - q)s^2} \\sim F_{|p-q|,\\nu}\n\\]\nNote that it doesn’t matter which is nested: \\(S_1 - S_2\\) has the same sign as \\(p-q\\), so it’s always positive.\n\n\nOmnibus Tests for Multiple Predictors\nSuppose we want to test if any bill measurement is useful.\n\nBill length and depth are highly correlated - marginal CIs won’t be valid.\nConfidence Regions are hard (and only work in 2D)\n\nInstead, we can use the ESS to test for a subset of predictors!\n\nM1: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength} + \\beta_2 \\texttt{billlength} + \\beta_3 \\texttt{billdepth}\\)\nM2: \\(\\texttt{bodymass} = \\beta_0 + \\beta_1 \\texttt{flipperlength}\\)\n\n\\(S_1 = S_2\\) is equivalent to \\(\\beta_2 = \\beta_3 = 0\\), and it accounts for their covariance!\nIf significant, then at least one of \\((\\beta_2, \\beta_3)\\) is not 0.\n\n\nIn R\n\nlibrary(palmerpenguins)\npeng &lt;- penguins[complete.cases(penguins),]\nm1 &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n    data = peng)\nm2 &lt;- lm(body_mass_g ~ flipper_length_mm,\n    data = peng)\nanova(m1, m2) |&gt; knitr::kable()\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n329\n50814912\nNA\nNA\nNA\nNA\n\n\n331\n51211963\n-2\n-397050.9\n1.285349\n0.2779392\n\n\n\n\n\n\n\nNext time\n\nWhen to check ESS\nHow to check all ESS\nWhat is R’s anova() function even doing??"
  },
  {
    "objectID": "L07-Exampless.html#review",
    "href": "L07-Exampless.html#review",
    "title": "7  L07: ESS Exampless",
    "section": "7.1 Review",
    "text": "7.1 Review\nFrom last time, we basically learned what the following means:\n\\[\n\\frac{SS(\\hat\\beta_{q+1}, ..., \\hat\\beta_p | \\hat\\beta_0, ... \\hat\\beta_q)}{(p-q)s^2} =\\frac{S_1 - S(\\hat\\beta_0) - (S_2 - S(\\hat\\beta_0))}{(p-q)s^2}\\sim F_{p-q, \\max(p, q)}\n\\] where \\(s^2\\) is the MSE calculated from the larger model.\nThis allows us to do a test for whether \\(\\beta_{q+1} = \\beta_{q+2} = ... = \\beta_p = 0\\).\nThe R code to do this test is as follows. In this code, we believe that the bill length and bill depth are strongly correlated, and thus we cannot trust the CIs that we get from summary(lm()) (we saw “Confidence Regions” in the slides and code for L05).\n\nnrow(peng)\n\n[1] 333\n\n\n\nlm1 &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n    data = peng)\nlm2 &lt;- lm(body_mass_g ~ flipper_length_mm,\n    data = peng)\nanova(lm2, lm1)\n\nAnalysis of Variance Table\n\nModel 1: body_mass_g ~ flipper_length_mm\nModel 2: body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm\n  Res.Df      RSS Df Sum of Sq      F Pr(&gt;F)\n1    331 51211963                           \n2    329 50814912  2    397051 1.2853 0.2779\n\n\nLet’s try and calculate these values ourselves in a couple different ways!\n\nanova(lm1)\n\nAnalysis of Variance Table\n\nResponse: body_mass_g\n                   Df    Sum Sq   Mean Sq   F value Pr(&gt;F)    \nflipper_length_mm   1 164047703 164047703 1062.1232 &lt;2e-16 ***\nbill_length_mm      1    140000    140000    0.9064 0.3418    \nbill_depth_mm       1    257051    257051    1.6643 0.1979    \nResiduals         329  50814912    154453                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom this model, SSE is 50814912 on 329 degrees of freedom. This is the same as the SSE in the output of anova(lm2, lm1).\n\nanova(lm2)\n\nAnalysis of Variance Table\n\nResponse: body_mass_g\n                   Df    Sum Sq   Mean Sq F value    Pr(&gt;F)    \nflipper_length_mm   1 164047703 164047703  1060.3 &lt; 2.2e-16 ***\nResiduals         331  51211963    154719                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAgain, the SSE of 51211963 matches what we saw in anova(lm2, lm1), and we have 331 degrees of freedom (as expected, the differences in degrees of freedom is 2).\nNote that the F-value in anova() is just the ratio of the MSEs, but this is not the case here. Instead, we need to calculate \\(s^2\\).\nWe can calculate \\(s^2\\) as the MSE for the larger model:\n\ns2 &lt;- 50814912/329\ns2\n\n[1] 154452.6\n\n\nAnd now we can calculate the F-value as expected:\n\n(51211963 - 50814912)/ (2 * s2)\n\n[1] 1.285349\n\n\n\nprint(1- pf(1.28539, 2, 329))\n\n[1] 0.2779278\n\n\nIt is left as an exercise to calculate these values based on matrix multiplication. I highly suggest trying it with and without correction factors to convince yourself that both of them work (and to convince yourself that you know what the correction factor is and why it’s necessary)."
  },
  {
    "objectID": "L07-Exampless.html#ess-algorithms",
    "href": "L07-Exampless.html#ess-algorithms",
    "title": "7  L07: ESS Exampless",
    "section": "7.2 ESS Algorithms",
    "text": "7.2 ESS Algorithms\nThe idea above is based on testing a subset of predictors for at least one significant coefficient. This is usually what we want.\nHowever, there are also times where we want to check all predictors one-by-one. This is much less common than the textbook may lead you to believe, but it still happens.\nThere are three ways to calculate the ESS for all predictors. They are very helpfully labelled Types I, II, and III.\n\nType I: Sequential Sum-of-Squares (with interactions)\n\nCheck \\(SS(\\hat\\beta_1|\\hat\\beta_0)\\)\nCheck \\(SS(\\hat\\beta_2|\\hat\\beta_0, \\hat\\beta_1)\\)\nCheck \\(SS(\\hat\\beta_2:\\hat\\beta_1|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\n\n\\(\\hat\\beta_2:\\hat\\beta_1\\) is an interaction term, which means we use a formula like y ~ x1 + x2 + x1*x2 (although we’ll learn why R uses different notation than this).\n\nCheck \\(SS(\\hat\\beta_3|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\nCheck all interactions between x1, x2, and x3,\n…\n\n\nThis will give us every possible sum-of-squares. This is very very dubious, and can lead to a multiple comparisons problem!\n\nType 2: Sequential Sum-of-Squares (R’s Default)\n\nCheck \\(SS(\\hat\\beta_1|\\hat\\beta_0)\\)\nCheck \\(SS(\\hat\\beta_2|\\hat\\beta_0, \\hat\\beta_1)\\)\nCheck \\(SS(\\hat\\beta_3|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\n…\n\n\nThis gives us an ordered sequence of “is it worth adding x1?”, “if we have x1, is it worth adding x2?”, etc. This is only meaningful if the predictors are naturally ordered (such as polynomial regression, see below).\n\nType 3: Last-entry sum-of-squares\n\nCheck \\(SS(\\hat\\beta_1|\\hat\\beta_0, \\hat\\beta_2, \\hat\\beta_3)\\)\nCheck \\(SS(\\hat\\beta_2|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_3)\\)\nCheck \\(SS(\\hat\\beta_3|\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)\\)\n\n\nThis checks whether adding predictor \\(x_i\\) is worth it, considering all other predictors are already in the model.\n\nType 2 ANOVA (Sequental Sum-of-Squares)\nBy default, R does sequential sum-of-squares. This is a very important fact to know!\nIn Types I and II, the order of the predictors matters. In fact, you cannot make any conclusions about the significance that doesn’t make reference to this fact.\n\n## Try changing the order to see how the significance changes!\nmylm &lt;- lm(mpg ~ qsec + disp + wt, data = mtcars)\nanova(mylm)\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nqsec       1 197.39  197.39  28.276 1.165e-05 ***\ndisp       1 615.12  615.12  88.116 3.816e-10 ***\nwt         1 118.07  118.07  16.914 0.0003104 ***\nResiduals 28 195.46    6.98                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mylm)$coef # No obvious connection to anova\n\n                 Estimate Std. Error     t value     Pr(&gt;|t|)\n(Intercept) 19.7775575655 5.93828659  3.33051585 0.0024420674\nqsec         0.9266492353 0.34209668  2.70873496 0.0113897664\ndisp        -0.0001278962 0.01056025 -0.01211109 0.9904228666\nwt          -5.0344097167 1.22411993 -4.11267686 0.0003104157\n\n\nHowever, there is at least one case where we do care about the order of the predictors. Consider polynomial regression, which we will return to later. For now, it is sufficient to know that we’re dealing with the model: \\[\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + ... + \\beta_{p-1}x_i^{p-1} + \\epsilon_i\n\\]In this model, notice that we only have one predictor \\(x\\), but we have performed non-linear transformations (HMWK: why is it important that the transformations are non-linear?).\nIn this case, a sequential SS setup makes quite a bit of sense. Given we have a linear model, is it worth making it quadratic? Given that we have a quadratic model, is it worth making it cubic? Given that we have a cubic model…\nIn the code below, I use the I() function (the I means identity) to make the polynomial model. The “formula” notation in R, y ~ x + z, has a lot of options. Including x^2, rather than I(x^2), makes R think we want to do one of the more fancy things, but the I() tells it that we want to literally square it. In the future, we’ll use a better way of doing this.\n\nx &lt;- runif(600, 0, 20)\ny &lt;- 2 - 3*x + 3*x^2 - 0.3*x^3 + rnorm(600, 0, 100)\nplot(y ~ x)\n\n\n\nmylm &lt;- lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))\nanova(mylm)\n\nAnalysis of Variance Table\n\nResponse: y\n           Df   Sum Sq  Mean Sq   F value Pr(&gt;F)    \nx           1 49560412 49560412 4726.8201 &lt;2e-16 ***\nI(x^2)      1 19661307 19661307 1875.1955 &lt;2e-16 ***\nI(x^3)      1  1150679  1150679  109.7459 &lt;2e-16 ***\nI(x^4)      1      661      661    0.0630 0.8018    \nI(x^5)      1      112      112    0.0107 0.9177    \nI(x^6)      1     6274     6274    0.5984 0.4395    \nResiduals 593  6217568    10485                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom the table above, we can clearly see that this should just be a cubic model (which is the true model that we generated). Try changing things around to see if, say, it will still detect an order 5 polynomial if if there’s no terms of order 3 or 4.\n\n\nA note on calculations\nTake a moment to consider the following. Suppose I checked the following two (Type II) ANOVA tables:\n\nanova(lm(mpg ~ disp, data = mtcars))\nanova(lm(mpg ~ disp + wt, data = mtcars))\n\nBoth tables will have the first row labelled “disp” and include its sum-of-squares along with the F-value. Do you expect these two rows to be the same?\nThink about it.\nThink a little more.\nWhat values do you expect to be used in the calculation?\nWhich sums-of-squares? Which variances?\nLet’s test it out:\n\nanova(lm(mpg ~ disp, data = mtcars))\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndisp       1 808.89  808.89  76.513 9.38e-10 ***\nResiduals 30 317.16   10.57                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm(mpg ~ disp + wt, data = mtcars))\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndisp       1 808.89  808.89 95.0929 1.164e-10 ***\nwt         1  70.48   70.48  8.2852  0.007431 ** \nResiduals 29 246.68    8.51                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThey’re different! As homework, find out where the F value for disp is coming from in both tables. (All required values are in the table, and the answer was stated earlier!)\nWith both the polynomial and the disp example, we see that the interpretation of the anova table is highly, extremely, extraordinarily dependent on which predictors we choose to include AND the order in which we choose to include them. So, yeah. Be careful.\n\n\nType III SS in R\nThere isn’t a built-in function to do this. To create this, we can either use our math (my preferred method) or test each one individually.\n\nanova(\n    lm(mpg ~ disp + wt, data = mtcars),\n    lm(mpg ~ disp + wt + qsec, data = mtcars)\n)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp + wt\nModel 2: mpg ~ disp + wt + qsec\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     29 246.68                              \n2     28 195.46  1     51.22 7.3372 0.01139 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(\n    lm(mpg ~ disp + qsec, data = mtcars),\n    lm(mpg ~ disp + wt + qsec, data = mtcars)\n)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp + qsec\nModel 2: mpg ~ disp + wt + qsec\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     29 313.54                                  \n2     28 195.46  1    118.07 16.914 0.0003104 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(\n    lm(mpg ~ wt + qsec, data = mtcars),\n    lm(mpg ~ disp + wt + qsec, data = mtcars)\n)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ wt + qsec\nModel 2: mpg ~ disp + wt + qsec\n  Res.Df    RSS Df Sum of Sq     F Pr(&gt;F)\n1     29 195.46                          \n2     28 195.46  1 0.0010239 1e-04 0.9904"
  },
  {
    "objectID": "L07-Exampless.html#modelling-strategies",
    "href": "L07-Exampless.html#modelling-strategies",
    "title": "7  L07: ESS Exampless",
    "section": "7.3 Modelling Strategies",
    "text": "7.3 Modelling Strategies\nIf you are in a situation where you think to yourself “my predictors are logically ordered and I want to check for the significance of all of them one-by-one”, you want Type II.\nIf you think “they’re not ordered but I want to check significance”, you might want to check the overall F test for all predictors and then check t-tests for individual parameters.\nIf you think “what would happen if each predictor were the last one I put in the model”, then you want Type III. I can’t think of a good situation for Type I - you’re pretty much guaranteed to have a multiple comparisons issue.\nI also want to call attention to the fact that all of these algorithms assume that you have a set of predictors that you already know you want to check. If you noticed, there are other predictors in the mtcars dataset that we did not consider!\nWe’ll slowly build up some intuition over time, but my advice for choosing which predictors to include is as follows:\n\nStart with a lot of plots.\nBased on the plots and your knowledge of the context, create a candidate set of predictors that you think will be the final model.\nCheck the model fit (p-values, residuals, etc).\nBased on your knowledge of the context, check significance of groups of predictors that you think are highly correlated.\nYour final model will be based on the tests for groups of (or individual) predictors that you suspect would be relevant.\n\nThe purpose of this method for selecting predictors is to minimize the number of p-values that you check. The ESS techniques that we learned today (especially for the bill length/depth, where our knowledge of the problem informed us of which predictors to check) are an important part of the modelling process, but there is more to learn!"
  },
  {
    "objectID": "L09-Serial_Correlation.html#serial-correlation-in-the-residuals",
    "href": "L09-Serial_Correlation.html#serial-correlation-in-the-residuals",
    "title": "8  L09: Cereal Correlation in the Residuals",
    "section": "8.1 Serial Correlation in the Residuals",
    "text": "8.1 Serial Correlation in the Residuals\n\nAssumptions and Definitions\nThe time of each observation is recorded, and they are equally spaced.\n\nIn other words, \\(x_1\\) is observed at time 1, \\(x_2\\) is observed at time 2, etc.\n\nSerial Correlation: \\(cor(\\epsilon_{t-1}, \\epsilon_t)\\ne 0\\)\n\nSerial Correlation is not causation.\n\nKnowledge of one gives you more knowledge of the other.\n\nSerial correlation can be negative\n\nExample: didn’t hit quota today, so big push tomorrow.\n\n\n\n\nVisualizing Serial Correlation in the Residuals\nR"
  },
  {
    "objectID": "L09-Serial_Correlation.html#durbin-watson",
    "href": "L09-Serial_Correlation.html#durbin-watson",
    "title": "8  L09: Cereal Correlation in the Residuals",
    "section": "8.2 Durbin-Watson",
    "text": "8.2 Durbin-Watson\n\nStrong Assumption about Correlation\nThe usual model: \\(Y = X\\underline{\\beta} + \\underline \\epsilon\\).\nAssume that \\(cor(\\epsilon_{t-1}, \\epsilon_t) = \\rho\\), \\(cor(\\epsilon_{t-2}, \\epsilon_t)= \\rho^2\\), \\(cor(\\epsilon_{t-3}, \\epsilon_t) = \\rho^3\\), etc.\n\nThe correlation is proportional to the distance in time.\n\nThis can be written as: \\[\n\\epsilon_t = \\rho\\epsilon_{t-1} + z_t\n\\] where \\(z_t\\sim N(0,\\sigma^2)\\). Note that \\(V(\\hat\\epsilon_t) = \\frac{\\sigma^2}{1 - \\rho^2}\\).\n\n\nThe Durbin-Watson test statistic\nAs usual, we find a quantity with a known distribution: \\[\nd = \\dfrac{\\sum_{s=2}^n(\\hat\\epsilon_s - \\hat\\epsilon_{s-1})^2}{\\sum_{s=1}^n\\hat\\epsilon_s^2}\\sim\\text{ some complicated distribution}\n\\]\n\nDistribution has a closed form, but I hate it.\n\\(d\\in [0, 4]\\), with \\(d=2\\) corresponding to the null.\nR will calculate the values for you.\n\nTextbook has pages and pages of tables. Textbook was written before iPhones existed.\n\n\nI’ll show an example of DW later.\n\n\nCautions with Durbin-Watson\n\nTests the hypotheses \\(H_a:\\;cor(\\epsilon_{t-s}, \\epsilon_t) = \\rho^s\\) versus not that.\n\nThere are many, many other \\(H_a\\). DW has low power for these situations.\n\nGraphical summaries will reveal strong patterns; patterns found by DW might not be worrisome. \nIt’s more p-values to look at. We want to minimize the number of p-values we look at."
  },
  {
    "objectID": "L09-Serial_Correlation.html#graphical-methods",
    "href": "L09-Serial_Correlation.html#graphical-methods",
    "title": "8  L09: Cereal Correlation in the Residuals",
    "section": "8.3 Graphical Methods",
    "text": "8.3 Graphical Methods\n\nEmpirical Autocorrelation\n\n\n\nWe can simply find the correlation between \\(\\hat \\epsilon_t\\) and \\(\\hat \\epsilon_{t-1}\\)!\nWe can do the same for \\(\\hat \\epsilon_t\\) and \\(\\hat \\epsilon_{t-2}\\).\n\netc.\n\n\n\n\n\n\n\n\n\n\n\n\nt\n\\(\\hat \\epsilon_t\\)\n\\(\\hat \\epsilon_{t-1}\\)\n\\(\\hat \\epsilon_{t-2}\\)\n\n\n\n\n2\n\\(\\hat \\epsilon_1\\)\nNA\nNA\n\n\n2\n\\(\\hat \\epsilon_2\\)\n\\(\\hat \\epsilon_1\\)\nNA\n\n\n3\n\\(\\hat \\epsilon_3\\)\n\\(\\hat \\epsilon_2\\)\n\\(\\hat \\epsilon_1\\)\n\n\n4\n\\(\\hat \\epsilon_4\\)\n\\(\\hat \\epsilon_3\\)\n\\(\\hat \\epsilon_2\\)\n\n\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\n\n\n\n\n\n\nThe ACF: Empirical Autocorrelations of lag \\(k\\)\n\n\nACF: AutoCorrelation Function\nThe x-axis shows the lag, the y axis shows the correlations\nThe plot on the right shows an example of time series data.\n\n\npar(mfrow = c(2, 1))\nplot(co2, main = \"Time series of CO2 data\")\nacf(co2, main = \"ACF of CO2 data (not residuals)\")\n\n\n\n\n\n\n\n\nACF isn’t ideal\nIn the model we saw for DW, \\[\n\\epsilon_t = \\rho\\epsilon_{t-1} + z_t\n\\] which means that \\[\n\\epsilon_t = \\rho(\\rho\\epsilon_{t-2} + z_{t-1}) + z_t\n\\]\nThe lag 2 correlation (the \\(\\rho^2\\) term) includes the lag 1 correlation!\n\n\nPartial Autocorrelations\n\n\nIf we extend the model to: \\[\n\\epsilon_t = \\rho_1\\epsilon_{t-1} + \\rho_2\\epsilon_{t-2} + z_t,\n\\] then \\(\\rho_2\\) is the correlation in the lag 2 terms, accounting for lag 1 terms!\nThis is the PACF, and it’s often much more useful.\nThe plot on the right shows a cyclic trend.\n\n\npar(mfrow = c(2, 1))\nplot(co2, main = \"Time series of CO2 data\")\npacf(co2, main = \"PACF of CO2 data (not residuals)\")\n\n\n\n\n\n\n\n\nDW, ACF, and PACF in practice\nMost of the time, just check the PACF.\n\nIf you see something, check ACF.\nIf you’re in a field that requires p-values, show them the DW statistic.\n\nOr use a non-parametric test…\n\n\n\n\nWhat to do if there is autocorrelation?\n\nCorrelation in the residuals might mean correlation in the \\(y_i\\)’s\n\nTry time series modelling!\n\nIf it’s simple (lag 1) autocorrlation, the data could potentially be transformed to remove the autocorrelation.\n\n\\(y_t - y_{t-1} = X\\underline \\beta\\).\nChange estimation to account for autocorrelation.\n\nIf it’s complicated, get a PhD student to do it for you."
  },
  {
    "objectID": "L09-Serial_Correlation.html#participation",
    "href": "L09-Serial_Correlation.html#participation",
    "title": "8  L09: Cereal Correlation in the Residuals",
    "section": "8.4 Participation",
    "text": "8.4 Participation\n\nQ1\nSerial correlation can be tested for any data set.\n\n\nTrue\nFalse\n\n\n\n\nQ2\nA non-significant result from the DW test means there is no autocorrelation in the residuals.\n\nTrue\n\n\nFalse\n\n\n\nQ3\nThe quantity \\(d = \\dfrac{\\sum_{s=2}^n(\\hat\\epsilon_s - \\hat\\epsilon_{s-1})^2}{\\sum_{s=1}^n\\hat\\epsilon_s^2}\\) is a statistic because:\n\nIt’s a value calculated from the data, possibly including information from outside the data.\nIt’s a value calculated from the data only, with no other information.\nIt’s a value calculated from the data only, with no other information, and has a known distribution.\nIt’s not a statistic since it’s not estimating a population parameter.\n\n\n\nQ4\nThe “partial” in PACF refers to:\n\nThe PACF plot only contains some of (partial) information.\nThe PACF evaluates the lag \\(k\\) correlation after controlling for lags 1 to \\(k-1\\).\nThe PACF is only evaluated for a portion of the data.\nThe PACF for a lag of \\(k\\) cannot use the first \\(k-1\\) data points (they are the NAs in the table).\n\n\n\nQ5\nWhich of the following is not an assumption of the DW test?\n\nThe time points are all equally spaced.\nThe correlation between two residuals is equal to \\(\\rho\\).\nThe further apart two residuals are, the less correlated they are.\nThere is no missing data.\n\n\n\nQ6\nAutocorrelation means that there are no possible insights into the data.\n\nTrue, the study was worthless.\nFalse, there are standard methods that will work for any situation.\nFalse, but we won’t get into the details in this course."
  },
  {
    "objectID": "L09-Serial_Correlation.html#non-parametric-test-for-autocorrelation",
    "href": "L09-Serial_Correlation.html#non-parametric-test-for-autocorrelation",
    "title": "8  L09: Cereal Correlation in the Residuals",
    "section": "8.5 Non-Parametric Test for Autocorrelation",
    "text": "8.5 Non-Parametric Test for Autocorrelation\n\nRuns\n\\(\\sum_{i=1}^n\\hat\\epsilon_i = 0\\), so some residuals are positive and some are negative.\nThe runs test just looks at the sign of the residuals. Consider the signs:\n+ + - + - - - - + + - + + + + \nThere are 7 runs in these residuals. Is this a lot of runs?\n\n\nDefining “A Lot Of Runs”\np-value: Probability of a result at least as extreme as the one obtained, under the null hypothesis.\n\nNull: random +’s and -’s.\n\nFor small numbers, we can look at all sequences of +’s and -’s and count the runs!\n\nP(7 or more runs) is an upper tailed test (-ive autocorrelation)\nP(7 or fewer runs) is a test for +ive autocorrelation\n\n\n\nLarge Numbers: Of course it’s Normal!\nGiven \\(n_1\\) +’s and \\(n_2\\) -’s, the mean and variance of the number of runs is: \\[\n\\mu = \\frac{2n_1n_2}{n_1 + n_2} + 1\\text{, and }\\sigma^2 = \\frac{2n_1n_2(2n_1n_2 - n_1 - n_2)}{(n_1+n_2)^2(n_1 + n_2 - 1)}\n\\]\nIn the actual distribution, \\(P(runs\\le \\mu) = P(runs\\le \\mu -1/2) = P(runs &lt; \\mu + 1/2)\\).\nIn the normal distribution this isn’t true, so we apply a correction factor:\n\nLower-tailed test: \\(runs\\sim N(\\mu + 1/2, \\sigma^2)\\)\nUpper-tailed test: \\(runs\\sim N(\\mu - 1/2, \\sigma^2)\\)\nTwo-tailed test: \\(runs\\sim N(\\mu, \\sigma^2)\\) and we hope it averages out.\n\n\n\nExample\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/SerialCorrelation\")"
  },
  {
    "objectID": "L10-Hat-Resid_Plots-Cook.html#le-chapeau",
    "href": "L10-Hat-Resid_Plots-Cook.html#le-chapeau",
    "title": "9  L10: The Hat Matrix",
    "section": "9.1 Le Chapeau",
    "text": "9.1 Le Chapeau\n\nThe Hat Matrix\n\\[\nH = X(X^TX)^{-1}X^T\n\\]\nRecall: The hat matrix projects \\(Y\\) onto \\(\\hat Y\\), based on \\(X\\).\n\n\\(\\hat Y = HY\\)\n\n\\(\\hat Y_i = h_{ii} Y_i + \\sum_{j\\ne i}h_{ij}Y_j\\)\n\n\n\n\nVariance-Covariance matrix of \\(\\hat{\\underline\\epsilon}\\)\nJust like \\(\\beta_0\\) and \\(\\beta_1\\), each sample results in different \\(\\underline{\\hat\\epsilon}\\).\nAcross samples, we have: \\[\n\\underline{\\hat\\epsilon} - E(\\underline{\\hat\\epsilon}) = (I-H)(Y-X\\underline{\\beta}) = (I-H)\\underline{\\epsilon}\n\\] and therefore: \\[\\begin{align*}\nV(\\underline{\\hat\\epsilon}) &= E([\\underline{\\hat\\epsilon} - E(\\underline{\\hat\\epsilon})][\\underline{\\hat\\epsilon} - E(\\underline{\\hat\\epsilon})]^T)\\\\\n&= [I-H]E(\\underline{\\epsilon}\\underline{\\epsilon}^T)[I-H]^T\\\\\n&= [I-H]\\sigma^2[I-H]\\\\\n&= [I-H]\\sigma^2\n\\end{align*}\\] where we used the idempotency and symmetry of \\(I-H\\).\n\n\nThe variance of a residual\nGiven that \\(V(\\underline{\\hat\\epsilon}) = (I-H)\\sigma^2\\), \\[\nV(\\hat\\epsilon_i) = (1-h_{ii})\\sigma^2\n\\]\n\nThe variance of the residual depends on how much \\(Y_i\\) influences it’s own estimate.\n\nHigh influence = low variance.\n\n\nThe correlation between residuals is: \\[\n\\rho_{ij} = \\frac{Cov(\\hat\\epsilon_i, \\hat\\epsilon_j)}{\\sqrt{V(\\hat\\epsilon_i)V(\\hat\\epsilon_j)}} = \\frac{-h_{ij}}{\\sqrt{(1-h_{ii})(1-h_{jj})}}\n\\]\n\nThe covariance is negative! A large residual tells us there are small residuals.\n\n“Large” and “small” are relative\n\n\n\n\nMore H Facts\n\n\\(SS(\\hat{\\underline\\beta}) = \\hat{\\underline\\beta}^TX^TY = \\hat Y^TY = Y^TH^TY = Y^TH^THY = \\hat Y^T\\hat Y\\)\n\nWe used the facts \\(H^T= H^TH\\) and \\(\\hat Y = HY\\).\n\n\\(\\sum_{i=1}^nV(\\hat Y_i) = trace(H\\sigma^2) = p\\sigma^2\\)\n\n\\(p\\) is the number of parameters.\nProof is part of the assignment\n\n\\(H1 = 1\\) if the model contains a \\(\\beta_0\\) term.\n\n\\(1\\) is a column of 1s, not identity matrix.\nProof on next slide.\n\\(h_{ii} = 1 - \\sum_{j\\ne i}h_{ij}\\)\n\nNote that \\(h_{ij}\\in[-1, 1]\\).\n\n\n\n\n\nProof that \\(H1 = 1\\)\nNote that \\(HX = X\\) (as proven on A1).\n\nThe first column of \\(X\\) is all ones (\\(\\beta_0\\) term).\n\n\\([X]_{i1} = 1\\)\n\nTherefore \\([HX]_{i1}\\) is a column of ones.\n\nEvery row of \\(H\\) times the column of 1s in \\(X\\) results in a column of ones.\n\n\\([HX]_{i1}\\) is every row of \\(H\\) times the first column of \\(X\\).\n\nThe first column of \\(X\\) is 1s, which is equal to the first column of \\(HX\\), which is \\(H\\) times a column of ones.\nIn other words, \\(H1 = 1\\)"
  },
  {
    "objectID": "L10-Hat-Resid_Plots-Cook.html#studentized-residuals",
    "href": "L10-Hat-Resid_Plots-Cook.html#studentized-residuals",
    "title": "9  L10: The Hat Matrix",
    "section": "9.2 Studentized Residuals",
    "text": "9.2 Studentized Residuals\n\nInternally Studentized (not ideal)\nHow do you measure the size a residual?\nDivide by the variance, of course!\nWe know that \\(V(\\hat \\epsilon_i) = (1-h_{ii})\\sigma^2\\), and \\[\ns^2 = \\frac{\\sum_{y=1}^n(y_i-\\hat y_i)^2}{n-p} = \\frac{SSE}{df_E} = MSE\n\\] is an estimate of \\(\\sigma^2\\). Then, \\[\nr_i = \\frac{\\hat\\epsilon_i}{\\sqrt{s^2(1-h_{ii})}}\n\\] is called the internally studentized residual.\n\n\n“Internally” “Studentized”\nNote that \\[\ns^2 = \\frac{\\sum_{i=1}^n(y_i-\\hat y_i)^2}{n-p} = \\frac{\\sum_{i=1}^n(\\hat\\epsilon_i)^2}{n-p}\n\\] and therefore \\[\nr_i = \\frac{\\hat\\epsilon_i}{\\sqrt{s^2(1-h_{ii})}} = \\frac{\\hat\\epsilon_i}{\\sqrt{(\\hat\\epsilon_i^2 + \\sum_{j\\ne i}\\hat\\epsilon_j^2)(1-h_{ii})/(n-p)}}\n\\]\n\nIf \\(\\hat\\epsilon_i\\) is large, then \\(s^2\\) is large.\n\nIf \\(s^2\\) is large, then \\(s_i\\) is small!\n“Internally”: the variance includes the residual of interest.\n\n“Studentized” because Student made it popular.\nOften called “standardized”.\n\n\n\nExternally Studentized Step 1\nLike adding/removing predictors and checking the cahnge in SS, we can add/remove points!\n\nCalculate SS\nRemove the first point. Estimate the model again and calculate new SS.\nAdd the first point back, remove the second. Estimate the model again and check the SS.\n\nFor each point, we have an estimate of the variance without itself.\n\n\nExternally Studentized Step 2\nSkipping the math, \\[\ns^2_{(i)} = \\frac{(n-p)s^2 - \\hat\\epsilon_i^2/(1-h_{ii})}{n-p-1}\n\\] is the variance of the residuals without observation \\(i\\).\n\nThe influence tells us how much a point influenced the model\n\nWe can see what happened without it\nNo need to re-estimate the model!!!\n\n\n\n\nExternally Studentized Residuals\nUse \\(s^2_{(i)}\\) in place of \\(s^2\\). \\[\nt_i = \\frac{\\hat\\epsilon_i}{\\sqrt{s_{(i)}^2(1-h_{ii})}} \\sim t_{n-p-1}\n\\]\n\nFollows a \\(t\\) distribution!\n\nLarger than 2 is suspect, 3 is definitely an outlier!\n\nA large \\(t_i\\) is large relative to the other residuals\nUsually just called “Studentized”\n\nMost software uses Studentized residuals for plots/diagnostics!"
  },
  {
    "objectID": "L10-Hat-Resid_Plots-Cook.html#cooks-distance",
    "href": "L10-Hat-Resid_Plots-Cook.html#cooks-distance",
    "title": "9  L10: The Hat Matrix",
    "section": "9.3 Cook’s Distance",
    "text": "9.3 Cook’s Distance\n\nBetter Measures of Influence\nThe hat matrix is intepreted as influence, but it has problems.\n\n\\(y_i\\)’s influence on it’s own prediction,\n\ngiven all other points.\n\n\\(0 \\le h_{ii} \\le 1\\)\n\nWhat is a “big” influence?\n\nHow do you explain \\(h_{ii}\\) to nonstatisticians?\n\nA better measure is how much the predicted value changes with/without the obs.\n\n\nCooks Distance: Change in \\(\\hat y_i\\).\n\\[\nD_i = \\frac{\\sum_{i=1}^n(\\hat y_i - \\hat y_{(i)})^2}{ps^2}\n\\]\n\n\\(\\hat y_i\\) is the predicted value of \\(y_i\\) when all data are considered.\n\\(\\hat y_{(i)}\\) is the predicted value of \\(y_i\\) when observation \\(i\\) is removed.\n\\(s^2\\) is the MSE of the model with all of the data.\n\\(p\\) is the number of parameters\n\n\\(D_i\\) decreases as \\(p\\) increases!\n\n\nAgain, this would involve re-fitting the model \\(n\\) time (one for each obs).\n\n\nCook’s Distance: Alternate Form\n\\[\nD_i = \\left[\\frac{\\hat \\epsilon_i}{\\sqrt{s^2(1-h_{ii})}}\\right]^2\\left[\\frac{h_{ii}}{1 - h_{ii}}\\right]\\frac{1}{p} = r_i^2\\frac{\\text{variance of $i$th predicted value}}{\\text{variance of $i$th residual}}\\frac{1}{p}\n\\]\n\nAgain, use \\(H\\) rather than re-fitting the model.\nCook’s distance is a modification of the internally studentized residual.\n\nVariances are based on same “deletion” idea as studentized.\n\nRatio of Variances!\n\n\\(F\\) distribution, mean approaches 1 for large values of \\(n\\)\n\nCooks Distance of larger than 1 is suspect.\n\n\n\n\n\nNext Class\nPlots!"
  },
  {
    "objectID": "L10-Hat-Resid_Plots-Cook.html#participation",
    "href": "L10-Hat-Resid_Plots-Cook.html#participation",
    "title": "9  L10: The Hat Matrix",
    "section": "9.4 Participation",
    "text": "9.4 Participation\n\nQ1\nWhich does not describe a residual?\n\nThe observed difference between the measured value \\(y_i\\) and the one we predict with \\(\\hat y_i = X\\hat{\\underline\\beta}\\).\nGiven the true relationship \\(Y = X\\underline\\beta\\), residuals are deviations that cannot be observed.\nErrors that should be fixed.\n\n\n\nQ2\nWhich of the following is not true about the hat matrix (\\(H = X(X^TX)^{-1}X^T\\))?\n\n\\((I-H)(I-H)^T = (I-H)\\)\n\\(\\hat{\\underline\\beta} = (X^TX)^{-1}X^TY = X^{-1}HY\\)\n\\(h_{ii} = 1 - \\sum_{j\\ne i}h_{ij}\\)\n\\(H(I-H) = 0\\)\n\n\n\nQ3\nSuppose that \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\).\nWhich of the following statements is false?\n\n\\(V(\\hat\\epsilon_i) = (1 - h_{ii})\\sigma^2\\)\n\\(V(\\epsilon_i) = \\sigma^2\\)\n\\(V(Y_i) = \\sigma^2\\)\n\\(V(\\hat Y_i) \\ge V(\\hat \\epsilon_i)\\)\n\n\n\nQ4\nThe difference between internally and externally studentized residuals is:\n\n“Internal” uses all of the observations, “external” uses all of the observations except \\(i\\).\n“External” uses all of the observations, “internal” uses all of the observations except \\(i\\).\n\n\n\nQ5\nInternally studentized residuals follow a \\(t\\) distribution.\n\nTrue - they are a normal r.v. divided by a chi-square r.v.\nFalse - they are a normal r.v. divided by a chi-square r.v., but the two are not independent.\nFalse - They include a normal and a chi-square, but the \\(h_{ii}\\) make this not follow distributional assumptions.\nTrue - internally studentized residuals are a small modification to externally studentized residuals, which follow a \\(t\\) distribution.\n\n\n\nQ6\nWhich of the following is not useful for detecting outliers?\n\nStandardized residuals\nStudentized residuals\nCook’s distance\n\\(h_{ii}\\)"
  },
  {
    "objectID": "L11-Admin_Slides.html#participation-questions",
    "href": "L11-Admin_Slides.html#participation-questions",
    "title": "10  L11: The Hat Matrix 2",
    "section": "10.1 Participation Questions",
    "text": "10.1 Participation Questions\n\nQ1\nWhich of these does not give the diagonal of the hat matrix?\n\nhatvalues(mylm)\ndiag(X %*% solve(t(X) %*% X) %*% t(X))\naugment(mylm)$hat\n\n\n\nQ2\nRemoving an outlier will not change:\n\nThe diagonal of the hat matrix.\nThe off-diagonal of the hat matrix.\nSST\nAll of the above will change if we remove an outlier.\n\n\n\nQ3\nA large residual means large influence.\n\nTrue\nFalse\n\n\n\nQ4\nAll entries in the hat matrix are between -1 and 1.\n\nTrue\nFalse\n\n\n\nQ5\nWhich plot corresponds to the model with the most predictors?\n(All models are nested.)\n\n\n\n\n\n\n\nQ6\nIn the output of augment() in the broom package, the .sigma column refers to:\n\nThe MSE of the model.\nThe MSE of the model if it were fit without the observation in that row.\nThe variance of that residual, as calculated by \\(V(\\hat\\epsilon_i) = (1-h_{ii})s^2\\).\nThe variance of that residual, as calculated by \\(V(\\hat\\epsilon_i) = (1-h_{ii})s_{(i)}^2\\)."
  },
  {
    "objectID": "L12-Extra_Topics.html#standardizing-x",
    "href": "L12-Extra_Topics.html#standardizing-x",
    "title": "11  L12: Extra Topics",
    "section": "11.1 Standardizing \\(X\\)",
    "text": "11.1 Standardizing \\(X\\)\n\nMean-Centering\nConsider \\(y_i = \\beta_0 + \\beta_1 x'_i\\), where \\(x'_i\\) are the “centered” versions of \\(x_i\\): \\[\nx'_i = x_i - \\bar x\n\\]\nThen \\(\\bar{x'} = 0\\) and the coefficient estimates become: \\[\\begin{align*}\n\\hat\\beta_0 &= \\bar y - \\hat\\beta_1\\bar{x'} = \\bar y\\\\\n&\\text{and}\\\\\n\\hat\\beta_1 &= \\frac{S_{XX}}{S_{YY}} = \\frac{\\sum_{i=1}^n(x_i' - \\bar{x'})^2}{\\sum_{i=1}^n(x_i' - \\bar{x'})(y_i - \\bar{y})} = \\frac{\\sum_{i=1}^nx_i'^2}{\\sum_{i=1}^nx_i'(y_i - \\bar{y})}\n\\end{align*}\\]\n\n\nMean-Centering and Covariance\n\n\nFor un-centered data: \\[\\begin{align*}\nV(\\hat{\\underline\\beta}) &= (X^TX)^{-1}\\sigma^2,\\\\\n\\text{where }(X^TX)^{-1} &= \\frac{1}{nS_{XX}}\\begin{bmatrix}\\sum x_i^2 & -n\\bar x\\\\-n \\bar x & n\\end{bmatrix}\n\\end{align*}\\] Note also that \\(S_{x'x'} = \\sum_{i=1}^n{x'}_i^2\\), so \\[\\begin{align*}\nV(\\hat{\\underline\\beta}^c) &= \\frac{\\sigma^2}{nS_{X'X'}}\\begin{bmatrix}\\sum {x'}_i^2 & 0\\\\0 & n\\end{bmatrix}\\\\\n& = \\sigma^2\\begin{bmatrix}n^{-1} & 0 \\\\ 0 & (\\sum_{i=1}^n{x'}_i^2)^{-1}\\end{bmatrix}\n\\end{align*}\\]\n\\(\\implies\\) no covariance!!!\n\nSimulations with same data, but one uses centered data (code in L02 Rmd).\n\n\n\n\n\n\n\n\n\nComments on \\(V(\\hat{\\underline\\beta})\\)\n\\[\nV(\\hat{\\underline\\beta}^c) = \\sigma^2\\begin{bmatrix}n^{-1} & 0 \\\\ 0 & (\\sum_{i=1}^n{x'}_i^2)^{-1}\\end{bmatrix}\n\\]\n\n\\(V(\\hat\\beta_0) = \\sigma^2/n \\implies sd(\\hat\\beta_0) = \\sigma/\\sqrt{n}\\).\n\nThe t-test for significance of \\(\\beta_0\\) is just a hypothesis test for \\(\\bar y = 0\\). \n\nNote that \\(\\underline y\\) hasn’t changed, so \\(\\hat{\\underline\\epsilon}\\) and \\(\\sigma^2\\) are unchanged.\n\\(V(\\hat\\beta_0) = \\sigma^2/\\sum_{i=1}^n{x'}_i^2\\) isn’t all that interesting…\n\n\n\nStandardizing \\(\\underline x\\)\nIn addition to mean-centering, divide by the sd of \\(\\underline x\\): \\[\nz_i = \\frac{x_i - \\bar x}{\\sqrt{S_{XX}/(n-1)}}\n\\]\nThen \\(\\bar z = 0\\) and \\(sd(z) = 0 \\implies S_{ZZ} = n-1\\).\nIt can be shown that: \\[\nV(\\hat{\\underline\\beta}^s) = \\sigma^2\\begin{bmatrix}n^{-1} & 0 \\\\ 0 & (n-1)^{-1}\\end{bmatrix}\n\\]\n\n\\(\\underline x\\) doesn’t matter!!!\n\n\n\nStandardizing in Multiple Linear Regression\nSuppose we standardize each column of \\(X\\) (except the first).\nSeveral things happen:\n\nAll predictors are now in units of standard deviations!!!\n\nCoefficients are directly comparable!\n\nCovariances disappear!!!\n\nStandardizing doesn’t hurt and can often help \\(\\implies\\) it’s almost always worth it!"
  },
  {
    "objectID": "L12-Extra_Topics.html#general-linear-hypothesis",
    "href": "L12-Extra_Topics.html#general-linear-hypothesis",
    "title": "11  L12: Extra Topics",
    "section": "11.2 General Linear Hypothesis",
    "text": "11.2 General Linear Hypothesis\n\nDiet vs. Exercise\nWhich is more important for weight loss?\nWe can set this up in a linear regression framework: \\[\n\\text{Loss}_i = \\beta_0 + \\beta_1\\text{CaloriesConsumed}_i + \\beta_2\\text{ExercisesMinutes}_i\n\\] where we assume CaloriesConsumed and ExerciseMinutes are standardized.\nOur question about the importance of diet versus exercise becomes a hypothesis test: \\[\nH_0:\\; \\beta_1 = \\beta_2\\text{ vs. }H_a:\\; \\beta_1 \\ne \\beta_2\n\\] Alternatively, the null can be written as \\(\\beta_1 - \\beta_2 = 0\\).\n\n\nLinearly Independent Hypotheses\nIn some cases, we might have a collection of hypotheses. For ANOVA: \\[\nH_0:\\; \\beta_2 - \\beta_1 = 0,\\; \\beta_3 - \\beta_2 = 0,\\; \\beta_4 - \\beta_3 = 0,\\...,\\; and\\; \\beta_{p-1} - \\beta_{p-2} = 0\n\\] These hypotheses are linearly indepenent. To see why, we can write them in matrix form: \\[\n\\begin{bmatrix}\n0 & -1 & 1 & 0 & 0 & ...\\\\\n0 & 0 & -1 & 1 & 0 & ...\\\\\n0 & 0 & 0 & -1 & 1 & ...\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & ...\\\\\n\\end{bmatrix}\\hat{\\underline\\beta} = \\underline 0\n\\] where none of the rows are linear combinations of the others.\nWe’ll use the notation \\(C\\underline\\beta = \\underline 0\\).\n\n\nLinearly Independent Hypotheses\nThe \\(C\\) matrix can be row-reduced to the hypotheses \\(\\beta_i=0\\;\\forall i&gt;0\\). In this case, our hypothesized model is: \\[\nY_i = \\beta_0 + \\underline\\epsilon\n\\]\nWe have reduced \\(Y = X\\underline\\beta + \\underline\\epsilon\\) to \\(Y = Z\\underline\\alpha + \\underline\\epsilon\\), where \\(\\underline\\alpha = (\\beta_0)\\) and \\(Z\\) is a column of ones.\n\n\nLinearly Dependent Hypotheses\nConsider the model \\(Y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{11}x_1^2 + \\underline\\epsilon\\) and the hypotheses: \\[\nH_0:\\; \\beta_{11} = 0,\\ \\beta_1 - \\beta_2 = 0,\\; \\beta_1 - \\beta_2 + \\beta_3 = 0,\\; 2\\beta_1 - 2\\beta_2 + 3\\beta_3 = 0\n\\] We can write this as: \\[\n\\begin{bmatrix}\n0 & 0 & 0  & 1\\\\\n0 & 1 & -1 & 0\\\\\n0 & 1 & -1 & 1\\\\\n0 & 2 & -2 & 3\n\\end{bmatrix}\\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\\\\\beta_{11}\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\\\0\\\\0\\end{bmatrix}\n\\] With a little work, we can show that this reduces to the model: \\[\nY = \\beta_0 + \\beta(x_1 + x_2) + \\underline\\epsilon \\Leftrightarrow Y = Z\\underline\\alpha + \\underline\\epsilon\n\\]\n\n\nTesting General Linear Hypotheses\nConsider an arbitrary matrix for \\(C\\) (not linearly dependent), such that we can row-reduce \\(C\\) to \\(q\\) linearly independent hypotheses.\n\nFull Model\n\n\\(SS_E = Y^TY - \\hat{\\underline\\beta}^TX^TY\\) on \\(n-p\\) df.\n\nHypothesized Model\n\n\\(SS_W = Y^TY - \\hat{\\underline\\alpha}^TZ^TY\\) on \\(n-p-q\\) df.\n\n\nFrom these, we get: \\[\n\\left(\\frac{SSW-SSE}{q}\\right)/\\left(\\frac{SSE}{n-p}\\right) \\sim F_{q, n-p}\n\\] In other words, we test whether the restrictions significantly change the \\(SS_E\\)!"
  },
  {
    "objectID": "L12-Extra_Topics.html#generalized-least-squares",
    "href": "L12-Extra_Topics.html#generalized-least-squares",
    "title": "11  L12: Extra Topics",
    "section": "11.3 Generalized Least Squares",
    "text": "11.3 Generalized Least Squares\n\nMain Idea\nWhat if the variance of \\(\\epsilon_i\\) isn’t the same for all \\(i\\)?\nIn other words, \\(V(\\underline\\epsilon) = V\\sigma^2\\) for some matrix \\(V\\).\n\nThe structure of \\(V\\) changes how we approach this.\n\nWeighted least squares: \\(V\\) is diagonal.\nGeneralized: \\(V\\) is symmetric and positive-definite, but otherwise arbitrary.\n\n\n\n\nTransforming the Instability Away\nIn the model \\(Y = X\\underline\\beta + \\underline\\epsilon\\), we want \\(V(Y) = I\\sigma^2\\), but we have \\(V(Y) = V\\sigma^2\\)\nSince \\(V\\) is symmetric and positive-definite, we can find a matrix \\(P\\) such that: \\[\nP^TP = PP = P^2 = V\n\\]\nWe can pre-multiply the model by \\(P^{-1}\\) so that \\(V(P^{-1}Y) = V^{-1}V\\sigma^2 = I\\sigma^2\\): \\[\nP^{-1}Y = P^{-1}X\\underline\\beta + P^{-1}\\underline\\epsilon \\Leftrightarrow Z = Q\\underline\\beta + \\underline f\n\\]\n\n\nGeneralized Least Squares Results\n\\[\\begin{align*}\n\\underline f^T\\underline f &= \\underline\\epsilon^TV^{-1}\\underline\\epsilon = (Y - X\\underline\\beta)^TV^{-1}(Y - X\\underline\\beta)\\\\\n\\hat{\\underline\\beta} &= (X^TV^{-1}X)^{-1}X^TV^{-1}Y\\\\\nSS_T &= \\hat{\\underline\\beta}^TQ^TZ = Y^TV^{-1}X(X^TV^{-1}X)^{-1}X^TV^{-1}Y\\\\\nSST &= Z^TZ = Y^TV^{-1}Y\\\\\n\\hat Y &= X\\hat{\\underline\\beta}\\\\\n\\hat{\\underline f} &= P^{-1}(Y-\\hat Y) = P^{-1}(I-X(X^TV^{-1}X)^{-1}X^TV^{-1})Y\n\\end{align*}\\]\nMost things are just switching \\(Y\\) with \\(P^{-1}Y\\), etc., except one…\n\n\nOLS when you should have used GLS\nSuppose the true model has \\(V(\\underline\\epsilon) = V\\sigma^2\\).\nLet \\(\\hat{\\underline\\beta}_O\\) be the estimate of \\(\\underline\\beta\\) if we were to fit with Ordinary Least Squares. Then:\n\n\\(E(\\hat{\\underline\\beta}_O) = \\underline\\beta\\)\n\\(V(\\hat{\\underline\\beta}_O) = (X^TX)^{-1}X^TVX(X^TX)^{-1}\\sigma^2\\)\n\nThe OLS estimate is unbiased, but has a much higher variance!\n\n\nChoosing \\(V\\)\n\nFor serially correlated data, \\(V_{ii} = 1\\) and \\(V_{ij} = \\rho^{|i-j|}\\)\n\nThis is choosing \\(V\\) based on model assumptions!\n\\(\\rho\\) must be estimated ahead of time.\n\nIf we have repeteated \\(x\\)-values, we can use the estimated variance from there.\n\nChoosing \\(V\\) based on the data\n\nIn a controlled experiment, where we have known weights for different \\(x\\)-values\n\nE.g., more skilled surgeons, machine age."
  },
  {
    "objectID": "L12-Extra_Topics.html#participation-questions",
    "href": "L12-Extra_Topics.html#participation-questions",
    "title": "11  L12: Extra Topics",
    "section": "11.4 Participation Questions",
    "text": "11.4 Participation Questions\n\nQ1\nWhich of the following will result in no correlation between \\(\\beta_0\\) and \\(\\beta_1\\)?\n\nCentering\nStandardizing\nBoth centering and standardizing\nNone of the above\n\n\n\nQ2\nWhat’s the primary reason for standardizing the predictors?\n\nRemove correlation between the \\(\\hat\\beta\\)s\nMake it so that the variance is not a function of the \\(X\\)-values.\nEnsure that the values of \\(\\hat\\beta\\) are comparable.\nMake it so that general linear hypotheses are possible.\n\n\n\nQ3\nIn a general linear hypothesis, \\(q\\) is the rank of the \\(C\\) matrix in \\(C\\underline\\beta = \\underline 0\\).\n\nTrue\nFalse\n\n\n\nQ4\nGeneralized Least Squares requires strong assumptions about the matrix \\(V\\).\n\nTrue\nFalse\n\n\n\nQ5\nIgnoring correlation/unequal variance in \\(\\underline\\epsilon\\) will lead to a biased estimate of \\(\\underline\\beta\\)\n\nTrue\nFalse\n\n\n\nQ6\nWhat do you expect the hat matrix to be for GLS?\n\n\\(X(X^TX)^{-1}X^TY\\)\n\\(X(X^TV^{-1}X)^{-1}X^TV^{-1}Y\\)\n\\(XV^{-1}(X^TV^{-1}X)^{-1}X^TV^{-1}Y\\)\n\\(XV^{-T}(X^TV^{-1}X)^{-1}X^TV^{-1}Y\\)"
  },
  {
    "objectID": "L13-Wrong_Model.html#the-wrong-model",
    "href": "L13-Wrong_Model.html#the-wrong-model",
    "title": "12  L13: Getting the Wrong Model",
    "section": "12.1 The Wrong Model",
    "text": "12.1 The Wrong Model\n\nThe Right Model?\nRecall: All models are wrong, some are useful!\nBut how wrong can a model be while still being useful?\n\nThis is an extraordinarily challenging philosophical question.\nWe will touch on a very small part of it\n\n\n\nThe Wrong Predictors\nSo far, we’ve talked about a model of the form \\(Y=X\\underline\\beta + \\underline\\epsilon\\).\n\n\\(E(\\hat{\\underline\\beta}) = E((X^TX)^{-1}X^TY) = (X^TX)^{-1}X^TX\\underline\\beta = \\underline\\beta\\)\n\nHowever, what if we are missing some predictors?\nWhat if the true model is \\(Y=X\\underline\\beta + X_2\\underline\\beta_2 + \\underline\\epsilon\\)? \\[\\begin{align*}\nE(\\hat{\\underline\\beta}) &= E((X^TX)^{-1}X^TY)\\\\\n& = (X^TX)^{-1}X^T(X\\underline\\beta + X_2\\underline\\beta_2) \\\\\n& = (X^TX)^{-1}X^TX\\underline\\beta + (X^TX)^{-1}X^TX_2\\underline\\beta_2 \\\\\n&= \\underline\\beta+ (X^TX)^{-1}X^TX_2\\underline\\beta_2\\\\\n&= \\underline\\beta + A\\underline\\beta_2\n\\end{align*}\\]\n\n\nBias due to wrong predictors\nThe bias of an estimator is: \\[\n\\text{Bias}(\\hat{\\underline\\beta}) = \\underline\\beta - E(\\hat{\\underline\\beta})\n\\]\nFor the case where \\(Y = X\\underline\\beta + X_2\\underline\\beta_2 +\\underline\\epsilon\\), \\[\n\\text{Bias}(\\hat{\\underline\\beta}) = \\underline\\beta - (\\underline\\beta + A\\underline\\beta_2) = A\\underline\\beta_2\n\\]\n\n\nExpected Mean Square\nSee text.\nUses the identity: For an \\(n\\times n\\) matrix \\(Q\\) and \\(n\\times 1\\) random vector \\(Y\\) with variance \\(V(Y)=\\Sigma\\), \\[\nE(Y^TQY) = (E(Y))^TQE(Y) + trace(Q\\Sigma)\n\\]\nThis may be useful for a future assignment question (will notify if you need it), but for now I’m going to explore this via simulation in the Rmd.\n\n\nSummary\n\nChoosing the wrong set of predictors can affect the model!"
  },
  {
    "objectID": "L13-Wrong_Model.html#participation-questions",
    "href": "L13-Wrong_Model.html#participation-questions",
    "title": "12  L13: Getting the Wrong Model",
    "section": "12.2 Participation Questions",
    "text": "12.2 Participation Questions\n\nQ1\nChoosing a model is easy.\n\nTrue\nFalse\n\n\n\nQ2\nWhich statement is false?\n\nIf you have the correct subset of predictors, you will have an unbiased model.\nIf you do not, your model is likely biased.\nIf you’re only interested in the estimate of one predictor, then it’s okay if the other estimates are biased.\nAll of the above are true.\n\n\n\nQ3\nProxy measures of important predictors help remove bias, but the coefficient has no relation to the data generating process.\n\nTrue\nFalse"
  },
  {
    "objectID": "L14-Nonlinear.html#non-linear-relationships",
    "href": "L14-Nonlinear.html#non-linear-relationships",
    "title": "13  L14: Non-Linear Relationships with Linear Models",
    "section": "13.1 Non-Linear Relationships",
    "text": "13.1 Non-Linear Relationships\n\nArbitrarily Shaped Functions\n\n\nThe plot on the right is the function: \\[\ny = 2 + \\frac{1}{5}x^2 - 8\\log(x) - 0.005x^3 + 20\\sin\\left(\\frac{x}{2}\\right) + \\epsilon\n\\]\n\n\n\n\n\n\n\n\nThe twist: The fitted line is just a polynomial model: \\(y = \\beta_0 + \\sum_{j=1}^{12}\\beta_jx^j\\)\n\n\nFitting a Polynomial\nTo fit a polynomial of order \\(k\\): \\[\ny = \\beta_0 + \\sum_{j=1}^{k}\\beta_jx^j + \\epsilon\n\\] we can simply fit a linear model to transformed predictors, i.e.: \\[\nx_1 = x;\\; x_2 = x^2;\\; x_3 = x^3;\\;...\n\\] and we can just fit a linear model as usual!\n… that seems too easy?\n\n\nChoosing Polynomial Order\nThere are two common options:\n\nDomain knowledge\n\nIs there a theoretical reason to use a cubic?\n\nReduce prediction error\n\nCross-validation or ANOVA, depending on problem.\n\n\n\n\nDomain Knowledge: Stopping Distance\n\n\nThe stopping distance is theoretically proportional to the square of the speed.\n\nA line might fit\n\nFits poorly at 0 (negative stopping distances for positive speed?)\n\nA quadratic fits better?\nA cubic does something funky at 0.\n\n\n\n\n\n\n\n\n\n\n\nChoosing Order with ANOVA\n\nX &lt;- data.frame(dist = cars$dist, x1 = cars$speed, x2 = cars$speed^2, \n    x3 = cars$speed^3, x4 = cars$speed^4)\nlm(dist ~ ., data = X) |&gt; anova()\n\nAnalysis of Variance Table\n\nResponse: dist\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nx1         1 21185.5 21185.5 92.5775 1.716e-12 ***\nx2         1   528.8   528.8  2.3108    0.1355    \nx3         1   190.4   190.4  0.8318    0.3666    \nx4         1   336.5   336.5  1.4707    0.2316    \nResiduals 45 10297.8   228.8                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this situation, Sequential Sum-of-Squares makes sense! (Disagrees with theory, though. Go with theory.)\n\n\nStopping Distance \\(\\propto\\) Speed\\(^2\\)\n\n\nA second order polynomial is: \\[\ny = \\beta_0 + \\beta_1x + \\beta_2x^2\n\\]\nThe implied model is: \\[\ny = \\beta_2x^2\n\\]\n\n\n\n\n\n\n\n\n\n\nUnconventional ESS\n\nX &lt;- data.frame(dist = cars$dist, x1 = cars$speed, x2 = cars$speed^2, \n    x3 = cars$speed^3, x4 = cars$speed^4)\nm1 &lt;- lm(dist ~ x1 + x2, data = X)\nm2 &lt;- lm(dist ~ -1 + x2, data = X)\nanova(m2, m1)\n\nAnalysis of Variance Table\n\nModel 1: dist ~ -1 + x2\nModel 2: dist ~ x1 + x2\n  Res.Df   RSS Df Sum of Sq      F Pr(&gt;F)\n1     49 11936                           \n2     47 10825  2    1111.2 2.4123 0.1006\n\n\nNot a significant difference in models, so go with simpler one: \\(y = \\beta_2x^2\\)\n\nThis is highly specific to this situation - see cautions later.\n\n\n\nPolynomial Models will Overfit!\nTrue model: \\(f(x) = 2 + 25x + 5x^2 - x^3\\) (cubic), Var(\\(\\epsilon\\)) = 40\n\n\n\n\n\n\n\nMultiple Regression Polynomial Models\nA full polynomial model of order 2 with two predictors is: \\[\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2 + \\beta_{12}x_1x_2\n\\] In R this can be specified as:\n\nlm(y ~ (x1 + x2)^2)\n\n\nThis is why you can’t use y ~ x + x^2 to get a polynomial model - R tries to interpret this as a model specification rather than a transformation. \nWe’ll learn more about interactions and transformations in the next few lectures."
  },
  {
    "objectID": "L14-Nonlinear.html#cautions-about-polynomials",
    "href": "L14-Nonlinear.html#cautions-about-polynomials",
    "title": "13  L14: Non-Linear Relationships with Linear Models",
    "section": "13.2 Cautions about Polynomials",
    "text": "13.2 Cautions about Polynomials\n\nLower Order Terms\nUnless there’s a strong physical reason,\n\n\\((ax - b)^2\\) is the model, not \\(\\beta_0 + \\beta_1x + \\beta_{11}x^2\\)\n\n\n\nOrders higher than 3 are rarely jutified\nRecall the interpretation of a slope:\n\nA one unit increase in \\(x\\) is associated with a \\(\\beta_1\\) unit increase in \\(y\\).\n\nThis interpretation fails in quadratrics, and fails spectacularly in higher orders.\n\n\nSee splines for more flexibility\n\n\nExtrapolation is Fraught with Peril\nUnless you have the true order (you don’t), polynomials diverge almost immediately.\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/polyFit\")\n\n\n\n\\(x\\) and \\(x^2\\) are correlated\nThis strongly affects parameter estimates.\n… unless…\n\n\npoly() uses orthogonal polynomials\n\nbetas &lt;- replicate(1000, \n    coef(lm(y ~ poly(x, 2, raw = TRUE),\n         data = data.frame(x = runif(30, 0, 10), \n            y = (x - 2)^2 + rnorm(30, 0, 10)))))\nplot(t(betas))\n\nAfter squaring, cubing, etc., each column of X is transformed to be orthogonal to the previous.\n\nTakes care of transformations for you when using predict().\nThe coef() function is useless.\nMean-centering also helps!\n\n\n\nOrthogonal Polynomials\n\nx &lt;- sort(runif(60, 0, 10))\npar(mfrow = c(2,3))\nfor(i in 1:3) {\n    plot(x, poly(x, 3, raw = TRUE)[,i])\n}\nfor(i in 1:3) {\n    plot(x, poly(x, 3, raw = FALSE)[,i])\n}"
  },
  {
    "objectID": "L14-Nonlinear.html#should-i-use-a-polynomial",
    "href": "L14-Nonlinear.html#should-i-use-a-polynomial",
    "title": "13  L14: Non-Linear Relationships with Linear Models",
    "section": "13.3 Should I Use a Polynomial?",
    "text": "13.3 Should I Use a Polynomial?\n\nExample: mtcars\n[code]\n\n\nSummary"
  },
  {
    "objectID": "L14-Nonlinear.html#participation-questions",
    "href": "L14-Nonlinear.html#participation-questions",
    "title": "13  L14: Non-Linear Relationships with Linear Models",
    "section": "13.4 Participation Questions",
    "text": "13.4 Participation Questions\n\nQ1\nGiven a correctly structured model matrix (\\(X\\)), polynomial models can be fit with the same routines as linear models (without modification).\n\nTrue\nFalse\n\n\n\nQ2\nAny smooth function can be approximated by a polynomial.\n\nTrue\nFalse\n\n\n\nQ3\nOrthogonal polynomials are used because:\n\nThey remove covariance between \\(x\\), \\(x^2\\), \\(x^3\\), etc.\nThey remove the covariance between \\(\\beta_1\\), \\(\\beta_{11}\\), \\(\\beta_{111}\\), etc.\nThey result in p-values that make sense.\nAll of the above.\n\n\n\nQ4\nWhich statement is true?\n\nWe should start with a high order polynomial and use Sequential Sum-of-Squares to choose the order.\nWe should try second order polynomials if we think there’s a curve to our model, but should generally avoid polynomials unless there’s a strong contextual reason.\nWe saw last week that estimates are still unbiased in the presence of extraneous predictors, so it’s fine to include a higher order polynomial in our model."
  },
  {
    "objectID": "L15-Transforming_Response.html#preamble",
    "href": "L15-Transforming_Response.html#preamble",
    "title": "14  L15: Transforming the Response",
    "section": "14.1 Preamble",
    "text": "14.1 Preamble\n\nAnnouncements"
  },
  {
    "objectID": "L15-Transforming_Response.html#transformations",
    "href": "L15-Transforming_Response.html#transformations",
    "title": "14  L15: Transforming the Response",
    "section": "14.2 Transformations",
    "text": "14.2 Transformations\n\nTransforming the Predictors\nSuppose we found that the following second order polynomial model was a “good” fit: \\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_{11}x_{i1}^2 +\\beta_2x_{i2} + \\beta_{22}x_{i2}^2 + \\beta_{12}x_{i1}x_{i2} + \\epsilon_i\n\\]\nNow consider the model: \\[\n\\ln(y_i) = \\beta_0 + \\beta_1x_{i1} + \\beta-2x_{i2} + \\epsilon_i\n\\]\n\n3 parameters instead of 6!\n\nNo interaction term!\n\nIf we’re okay with the log scale for \\(y\\), easier to interpret.\n\n\n\nTransforming the predictors\n\n\nOriginal scale\n\n\n\n\n\n\nLog scale\n\n\n\n\n\n\n\nThe logarithm made it more linear, even if it’s not quite right.\n\n\nConsequences of Logarithms\nConsider the simple model \\(E(y_i) = \\beta x^2\\). Taking the logarithm of both sides: \\[\n\\ln(E(y_i)) = \\ln(\\beta) + 2\\ln(x) = \\beta_0 + \\beta_1 \\ln(x)\n\\] and we have something that looks more like a linear model.\n\nNote that, instead of \\(x^2\\), \\(x^{2.1}\\) would also work as a model.\n\nThe power of \\(x\\) can be estimated.\n\nIt’s also possible that the log scale is the correct scale for \\(y\\)\n\n\\(E(\\ln(y_i)) = \\beta_0 + \\beta_1x\\)\nIn other words, don’t get too bogged down by whether we take the ln of \\(x\\).\n\n\n\n\nLogarithms and Errors\nIf we believe that the log scale is a better scale for \\(y\\), we may postulate the model: \\[\n\\ln(y_i) = \\beta_0 + \\beta_1\\ln x_{i1} + \\beta_2\\ln x_{i2} + \\epsilon\n\\] which implies that the orginal scale for \\(y\\) has the form: \\[\ny_i = e^{\\beta_0}x_{i1}^{\\beta_1}x_{i2}^{\\beta_2}e^{\\epsilon_i}\n\\] The errors are multiplicative!!!\n\nOption 1: Accept this\n\nAllows us to use least squares.\n\nOption 2: Use the model \\(y_i = e^{\\beta_0}x_{i1}^{\\beta_1}x_{i2}^{\\beta_2} + \\epsilon_i\\)\n\nMight be better, but requires a bespoke estimation algorithm.\n\n\n\n\nGeneral Practice\nWe often simply use the model: \\[\n\\ln \\underline y = X\\beta + \\underline \\epsilon\n\\] and do everything on the log scale.\n\nSimpler, but still useful.\nGood predictions of \\(\\ln y_i\\) can be transformed to good predictions of \\(y_i\\).\n\nIn general: Decide on a functional relationship between \\(f(y)\\) and \\(X\\), then use additive errors on the scale of \\(f(y)\\).\nThis has consequences:"
  },
  {
    "objectID": "L15-Transforming_Response.html#residuals-in-transformed-space",
    "href": "L15-Transforming_Response.html#residuals-in-transformed-space",
    "title": "14  L15: Transforming the Response",
    "section": "14.3 Residuals in Transformed Space",
    "text": "14.3 Residuals in Transformed Space\n\nVariance Stabilization\nThe two main purposes of transformations:\n\nFit non-linear functional forms.\nStabilize the variance!\n\nScale-Location plot in the R defaults.\n\n\nFor example, the log function brings large values down a lot, small values down a little.\n\nThe scale of large residuals is decreased more than the scale of small residuals.\n\n\n\n\\(f(\\underline y) = X\\beta + \\underline\\epsilon\\)\nThe estimated residuals are \\(\\hat\\epsilon_i = f(y_i) - \\widehat{f(y_i)}\\)\n\nNote the awkwardly long hat!\n\nWe’re estimating the value of the function, not the value of \\(y_i\\).\nIf \\(f(y_i) - \\widehat{f(y_i)} = f(y_i - \\widehat{y_i})\\), then the original function must have been linear (and a transformation was useless).\n\nWe’re assuming \\(\\epsilon_i\\stackrel{iid}{\\sim} N(0, \\sigma^2)\\), which is difficult to translate to \\(f^{-1}(X\\beta + \\underline\\epsilon)\\).\n\nIn the special case of \\(\\ln\\), \\(\\exp{\\epsilon_i} \\sim \\text{LogNormal}(0, \\sigma^2)\\).\nNo assumption of independence on the original scale!!!\n\nWe assume that the residuals have the same variance on the transformed scale.\n\nLikely not true for the original scale of \\(y\\).\n\n\n\n\nSome Good News\nIf \\((a,b)\\) is a \\((1-\\alpha)\\) CI on the scale of \\(f(y)\\), then \\((f^{-1}(a), f^{-1}(b))\\) is a valid CI on the scale of \\(y\\).\n\nIt’s not the only valid CI!\n\nNote that it’s not a symmetric CI!\n\nWorks for \\(y\\) as well as the \\(\\beta\\) parameters.\n\nTransformation might induce dependence among the parameters.\nA CI for \\(\\beta_1\\) is useless if there’s high covariance with \\(\\beta_2\\)."
  },
  {
    "objectID": "L15-Transforming_Response.html#choosing-transformations",
    "href": "L15-Transforming_Response.html#choosing-transformations",
    "title": "14  L15: Transforming the Response",
    "section": "14.4 Choosing Transformations",
    "text": "14.4 Choosing Transformations\n\nMethods for Choosing Transformations\n\nTheory.\n\nIf theory says that the log transform makes sense, use that.\n\nDon’t even consider the next steps. Just go with theory.\n\nExample: Forest fire burn sizes are right skewed, the log-transform makes sense.\n\nIn my research, I used the lognorman lodel for the residuals to acheive the same effect.\n\n\nExperimentation after looking at the Scale-Location plot.\n\nIf log or sqrt don’t work, move on to step three.\n\nThe Box-Cox Transformation\n\nFinds an appropriate transformation using maximum likelihood.\n\n\n\n\nBox-Cox\nWe use the transformation: \\[\nV = \\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda \\dot{Y}^{\\lambda - 1}} & \\text{if }\\lambda \\ne 0\\\\\n\\dot{Y}\\ln(Y) & \\text{if }\\lambda = 0\n\\end{cases}\n\\] where \\(\\dot Y\\) is the geometric mean of \\(y\\).\n\\(\\lambda\\) is chosen through maximum likelihood\n\nEssentially, refit with each value of \\(\\lambda\\) and see which minimizes the residual variance.\n\nPlot the likelihhods and choose the highest.\n\n\n\n\nSimpler Box-Cox\nThe textbook recommends the previous formula, however R uses: \\[\nW = \\begin{cases}\n\\frac{Y^\\lambda - 1}{\\lambda} & \\text{if }\\lambda \\ne 0\\\\\n\\ln(Y) & \\text{if }\\lambda = 0\n\\end{cases}\n\\]\n\n\nVariance of \\(\\lambda\\)\nIf we had a different data set, we’d get a different value of \\(\\lambda\\)!\nR reports the the log-likelihood values, along with the top 5%.\n\nAnything in the top 5% is reasonable.\n\nIt’s not an exact science.\n\nUsually, we check the best \\(\\lambda\\) values and round to something nice.\n\nlog, sqrt, squared, inverse, etc.\n\n\n\n\nSummary\n\nChoosing a transformation:\n\nTheory\nExploration\nRound the value from Box-Cox.\n\nWorking with a transformation:\n\nChoose functional form, assume additive errors (usually, not always!)\nStay on the transformed scale\n\nAll assumptions about residuals apply to the transformed scale!\n\n\n\nTo be useful, all transformations should consider the context of the problem!"
  },
  {
    "objectID": "L15-Transforming_Response.html#participation-questions",
    "href": "L15-Transforming_Response.html#participation-questions",
    "title": "14  L15: Transforming the Response",
    "section": "14.5 Participation Questions",
    "text": "14.5 Participation Questions\n\nQ1\nIf the true relationship has the form \\(y = f(X) + \\epsilon_i\\), we can always find a transformation \\(f^{-1}\\) to make it linear.\n\nTrue\nFalse\n\n\n\nQ2\nIf we find that \\(\\lambda = 2\\) is the best transformation, then the following models are equivalent: \\[\n\\frac{Y^2 - 1}{2} = X\\beta\\quad\\text{and}\\quad Y^2 = 2X\\beta + 1\n\\]\n\nTrue\nFalse\nTechnically not, but good enough in practice.\n\n\n\nQ3\nA transformation of the form \\(y = f(X) + \\epsilon\\) leads to multiplicative errors.\n\nTrue\nFalse\n\n\n\nQ4\nWhich of the following is not a good reason to investigate transformations?\n\nIf the variance looks unstable.\nIf the theory says a transformation is necessary.\nIf a transformation might lead to a much simpler model.\nIf \\(y\\) doesn’t look normal.\n\n\n\nQ5\nThe default residual plots in R can help diagnose the need for a transformation.\n\nTrue\nFalse"
  },
  {
    "objectID": "L16-Dummies.html#preamble",
    "href": "L16-Dummies.html#preamble",
    "title": "15  L16: Dummy Variables",
    "section": "15.1 Preamble",
    "text": "15.1 Preamble\n\nAnnouncements"
  },
  {
    "objectID": "L16-Dummies.html#predictors",
    "href": "L16-Dummies.html#predictors",
    "title": "15  L16: Dummy Variables",
    "section": "15.2 0/1 Predictors",
    "text": "15.2 0/1 Predictors\n\nDummy Coding\n“Dummy” variables are just predictors that only take the values 0 and 1.\n\n0 pairs of glasses versus 1 pair of glasses\n\nThis is a count that can only be 0 or 1\n\n0 means automatic, 1 means manual\n\nArbitrary choice of 0/1\n\n0 means off, 1 means on\n\nNatural choice of 0/1, but still arbitrary\n\n\n\n\nSlopes with a Dummy Variable\nUsual interpretation: as \\(x\\) increases by 1, \\(y\\) increases by \\(\\beta\\).\nThis doesn’t go away, but we get a new interpretation!\n\n\\(x\\) can only increase by one (from 0 to 1).\n\n\\(\\beta\\) is the difference in groups.\n\n\nNote that we assume constant variance; this means the variance is the same in both groups\n\nExact same assumptions as a t-test.\n\n(Different from a “Welch” t-test)\n\n\n\n\nCategorical Variables\nConsider the cyl column in mtcars. We could code three dummy variables:\n\n\\(I(cyl == 4)\\)\n\\(I(cyl == 6)\\)\n\\(I(cyl == 8)\\)\n\nThis would lead to the model: \\[\ny = \\beta_0 + \\beta_1I(cyl == 4) + \\beta_2I(cyl == 6) + \\beta_3I(cyl == 8)\n\\]\nWhat is the intercept here?\n\n\nCategorical Variable Dummy Coding\nInstead, we set one as a reference variable and let the intercept “absorb” it:\n\n\\(I(cyl == 6)\\)\n\\(I(cyl == 8)\\)\n\nThis would lead to the model: \\[\ny = \\beta_0 + \\beta_1I(cyl == 6) + \\beta_2I(cyl == 8)\n\\] where\n\n\\(\\beta_0\\) is the mean of mpg when cyl = 4.\n\\(\\beta_1\\) is the difference in mean of mpg between 4 and 6 cylinder cars.\n\\(\\beta_2\\) is the difference in means for 4 versus 8.\n\nDifference btwn 6 and 8 can be found with some cleverness.\n\n\n\n\nModels with Categorical Variables\nThe model \\(y = \\beta_0 + \\beta_1I(cyl == 6) + \\beta_2I(cyl == 8)\\) is equivalent to: \\[\ny_i = \\begin{cases}\\beta_0 & \\text{if }\\; cyl == 4\\\\ \\beta_0 + \\beta_1 & \\text{if }\\; cyl == 6\\\\\\beta_0  + \\beta_2 & \\text{if }\\; cyl == 8\\end{cases}\n\\]\nThis is equivalent to fitting an intercept-only model \\(y = \\beta_0\\) for subsets of the data.\nBy putting them in the same model, we can easily test for significance."
  },
  {
    "objectID": "L16-Dummies.html#interactions",
    "href": "L16-Dummies.html#interactions",
    "title": "15  L16: Dummy Variables",
    "section": "15.3 Interactions",
    "text": "15.3 Interactions\n\nDifferent Intercepts, Same Slope\nIf we have cyl and disp in the model, we get the following:\n\\[\ny = \\beta_0 + \\beta_1I(cyl == 6) + \\beta_2I(cyl == 8) + \\beta_3 disp\n\\] which is equivalent to: \\[\ny_i = \\begin{cases}\\beta_0  + \\beta_3 disp& \\text{if }\\; cyl == 4\\\\ \\beta_0 + \\beta_1  + \\beta_3 disp& \\text{if }\\; cyl == 6\\\\\\beta_0  + \\beta_2  + \\beta_3 disp& \\text{if }\\; cyl == 8\\end{cases}\n\\] This is three different models of mpg versus disp, but with a different intercept depending on the value of cyl.\n\n\nInteraction Terms: Different Intercepts, Different Slopes\nWe can expand the model above with an interaction term. \\[\ny = \\beta_0 + \\beta_1I(6) + \\beta_2I(8) + \\beta_3 disp + \\beta_4I(6)disp + \\beta_5I(8)disp\n\\] where \\(I(6)\\) is just shorthand for \\(I(cyl == 6)\\).\nThis is the same as: \\[\ny_i = \\begin{cases}\\beta_0  + \\beta_3 disp& \\text{if }\\; cyl == 4\\\\ (\\beta_0 + \\beta_1)  + (\\beta_3 + \\beta_4) disp& \\text{if }\\; cyl == 6\\\\(\\beta_0  + \\beta_2)  + (\\beta_3 + \\beta_5) disp& \\text{if }\\; cyl == 8\\end{cases}\n\\] In this case, we might as well fit 3 completely different models!\n(Except we can test for significance!)\n\n\nANCOVA\nIf g is a categorical variable, then:\n\nlm(y ~ g) is a t-test if g is binary\nlm(y ~ g) is an ANOVA if g has more than 2 categories\nlm(y ~ x * g) is an ANCOVA model\n\nAnalysis of Covariance.\n\n\nMain idea: Is the covariance (or correlation) between \\(x\\) and \\(y\\) different for different categories of \\(g\\)?\n\nOnly a small extension to ANOVA\n\nt-test: exactly 2 means\nANOVA: 2+ means\nANCOVA: 2+ covariances\n\n\n\n\nBeyond \\(x\\) and \\(g\\)\n\nIn the simple cases, we’re doing t-test, ANOVA, or ANCOVA.\nBeyond this, we’re just doing regression, no special names.\n\n“Controlling for” is a term we’ll use later.\n\nChoosing interaction terms is hard.\n\nggplot2 makes parts of it a lot easier."
  },
  {
    "objectID": "L16-Dummies.html#participation-questions",
    "href": "L16-Dummies.html#participation-questions",
    "title": "15  L16: Dummy Variables",
    "section": "15.4 Participation Questions",
    "text": "15.4 Participation Questions\n\nQ1\nA dummy variable is:\n\nA stupid variable. Just a dumb, stupid variable that only idiots use.\nA variable that naturaly takes the values 0 and 1.\nA variable that has been coded as 0 and 1 to indicate different groups.\n\n\n\nQ2\nA reference category is used because:\n\nWe are always more interested in one particular category and how the other categories compare to it.\nThe intercept is the mean of y when all predictors are 0, so we need a case where all predictors are 0.\n\n\n\nQ3\nIn the mpg ~ factor(cyl) there will be a dummy variable labelled:\n\n6\ncyl6\nfactor(cyl)6\nfactor(cyl)4\n\n\n\nQ4\nWhich statement is false?\n\nt-test is a special case of linear regression.\nANCOVA is a special case of linear regression.\nWelch’s t-test for samples with unequal variances is a special case of regression.\nANOVA is a special case where Sequential Sum-of-Squares makes sense.\n\n\n\nQ5\nIt is possible to have interaction terms between two categorical variables.\n\nTrue\nFalse\n\n\n\nQ6\nIn the regression mpg ~ factor(cyl) + disp + factor(cyl):disp, if we find that disp is not significant then we can safely remove it.\n\nTrue\nFalse"
  },
  {
    "objectID": "L16-Dummies.html#significance-of-a-group",
    "href": "L16-Dummies.html#significance-of-a-group",
    "title": "15  L16: Dummy Variables",
    "section": "15.5 Significance of a Group",
    "text": "15.5 Significance of a Group\n\nOutput of summary.lm()\nThe output compares each slope to 0.\n\nFor a categorical predictor, this tests if a category is different from the reference.\n\nThere is not an easy built-in way to check significance of a specific group\n\nFor example, we may want to test for equality of intercepts and slopes for 6 and 8 cylinder cars, allowing 4 to have separate values.\n\nTest for equality of slopes is something covered in the textbook\nAlternative: Change Reference group and use ESS\n\n\n\n\nChanging the reference group\nSuppose we want to compare 6 and 8 cylinder cars. We can set up our model as: \\[\ny = \\beta_0 + \\beta_1I(8) + \\beta_2I(4) + \\beta_3disp + \\beta_4dispI(8) + \\beta_5dispI(4)\n\\] where now 6 is the reference group.\nWe can test \\(\\beta_1=\\beta_4 = 0\\) using ESS to test whether mpg versus disp is the same in these two categories.\nIn R, we need to set up our own dummies to do this."
  },
  {
    "objectID": "L17-Multicollinearity.html#the-problem",
    "href": "L17-Multicollinearity.html#the-problem",
    "title": "16  L17: Multicollinearity",
    "section": "16.1 The Problem",
    "text": "16.1 The Problem\n\nThe Problem with Multicollinearity\n\n\n\nMultiple regression fits a hyperplane\nIf the points form a “tube”, an infinite number of hyperplanes work.\n\nRotate plane around axis of tube.\n\n\n\n\n\n\n\n\n\n\n\n\nConsequences of the Problem\n\n\nHigh cor. in \\(X\\) \\(\\implies\\) high cor. in \\(\\hat{\\underline\\beta}\\).\n\nMany combos of \\(\\hat{\\underline\\beta}\\) are equally likely\nNo meaningful CIs\n\n\n\nset.seed(2112)\nreplicate(1000, {\n    y &lt;- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n    coef(lm(y ~ x1 + x2))[-1]\n}) |&gt; \n    t() |&gt; \n    plot(xlab = expression(hat(beta)[1]), \n        ylab = expression(hat(beta[2])),\n        main = \"Estimated betas for correlated\\npredictors, many samples\")\n\n\n\n\n\n\n\n\nAnother Formulation of the Problem\nConsider the model \\(y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\epsilon_i\\), where \\[\nx_{i1} = a + bx_{i2} + z_i\n\\] where \\(z_i\\) represents some extra uncertainty.\nFitting the model, we could:\n\nSet \\(\\hat\\beta_1\\) to 0, let \\(x_2\\) model all of the variance.\nSet \\(\\hat\\beta_2\\) to 0, let \\(x_1\\) model all of the variance.\nLet \\(x_1\\) model any proportion of the variance, let \\(x_2\\) model the rest.\n\nThe parameter estimates are not unique.\n\n\nThe Source of the Problem\n\\[\n\\hat{\\underline{\\beta}} = (X^TX)^{-1}X^TY,\\quad V(\\hat{\\underline{\\beta}}) = (X^TX)^{-1}\\sigma^2\n\\]\n\nIf two columns of \\(X\\) are linearly dependent, then \\(X^TX\\) is singular.\n\nConstant predictor value (linearly dependent with column of 1s).\nUnit change (one column for Celcius, one for Fahrenheit).\n\nIf two columns of \\(X\\) are nearly linearly dependent, then some elements of \\((X^TX)^{-1}\\) are humungous.\n\nTwo proxy measure for the same thing (e.g., daily high and low temperatures).\nNearly linear transformation (e.g., polynomial or BMI)\n\n\n\n\nDetecting the Problem\nThe variance-covariance matrix of \\(X\\) can be useful: \\[\nCov(X) = \\begin{bmatrix}\n0 & 0 & 0 & 0 & \\cdots\\\\\n0 & V(X_1) & Cov(X_1, X_2) & Cov(X_1, X_3) & \\cdots\\\\\n0 & Cov(X_1, X_2) & V(X_2) & Cov(X_2, X_3) & \\cdots\\\\\n0 & Cov(X_1, X_3) & Cov(X_2, X_3) & V(X_3) & \\cdots\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n\\end{bmatrix}\n\\] Why are the first column/row 0?\n\n\nPlotting \\(Cor(X)\\)\n\nlibrary(palmerpenguins); library(GGally)\nggcorr(penguins)\n\nWarning in ggcorr(penguins): data in column(s) 'species', 'island', 'sex' are\nnot numeric and were ignored\n\n\n\n\n\n\n\nDetecting the Problem: \\(V(\\hat{\\underline\\beta})\\)\nUnfortunately, the var-covar matrix is hard to get from R.\n\nWe can look at the SE column of the summary output!\n\nVery very very much not conclusive.\n\nThe Variance Inflation Factor\n\n\n\nThe Variance Inflation Factor\nWe can write the variance of each estimated coefficeint as: \\[\nV(\\hat\\beta_i) = VIF_i\\frac{\\sigma^2}{S_{ii}}\n\\] where \\(S_{ii} = \\sum_{k=1}^n(x_{ki} - \\bar{x_i})^2\\) is the “SS” for the \\(i\\)th column of \\(X\\).\n\nIf there is no “Variance Inflation”, then VIF = 1\n\n“Inflation” comes from the idea of rotating a plane around a “tube”.\nAlso interpreted as a measure of linear dependence with other columns of \\(X\\).\n\n\n\n\nInterpreting the Variance Inflation Factor\nConsider a regression of \\(X_i\\) against all other columns of \\(X\\).\n\nThe \\(R^2\\) measures how well the other predictors can model \\(X_i\\)\n\nLabel this \\(R_i^2\\) to indicate it’s the \\(R^2\\) for \\(X_i\\) against other columns.\n\nImportant: We’re not considering \\(\\underline y\\) at all!\n\nThe VIF can be calculated as: \\[\nVIF_i = \\frac{1}{1 - R_i^2}\n\\]\n\nIf \\(R_i^2=0\\), then \\(VIF_i = 1\\)\nIf \\(R_i^2\\rightarrow 1\\), then \\(VIF_i \\rightarrow \\infty\\)"
  },
  {
    "objectID": "L17-Multicollinearity.html#will-scaling-fix-the-problem",
    "href": "L17-Multicollinearity.html#will-scaling-fix-the-problem",
    "title": "16  L17: Multicollinearity",
    "section": "16.2 Will Scaling Fix the Problem",
    "text": "16.2 Will Scaling Fix the Problem\n\nScaling the Predictors\nIf we subtract the mean and divide by the sd, some of the correlation goes away.\n\nThis is actually kinda bad - we’ve hidden some multicollinearity from ourselves!\n\nIf \\(Z\\) is the standardized version of \\(X\\), then \\[\nCor(X) = Z^TZ/(n-1)\n\\]\nIf \\(Z\\) is the mean-centered version of \\(X\\), then \\[\nCov(X) = Z^TZ/(n-1)\n\\]"
  },
  {
    "objectID": "L17-Multicollinearity.html#fixing-the-problem",
    "href": "L17-Multicollinearity.html#fixing-the-problem",
    "title": "16  L17: Multicollinearity",
    "section": "16.3 Fixing The Problem",
    "text": "16.3 Fixing The Problem\n\nOne way to fix the problem\nDon’t.\nWe can’t get good estimates of the \\(\\hat\\beta\\)s, but we can still get good predictions.\n\nThis only works if the new values are in the same “tube” as the others.\nIf the multicollinearity is real, what estimates do you expect?\n\nWithout a controlled experiment, there isn’t a good way to estimate the effect of \\(X_1\\) on it’s own!\n\n\n\n\nRemoving predictors\nIf two predictors are measuring the same thing, then just include one?\n\nThis might lose some information!\n\nIt also might not!\n\nThe estimated \\(\\beta\\) won’t be meaningful.\n\nInferences will be difficult."
  },
  {
    "objectID": "L17-Multicollinearity.html#participation-questions",
    "href": "L17-Multicollinearity.html#participation-questions",
    "title": "16  L17: Multicollinearity",
    "section": "16.4 Participation Questions",
    "text": "16.4 Participation Questions\n\nQ1\nMulticollinearity can come from:\n\nUnit changes\nPolynomial terms\nProxy measures\nAll of the above\n\n\n\nQ2\nMulticollinearity is a problem because\n\nStrong correlation in \\(X\\) makes estimates of \\(\\beta\\) invalid.\nStrong correlation in \\(X\\) means there are many values of \\(\\underline\\beta\\) that are equally probable.\nThere’s no way to fix strong correlation in \\(X\\).\n\n\n\nQ3\nWhen multicollinearity is present, which of the following is still valid?\n\nInferences about the effect of one of the predictors.\nConfidence intervals for a single coefficients.\nPredictions.\nOverall F test for significance of any slope parameter.\n\n\n\nQ4\nThe VIF is defined as:\n\nThe amount that the MSE increases due to the variance in \\(\\hat{\\underline\\beta}\\).\nThe coefficient of determination of \\(X_i\\) against all other predictors.\nThe correlation between \\(X_i\\) and all other predictors.\nThe \\(R^2\\) value for \\(Y\\) against \\(X_i\\)."
  },
  {
    "objectID": "L18-Modelling_Poorly.html#preamble",
    "href": "L18-Modelling_Poorly.html#preamble",
    "title": "17  L18: Modelling Poorly",
    "section": "17.1 Preamble",
    "text": "17.1 Preamble\n\nAnnouncements\n\nI actually have the exams today!"
  },
  {
    "objectID": "L18-Modelling_Poorly.html#motivation",
    "href": "L18-Modelling_Poorly.html#motivation",
    "title": "17  L18: Modelling Poorly",
    "section": "17.2 Motivation",
    "text": "17.2 Motivation\n\nSelecting Predictors\nAs you’ve seen from the assignment, choosing predictors is hard!\nWouldn’t it be nice if the computer would choose the best model for you?\n\n\nThe “Best” Model\n\nBest represents the Data Generating Process (DGP)\n\nBest for inference\n\nMisses the DGP, but provides useful insights into the relationships\n\nAlso best for inference\n\nFits the current data the best\n\nOverfitting?\n\nBest able to predict new values\n\nRandom Forests and Neural Nets\n\n\nThe “best” model depends on the goal of the study!"
  },
  {
    "objectID": "L18-Modelling_Poorly.html#automatic-predictor-selection",
    "href": "L18-Modelling_Poorly.html#automatic-predictor-selection",
    "title": "17  L18: Modelling Poorly",
    "section": "17.3 Automatic Predictor Selection",
    "text": "17.3 Automatic Predictor Selection\n\nModel Comparison Criteria\n\n\\(R^2\\), or adjusted \\(R^2\\)\n\nLower is better, but \\(R^2\\) increases as we add predictors\n\n\\(s^2\\), the residual variance.\n\nAdding predictors always decreases \\(s^2\\)\n\nMallow’s \\(C_p\\) statistic\n\n\\(C_p = RSS_p/s^2 - (n - 2p)\\)\n\n\\(RSS_p\\) is the RSS of the smaller model, with \\(p\\) parameters\n\\(s^2\\) is the MSE from the largest model under consideration\n\nAdding predictors does not increase this statistic.\n\nAIC, the Akiake Information Criterion\n\n\\(AIC = 2p - 2\\ln(\\hat L)\\), where \\(\\hat L\\) is the likelihood evaluated at the estimated parameters.\n\nE.g., when \\(\\epsilon_i\\sim N(0,\\sigma^2)\\), the likelihood is the product of normal distributions with a mean of \\(X\\hat{\\underline\\beta}\\) and variance \\(s^2\\).\n\nDoes not increase with added predictors.\n\n\n\n\nMore on AIC\n\\[\nAIC = 2p - 2\\ln(\\hat L)\n\\]\nRecall from Maximum Likelihood Estimation, the likelihood is the likelihood of observing the particular data, given the parameters: \\[\nL(y|\\underline\\beta, X, \\sigma) = \\prod_{i=1}^nf_Y(X|\\underline\\beta, \\sigma^2),\n\\] where \\(f_Y(X|\\underline\\beta, \\sigma^2)\\) is the normal distribution.\n\nA high AIC means we either:\n\nHave too many parameters, or\nOur model doesn’t fit the data well.\n\nA low AIC means we’ve got a good model that isn’t overly complicated\n\n“Low” is relative to other models\n\n\n\n\nBest Subset\n\nFind the collection of predictors that optimizes the statistic of interest.\n\nThat’s it. You just try them all.\n\n\nBackward and Forward Selection\n\nBackward Selection\n\nInclude all predictors, try removing one\n\nCheck the \\(R^2\\), p-values, Mallow’s Cp, or AIC\n\nPut that one back in the model, try removing another\n\nForward Selection\n\nFind the best predictor to include first\nFind the best predictor to include second\n\n\nBoth have some sort of stopping criteria.\n\n\nBackward Selection\n\nFit a model with all \\(p\\) predictors\nTry all models with \\(p-1\\) predictors.\n\nIdentify the best one, say remove \\(x_j\\)\nCheck stopping criteria.\n\nIf stopping critera not met, try all models with \\(p-2\\) predictors, not including \\(x_j\\).\n\n\n\nBackward Selection Example\n\nStart with mpg ~ disp + wt + am + cyl + qsec.\nCheck all of the AICs, remove cyl.\nCheck all of the AICs, remove disp.\nCheck all AICs, stop.\n\nFinal model: mpg ~ wt + am + qsec\n\n\nForward Selection\n\nStart with mpg ~ 1.\nTest each predictor individually, check AIC, keep wt.\nTest each remaining predictor, check AIC, keep cyl.\nTest each remaining predictor, stop.\n\nFinal model: mpg ~ wt + cyl\n\n\nBest Subset Selection\nTest every combination of predictors, keep the one with the lowest AIC (or other stat).\n\n\nThink-Pair-Share\nWhat might these methods be missing?\nWhen would these methods be useful?\n\n\nEvaluating Algorithmic Predictor Selection\nSuppose we have measured 30 predictors that we know are not related to the response.\nHow many predictors should Backwards Selection select?"
  },
  {
    "objectID": "L18-Modelling_Poorly.html#paticipation-questions",
    "href": "L18-Modelling_Poorly.html#paticipation-questions",
    "title": "17  L18: Modelling Poorly",
    "section": "17.4 Paticipation Questions",
    "text": "17.4 Paticipation Questions\n\nQ1\nWhy do Forward and Backward selection procedures not choose the same model?\n\nThey do choose the same model.\nBecause the order that the predictors enter the model matters.\nBecause the best model is different depending on which approach you use.\n\n\n\nQ2\nBest Subset selection will always choose the same model regardless of which statistic it’s based on (\\(R^2\\), Mallow’s \\(C_p\\), AIC, etc).\n\nTrue\nFalse\n\n\n\nQ3\nUnder the null hypothesis, p-values follow a uniform distribution.\n\nTrue\nFalse\nTrue, but only if you do the study right.\n\n\n\nQ4\nSuppose we have 30 predictors in the model. How many should we keep?\n\nMost of them.\nAbout half of them.\nA few of them.\nWhat? What kind of question is that? How could I possibly know without seeing the context of the problem?!?"
  },
  {
    "objectID": "L18-Modelling_Poorly.html#the-best-model-1",
    "href": "L18-Modelling_Poorly.html#the-best-model-1",
    "title": "17  L18: Modelling Poorly",
    "section": "17.5 The Best Model",
    "text": "17.5 The Best Model\n\nNeural Networks and Random Forests\n\nNeural Networks\n\nEssentially a series of linear regressions with a minor non-linear transformation.\nA “deep” neural network is non-linear transformations and all interactions.\nVery finicky, but very powerful when necessary.\n\nRandom Forests\n\nAlso a series of non-linear effects with interactions.\nMuch much much less finicky.\n\n\n\n\nCausal Inference\nExperiments are our way of controlling variables so that we can isolate their effect.\nMost data we tend to use is observational.\n\nCausal inference is statistical magic to determine causality from observation.\n\n… with varying degrees of success.\n\n\n\n\nWhy are you telling us this, Devan?\nWhich model is “best”?\n\nBest predictions?\n\nNN and RF, with cross-validation.\n\nBest inference?\n\nBuild a model based on the context of the problem.\nChoose transformations and interactions appropriately.\nOnly check p-values at the very end.\n\nBest subset of predictors?\n\nRecall: multicollinearity. Without an experiment, correlated predictors mean that there’s no way to tell which predictors are best.\n\n\nOpinion: Algorithmic selection methods are bad approximations to better techniques that are outside out the scope of this course."
  },
  {
    "objectID": "L19-Example_Analysis_mtcars.html#exploratory-data-analysis",
    "href": "L19-Example_Analysis_mtcars.html#exploratory-data-analysis",
    "title": "18  Analysis of MTCars",
    "section": "18.1 Exploratory Data Analysis",
    "text": "18.1 Exploratory Data Analysis\n\nUnderstanding Data\nWhat do the column names mean?\nThe help file gives a (very) brief description. I spent a few minutes just looking at the descriptions and trying to guess what relationships I might find.\nOverall, most of the predictors are trying to answer the question “Is this a powerful car?”\n\n\nPlotting the Data\nFrom a pairs plot (pairs(mtcars), which I have not included to reduce the amount of output):\n\nOnly 1 car has carb = 6, 1 has carb = 8\nwt and drat are (-ively) correlated\n\ndisp and hp\ndisp and drat (-ive)\ndisp and wt\nhp and wt\nhp and qsec\n\n\nwt and disp are clearly multicollinear, and they’re measuring the same thing so I might want to include just one of them.\n\n\nPatterns in the Predictors\nIn the following code, I tried x as am, cyl, gear, and carb. The y axis was wt, disp, drat, and qsec. I essentially tried every combination of these and wrote down the most interesting patterns.\n\n# Continuous versus categorical\nggplot(mtcars) +\n    aes(x = factor(am), y = wt) +\n    geom_boxplot()\n\n\n\n\n\nwt is different across categories of am, cyl, carb, gear (all positive)\n\ndisp has same relationships\nhp has same relationships, except 4 gear cars have lower hp than 3 and 5 gear cars\ndrat has opposite relationships\n\n\nI did something similar with the following code, checking every combination of all relevant predictors and writing down anything that stuck out to me.\n\n# Continuous vs. continuous\nggplot(mtcars) +\n    aes(x = disp, y = wt, colour = factor(cyl)) +\n    geom_point()\n\n\n\n\n\nClear separation between disp and wt when coloured by am or cyl.\n\nIn other words, there are distinct groups. This probably means that one of the continuous predictors has all of the information necessary, and it won’t be necessary to include an interaction between continuous predictors (it rarely is).\n\nOtherwise, there are not many relationships that might be present.\n\nThe following plot was also used with all combinations of categorical predictors.\n\n# categorical variables\nggplot(mtcars) + \n    aes(x = factor(am), fill = factor(vs)) +\n    geom_bar(position = \"dodge\")\n\n\n\n\n\nSome kind of “correlation” between am and cyl.\n\nMeasuring something similar, but from different perspectives.\n\nVery little relation between am and vs - they’re measuring different things.\n\nMight be worth checking models where am is switched with vs. \n\n\n\n\nConclusions\nMost things are measuring “how powerful is this car”, so we should just choose the ones that make sense to us and check a few categorical predictors.\nwt and disp make the most sense as measures for mpg, and am and cyl also make some sense. I’ll try switching out some of the other predictors, but I expect that the final model will either be wt*am or disp*cyl."
  },
  {
    "objectID": "L19-Example_Analysis_mtcars.html#more-eda-relationships-with-the-response-interactions",
    "href": "L19-Example_Analysis_mtcars.html#more-eda-relationships-with-the-response-interactions",
    "title": "18  Analysis of MTCars",
    "section": "18.2 More EDA: Relationships with the Response / Interactions",
    "text": "18.2 More EDA: Relationships with the Response / Interactions\nNow we’re finally looking at mpg!\n\nggplot(mtcars) +\n    aes(y = mpg, x = disp, colour = factor(cyl)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = TRUE, formula = y ~ x)\n\n\n\n\nFrom looking at many many plots, I propose the following candidate models:\n\nmpg versus disp * cyl\nmpg versus wt * am (cyl?)\nmpg versus wt * vs (maybe not an interaction)\nmpg versus wt * gear?\n\nI had also considered including qsec, but a plot of mpg versus qsec with colours from cyl revealed that cyl explains the relationship; if we include cyl, then the slope for mpg versus qsec is 0. The same thing happens with drat, so cyl is probably enough to include in the model rather than either qsec or drat."
  },
  {
    "objectID": "L19-Example_Analysis_mtcars.html#modelling",
    "href": "L19-Example_Analysis_mtcars.html#modelling",
    "title": "18  Analysis of MTCars",
    "section": "18.3 Modelling",
    "text": "18.3 Modelling\nLet’s test out our models!\nAgain, to reduce the amount of output I have to wade through, I changed the following code a bunch and left it at something meaningful to my final analysis.\n\ndispmodel &lt;- lm(mpg ~ disp * factor(cyl), data = mtcars)\n\npar(mfrow = c(2,2))\nplot(dispmodel, col = mtcars$cyl)\n\n\n\n\n\nResiduals versus fitted looks good\nQQ norm looks great! For this small of a data set, we don’t expect much from the qq-plot, so this is actually very nice.\nScale-Location has a slight U shape, which isn’t ideal. There may still be a predictor that’s worth including.\nThere’s a high influence point. This is likely due to the interaction between cyl and disp.\n\nWhen we have this kind of interaction, there are essentially three lines, each with fewer observations. It is much easier for a point to be influential with interaction present.\n\n\n\nwtmodel &lt;- lm(mpg ~ wt * am, data = mtcars)\n\npar(mfrow = c(2,2))\nplot(wtmodel)\n\n\n\n\n\nFirst plot looks good!\nQQplot has some heavy tails - not bad, but not ideal. dispmodel was better.\nScale-location is great!\nNo high leverage points.\n\nBoth models are good in different ways. Let’s check their summaries.\n\nsummary(dispmodel)\n\n\nCall:\nlm(formula = mpg ~ disp * factor(cyl), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4766 -1.8101 -0.2297  1.3523  5.0208 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        40.87196    3.02012  13.533 2.79e-13 ***\ndisp               -0.13514    0.02791  -4.842 5.10e-05 ***\nfactor(cyl)6      -21.78997    5.30660  -4.106 0.000354 ***\nfactor(cyl)8      -18.83916    4.61166  -4.085 0.000374 ***\ndisp:factor(cyl)6   0.13875    0.03635   3.817 0.000753 ***\ndisp:factor(cyl)8   0.11551    0.02955   3.909 0.000592 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.372 on 26 degrees of freedom\nMultiple R-squared:  0.8701,    Adjusted R-squared:  0.8452 \nF-statistic: 34.84 on 5 and 26 DF,  p-value: 9.968e-11\n\n\n\nsummary(wtmodel)\n\n\nCall:\nlm(formula = mpg ~ wt * am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6004 -1.5446 -0.5325  0.9012  6.0909 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  31.4161     3.0201  10.402 4.00e-11 ***\nwt           -3.7859     0.7856  -4.819 4.55e-05 ***\nam           14.8784     4.2640   3.489  0.00162 ** \nwt:am        -5.2984     1.4447  -3.667  0.00102 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.591 on 28 degrees of freedom\nMultiple R-squared:  0.833, Adjusted R-squared:  0.8151 \nF-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11\n\n\nThe \\(R^2\\) for dispmodel is a fair bit higher (although there’s no standard for how much an \\(R^2\\) should change, so this might not be a meaningful difference). As we saw in class, the \\(R^2\\) is based on the same quantities as the F-test for different models.\n\nanova(dispmodel, wtmodel)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ disp * factor(cyl)\nModel 2: mpg ~ wt * am\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     26 146.23                              \n2     28 188.01 -2   -41.773 3.7136 0.03814 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe models fit significantly differently. Which one fits better?\n\n# MSE values\nsummary(dispmodel)$sigma\n\n[1] 2.371581\n\nsummary(wtmodel)$sigma\n\n[1] 2.591247\n\n\ndispmodel has a higher \\(R^2\\) and a lower MSE, so it seems to be the winner.\nFrom the pairs plot, I saw that disp has a slight relationship with other continuous predictors, and the scale-location plot wasn’t perfect. Perhaps another predictor will help?\nI can do this with the magical update() function. The ~ . + hp notation means the response versus (~) everything ., then add hp. The ~ means “versus” (with the response on the left, which isn’t allowed to change in this case, and the predictors on the right), and the . means “everything”, which in this case refers to everything that was already in the model. The form lm(mpg ~ ., data = mtcars) will fit mpg against everything else it sees in the mtcars dataset.\n\nsummary(update(dispmodel, ~ . + hp)) \n\n\nCall:\nlm(formula = mpg ~ disp + factor(cyl) + hp + disp:factor(cyl), \n    data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3442 -1.7647  0.0994  1.4480  4.4796 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        41.53521    3.04752  13.629 4.48e-13 ***\ndisp               -0.13037    0.02798  -4.660 9.00e-05 ***\nfactor(cyl)6      -19.95406    5.48572  -3.637 0.001249 ** \nfactor(cyl)8      -16.99535    4.83011  -3.519 0.001685 ** \nhp                 -0.01410    0.01184  -1.191 0.245018    \ndisp:factor(cyl)6   0.12975    0.03685   3.521 0.001675 ** \ndisp:factor(cyl)8   0.11199    0.02946   3.801 0.000824 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.353 on 25 degrees of freedom\nMultiple R-squared:  0.8771,    Adjusted R-squared:  0.8476 \nF-statistic: 29.74 on 6 and 25 DF,  p-value: 3.199e-10\n\n\nI checked qsec, drat, and hp, and none seemed worth including in the model. I’ll just leave it as is.\nTo interpret the model we must be careful about the interaction term!\n\\[\nmpg = \\begin{cases}\n\\beta_0 + \\beta_1 disp & \\text{if }cyl == 4\\\\\n(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_4) disp & \\text{if }cyl == 6\\\\\n(\\beta_0 + \\beta_3) + (\\beta_1 + \\beta_5) disp & \\text{if }cyl == 8\\\\\n\\end{cases}\n\\]\n\nFor 4 cylinder cars, the baseline mpg is 40 and decreases by 0.135 for each one unit increase in disp.\nFor 6 cylinder cars, the baseline mpg is about 21.5 and isn’t really related to the displacement.\nFor 8 cylinder cars, the baseline mpg is about 24.5 and decreases by about 0.02 for each one-unit increase in displacement.\n\nNote that displacement has really large units, so 0.02 over hundreds of one-unit increases is still a lot!"
  },
  {
    "objectID": "L20-Degrees_of_Freedom.html#generating-data-for-the-project",
    "href": "L20-Degrees_of_Freedom.html#generating-data-for-the-project",
    "title": "19  L20: Degrees of Freedom",
    "section": "19.1 Generating Data (for the Project)",
    "text": "19.1 Generating Data (for the Project)\n\nmodel.matrix() and Matrix Multiplication\n\nYou may find it convenient to use the model.matrix() function to set up dummy variables and polynomial terms for you. It will make sure that there is a reference category, which it will choose alphabetically.\nAlso note that you’ll want to set raw = TRUE to ensure that people can find the coefficient values that you set. With raw = FALSE they will get the exact same predictions (and thus the same error), but not the same coefficient values.\nIn the code below, I also demonstrate a log-transform for \\(y\\). To get a log on the left side of the equation, we use an exponential on the right.\n\n\nmycat &lt;- c(\"category\", \"category\", \"dogegory\", \"category\", \"birdegory\")\nmycont &lt;- c(12, 14, 4, 10,  20 )\n\nX &lt;- model.matrix(~ mycat + poly(mycont, 2, raw = TRUE))\nX\n\n  (Intercept) mycatcategory mycatdogegory poly(mycont, 2, raw = TRUE)1\n1           1             1             0                           12\n2           1             1             0                           14\n3           1             0             1                            4\n4           1             1             0                           10\n5           1             0             0                           20\n  poly(mycont, 2, raw = TRUE)2\n1                          144\n2                          196\n3                           16\n4                          100\n5                          400\nattr(,\"assign\")\n[1] 0 1 1 2 2\nattr(,\"contrasts\")\nattr(,\"contrasts\")$mycat\n[1] \"contr.treatment\"\n\n# Names are just for my own purposes, they don't need to be included.\n# The names help me keep track of which column in X they correspond to.\nbetas &lt;- c(intercpt = 10, cat = 20, dog = 0, c1 = 0, c2 = 0.05)\nX %*% betas\n\n  [,1]\n1 37.2\n2 39.8\n3 10.8\n4 35.0\n5 30.0\n\nmydf &lt;- data.frame(\n    y = exp(X %*% betas + rnorm(5, 0, 0.01)), \n    mycat = mycat, \n    mycont = mycont\n)\ncoef(lm(log(y) ~ mycat + poly(mycont, 2, raw = TRUE), data = mydf))\n\n                 (Intercept)                mycatcategory \n                  9.87889030                  19.93942368 \n               mycatdogegory poly(mycont, 2, raw = TRUE)1 \n                  0.03437018                   0.02583857 \npoly(mycont, 2, raw = TRUE)2 \n                  0.04902665 \n\n\nFor the project, you ould only need to submit the mydf object; the others should be able to recover your coefficient values from that alone!"
  },
  {
    "objectID": "L20-Degrees_of_Freedom.html#df",
    "href": "L20-Degrees_of_Freedom.html#df",
    "title": "19  L20: Degrees of Freedom",
    "section": "19.2 df",
    "text": "19.2 df\n\nDegrees of Freedom\nA measure of how much information you chose to put in the model.\n\nContinuous predictors add 1\nCategorical predictors add \\(k-1\\)\nEach term in a polynomial adds 1\nInteractions add 1\netc.\n\n\n\nChoose it and Use it\n\nDecide on the df, try to use all of them\n\nChoice is based on how much information you think you can extract.\nMany rows = much information; you can probably use more predictors.\n\n\n\n\nBad use of df\n\nDiscretising a categorical variable\n\nYou better have a reeeaaaalllllyyyy good justification\nFor example, discretising age to match up with insurance categories.\nYou should almost never choose to discretise based on your own logic; only to fit in with other analyses or use cases.\n\nPolynomial terms when interactions would work.\nRedundant categories\n\nCategories that are redundant\nIf you have redundant categories, then you have categories that are redundant\nFor example, if the “JobTitle” column has entries like “Data Scientist” as well as “Data Scientist and Machine Learning Expert”, you could just code those both as “DS”.\n\n\n\n\nSaving df\n\nCombine categories\n\nJust “Data Scientist” or “Software Engineer”\n\nCombine predictors\n\nVolume of beak = \\(\\pi r^2h/3\\)?\n\nTransform response rather than add polynomial terms\n\nNot always recommended.\n\n\n\nHere’s an example of using transformations to (1) match the context of the problem and (2) get better results with fewer degrees of freedom.\nWe’ll use the trees dataset that’s built into R. The “Girth” column is actually the diameter, and it’s the only column measured in inches rather than feet. I’m going to make a new predictor based on Girth that’s more useful for later models.\n\n# Saving df\nhead(trees) # \"Girth\" is actually diameter, according to help file\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\ntrees$Radius &lt;- trees$Girth/24\n\nA naive model might be a basic multiple linear regression.\n\nmultiple_lm &lt;- lm(Volume ~ Radius + Height, data = trees)\nsummary(multiple_lm)\n\n\nCall:\nlm(formula = Volume ~ Radius + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4065 -2.6493 -0.2876  2.2003  8.4847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\nRadius      112.9959     6.3424  17.816  &lt; 2e-16 ***\nHeight        0.3393     0.1302   2.607   0.0145 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.882 on 28 degrees of freedom\nMultiple R-squared:  0.948, Adjusted R-squared:  0.9442 \nF-statistic:   255 on 2 and 28 DF,  p-value: &lt; 2.2e-16\n\n\nWe could make this fit better by blindly adding polynomial terms and doing a transformation:\n\ntransformed_and_poly &lt;- lm(log(Volume) ~ poly(Girth, 2) + poly(Height, 2), data = trees)\nsummary(transformed_and_poly)\n\n\nCall:\nlm(formula = log(Volume) ~ poly(Girth, 2) + poly(Height, 2), \n    data = trees)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.16094 -0.04023 -0.00295  0.05474  0.13434 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3.27273    0.01492 219.360  &lt; 2e-16 ***\npoly(Girth, 2)1   2.51150    0.09732  25.808  &lt; 2e-16 ***\npoly(Girth, 2)2  -0.26046    0.09206  -2.829  0.00887 ** \npoly(Height, 2)1  0.54845    0.09746   5.628 6.47e-06 ***\npoly(Height, 2)2 -0.05518    0.09191  -0.600  0.55349    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08307 on 26 degrees of freedom\nMultiple R-squared:  0.9784,    Adjusted R-squared:  0.9751 \nF-statistic: 294.5 on 4 and 26 DF,  p-value: &lt; 2.2e-16\n\n\n\nIt is interesting that the squared term for height is not significant. Why is this so interesting to me? Look at the next equation in this lesson…\n\nA slightly better model might be one of the form \\[\nV = \\pi r^2h\n\\] which assumes that trees are perfect cylinders. This can be accomplished by modelling: \\[\\begin{align*}\n\\log(V) &= \\beta_0 + \\beta_1\\log(r) + \\beta_2\\log(h) + \\epsilon\\\\\n\\implies V &= \\exp(\\beta_0)r^\\beta_1h^\\beta_2\\exp(\\epsilon)\n\\end{align*}\\] and expecting that \\(\\exp(\\beta_0)\\) is close to \\(\\pi\\), \\(\\beta_1 = 2\\), and \\(\\beta_2 = 1\\).1\n\nvolume_logs &lt;- lm(log(Volume) ~ log(Girth) + log(Height), data = trees)\nsummary(volume_logs)\n\n\nCall:\nlm(formula = log(Volume) ~ log(Girth) + log(Height), data = trees)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.168561 -0.048488  0.002431  0.063637  0.129223 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.63162    0.79979  -8.292 5.06e-09 ***\nlog(Girth)   1.98265    0.07501  26.432  &lt; 2e-16 ***\nlog(Height)  1.11712    0.20444   5.464 7.81e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08139 on 28 degrees of freedom\nMultiple R-squared:  0.9777,    Adjusted R-squared:  0.9761 \nF-statistic: 613.2 on 2 and 28 DF,  p-value: &lt; 2.2e-16\n\n\n\nWe get something close to our hopes!\n\nExcept for \\(\\beta_0\\), which we’ll talk about later.\n\nWith this model, we could do a hypothesis test for \\(\\beta_1 = 2\\) and \\(\\beta_2 = 1\\).\n\nIf these are reasonable values, loggers could confidently calculate the volume of a tree assuming that it’s a cylinder!\n\n\nWe could also assume these values from the start, and include a “naive” volume.\n\ntrees$naive_volume &lt;- pi * trees$Radius^2 * trees$Height\n\nWe could then model this according to \\[\\begin{align*}\nV & = \\beta_0N + \\epsilon\n\\end{align*}\\] where \\(N\\) is our “naive” volumne. We might have the expectation that \\(\\beta_0 = 1\\) if the naive volume is correct.\n\n# Assuming trees are cylinders\ndiff_from_cylinder &lt;- lm(Volume ~ -1 + naive_volume, data = trees)\nsummary(diff_from_cylinder)\n\n\nCall:\nlm(formula = Volume ~ -1 + naive_volume, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6696 -1.0832 -0.3341  1.6045  4.2944 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \nnaive_volume 0.386513   0.004991   77.44   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.455 on 30 degrees of freedom\nMultiple R-squared:  0.995, Adjusted R-squared:  0.9949 \nF-statistic:  5996 on 1 and 30 DF,  p-value: &lt; 2.2e-16\n\n\nThis tells us that the estimated usable lumber from a given tree is about 40% of what we would expect if the tree were a perfect cylinder.\nImportantly for this lecture, we have an \\(R^2\\) of 0.9949 on a single degree of freedom! \\(R^2\\) is not the greatest measure, but it’s informative in this case:\n\n\n\n\n\nmodel\nR2\ndf\n\n\n\n\nmultiple_lm\n0.9442322\n2\n\n\ntransformed_and_poly\n0.9750853\n4\n\n\nvolume_logs\n0.9760840\n2\n\n\ndiff_from_cylinder\n0.9948560\n1\n\n\n\n\n\nBy choosing our transformations carefully, we have a model that is both better and simpler! The coefficient estimate also relates to a physical quantity that is useful to us - the percent of usable wood we can get from a tree! Statistics is amazing.2\n\n\n\nResearcher Degrees of Freedom\nYou add information that isn’t measured by df!\n\nChoosing one predictor rather than another.\n\nBill length or bill depth?\n\nTransforming a predictor/response\nRemoving outliers\nUsing/not using autoregressive error structures\netc.\n\nThis is why we use RMarkdown/Quarto/Jupyter - all of this is (should be) recorded!"
  },
  {
    "objectID": "L20-Degrees_of_Freedom.html#footnotes",
    "href": "L20-Degrees_of_Freedom.html#footnotes",
    "title": "19  L20: Degrees of Freedom",
    "section": "",
    "text": "It makes me very happy that we’re taking the “log” when talking about lumber.↩︎\nT-shirt idea: “If you don’t think stats is lit af then you ain’t woke, fam!”↩︎"
  },
  {
    "objectID": "L21-Regularization.html#preamble",
    "href": "L21-Regularization.html#preamble",
    "title": "20  L21: Regularization Methods",
    "section": "20.1 Preamble",
    "text": "20.1 Preamble\n\nAnnouncements\n\nWork through the lab code prior to the lab!\n\nGet your hands dirty!\n\nCourse Notes updated with some extras from last class."
  },
  {
    "objectID": "L21-Regularization.html#regularization-basics",
    "href": "L21-Regularization.html#regularization-basics",
    "title": "20  L21: Regularization Methods",
    "section": "20.2 Regularization Basics",
    "text": "20.2 Regularization Basics"
  },
  {
    "objectID": "L21-Regularization.html#loss-functions",
    "href": "L21-Regularization.html#loss-functions",
    "title": "20  L21: Regularization Methods",
    "section": "20.3 Loss Functions",
    "text": "20.3 Loss Functions\n\nWhy are we minimizing the sum of squares?\nThe MSE is defined as: \\[\nMSE(\\underline\\beta) = \\frac{1}{n}\\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\n\\] This is the Maximum Likelihood Estimate, which seeks to model the mean of \\(Y\\) at each value of \\(X\\), \\(E(Y) = X\\underline\\beta\\), with Gaussian errors. \nMSE, seen as a function of \\(\\underline\\beta\\), is a loss function, i.e. the function we minimize to find our estimates.\nBut it’s FAR from the only loss function.\n\n\nOther loss functions\nBy minimizing \\[\nMAE(\\underline\\beta) = \\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right|\n\\] we end up estimating the \\(median\\) of \\(Y\\). \nOthers:\n\nMean Absolute Log Error\n\nLower penalty for larger errors.\nMore robust to outliers?\n\nMean Relative Error\n\nPenalize errors relative to size of \\(y\\) (larger errors at large \\(y\\) values aren’t as big of a deal).\nAssumes that variance depends on mean (kinda like Poisson).\n\netc.\n\n\n\nExamples\n\n0.5 hectares verus 4 hectares can make a huge difference\n\n100 versus 150, congrats on the great prediction!\n\nPredicting an income of 15,000 versus 25,000 is big\n\nModelling the average income is not usually reasonable.\n\n\n\n\nLoss Function Summary\n\nMinimize the loss function with respect to the parameters of interest.\nFor the same parameters, there can be many loss functions.\nOther names:\n\nLikelihood function (special case of loss function)\nCost function (synonym)"
  },
  {
    "objectID": "L21-Regularization.html#regularization",
    "href": "L21-Regularization.html#regularization",
    "title": "20  L21: Regularization Methods",
    "section": "20.4 Regularization",
    "text": "20.4 Regularization\n\nRegular Methods\nOrdinary least squares is a minimization problem: \\[\nRSS = \\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\n\\]\nWhat if I don’t like how big the \\(\\underline\\beta\\) values are?\n\n\nRegular Methods\nOrdinary least squares is a minimization problem: \\[\nRSS = \\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\mathlarger{\\mathlarger{\\beta}}_jx_{ij}\\right)^2\n\\]\nNo I wanted smaller.\n\n\nRegular Methods\nOrdinary least squares is a minimization problem: \\[\nRSS = \\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\mathlarger{\\mathlarger{\\mathlarger{\\mathlarger{\\beta}}}}_jx_{ij}\\right)^2\n\\]\nDevan stop.\n\n\nRegular Methods\nOrdinary least squares is a minimization problem: \\[\nRSS = \\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\mathlarger{\\mathlarger{\\mathlarger{\\mathlarger{\\mathlarger{\\mathlarger{\\beta}}}}}}_jx_{ij}\\right)^2\n\\]\nDevan that’s enough.\n\n\nRegular Methods\nOrdinary least squares is a minimization problem: \\[\nRSS = \\sum_{i=1}^n\\left(y_i-\\beta_0-\\sum_{j=1}^p\\mathsmaller{\\mathsmaller{\\mathsmaller{\\mathsmaller{\\mathsmaller{\\mathsmaller{\\beta}}}}}}_jx_{ij}\\right)^2\n\\]\nOh wait that’s perfect.\n\n\nRegularization Constraints\nLet’s arbitrarily say that \\(\\sum_{j=1}^p \\beta_j = 10\\).\nWith this constraint, one large \\(\\beta_j\\) can be countered by a large negative \\(\\beta_k\\).\n\n\nRegularizing with Norms\nThe \\(L_p\\)-norm of a vector is: \\[\n||\\beta||_p = \\left(\\sum_{j=1}^p|\\beta|^p\\right)^{1/p}\n\\]\n\nWhen \\(p = 2\\), this is the Euclidean distance.\n\nPythagoras strikes again!\n\nWhen \\(p = 1\\), this is the sum of absolute values.\nWhen \\(p=\\infty\\), this ends up being max(\\(|\\underline\\beta|\\)).\n\nNot useful for our purposes, but interesting!\n\n\n\n\nWhy choose \\(||\\underline\\beta||_p = 10\\)?\nOr, in general, why choose a particular value of \\(s\\) in \\(||\\underline\\beta||_p = s\\)?\nThoughts?\n\n\nChoosing \\(s\\) in \\(||\\underline\\beta||_p = s\\)\n\nRecall: more flexible models are able to estimate more subtle patterns, but may find patterns that aren’t there.\n\nToo flexible = bad out-of-sample prediction.\n\nFor linear models, the least flexible model is one where all \\(\\beta_j\\) values are given a fixed value.\n\nFor example, all are 0. \n\n\nFor a linear model, restricting the values with \\(||\\underline\\beta||_p = s\\) reduces flexibility, which can improve out-of-sample prediction performance.\n\n\nMLE estimates of \\(\\underline\\beta\\) are unbiased\n… therefore constrained estimates are biased.\n\n\nBut what about the scales of the features?\nWhat a great question! Thank you so much for asking! You must be smart.\nFor \\(||\\underline\\beta||_p\\) to make sense, the predictors must all have the same scale.\nThis is accomplished by standardizing the features: Replace each \\(x_{ij}\\) with \\[\n\\frac{x_{ij} - \\bar \\mathbf x_{j}}{\\frac{1}{n}\\sum_{i=1}^n(x_{ij} - \\bar \\mathbf x_{j})^2}\n\\]\n\n\nChoosing \\(\\lambda\\) via cross-validation\n\nFor each value of \\(\\lambda\\):\n\nSplit data into 5 “folds”.\nFor each “fold”:\n\nSet aside the data points in the current fold.\nFit the model to data in all other “folds” using your value of \\(\\lambda\\).\nPredict the missing points, record the average error.\n\n\n\nChoose the lambda with the lowest out-of-sample prediction error.\n\n\nCross-Validation\n\n\n\nSpecial Cases of Regularization: \\(L_1\\) or \\(L_2\\)?\nSo far, we’ve been talking about general \\(L_p\\) norms, i.e. \\(||\\underline\\beta||_p\\).\n\n\\(L_1\\): LASSO\n\\(L_2\\): Ridge\n\n\n\nGeometric Interpretation (Contours of the RSS)\n\n\n\n\n\nLASSO will set coefficients to 0.\n\n“Least Absolute Shrinkage and Selection Operator”\n\nRidge has less variance (why?)\n\n\n\n\n\nLangrangian Multipliers and Estimation\nWikipedia screenshot:\n\n\n\nLagrangian Multipliers and Estimation\nMinimize \\(MSE(\\underline\\beta)\\) subject to \\(||\\underline\\beta||_p\\).\nis equivalent to\nMinimize \\(MSE(\\underline\\beta) + \\lambda||\\underline\\beta||_p\\)\nFor the rest of your life, this is the way you’ll see Ridge and LASSO.\n\nRidge: Analytical solution, can calculate an arbitrary number of \\(\\lambda\\) values at once.\nLASSO: Non-iterative numerical technique\n\n\n\nRidge Regularization\n\nOne of the coefficients increases with a tighter constraint!\n\n\n\nLASSO Feature Selection as we Vary \\(\\lambda\\)\n\n\n\nAs \\(\\lambda\\) increases, more coefficients are allowed to be non-zero.\nIf \\(\\lambda\\) doesn’t constrain, we get the least squares estimate.\n\nDenoted as \\(\\hat\\beta\\) in the plot."
  },
  {
    "objectID": "L21-Regularization.html#participation-questions",
    "href": "L21-Regularization.html#participation-questions",
    "title": "20  L21: Regularization Methods",
    "section": "20.5 Participation Questions",
    "text": "20.5 Participation Questions\n\nQ1\nFor Ridge regression (L2 norm): as \\(\\lambda\\rightarrow\\infty\\), \\(\\sum_{j=1}^p|\\beta_j| \\rightarrow\\infty\\).\n\nTrue\nFalse\n\n\n\nQ2\nWhen we set the restriction on the sum of the absolute value of the coefficients, the intercept is included.\n\nTrue\nFalse\n\n\n\nQ3\nWhy would we restrict the value of the parameters?\n\nDepending on the context of the problem, we might have a specific maximum value for the sum of the coefficients.\nWe want to avoid overfitting the data, and restricting the parameter stops us from modelling irrelevant patterns.\nBecause in any given regression problem there are always predictors that should have a slope of 0.\nBecause some parameter estimates are impossible.\n\n\n\nQ4\nWhat’s the primary practical difference between Ridge and LASSO?\n\nLASSO has a tighter restriction on the parameters than Ridge.\nLASSO will set parameters to 0, whereas Ridge will just shrink them towards 0 without making them exactly 0.\nBecause coefficients are set to 0, LASSO has higher variance in its estimates.\nThe only difference is the norm that they use, which has no meaningful impact on the results.\n\n\n\nQ5\nWhat is cross-validation trying to minimize?\n\nThe out-of-sample prediction error.\nThe bias in the estimates.\nThe loss function (MSE).\n\\(\\mathlarger{\\mathlarger{\\beta}}\\) \\(\\beta\\) \\(\\mathsmaller{\\beta}\\) \\(\\mathsmaller{\\mathsmaller{\\beta}}\\)\n\n\n\nQ6\nWhen should you use regularization?\n\nWhen you want un unbiased estimate of the population parameters.\nWhen you want to avoid overfitting.\nWhen you want to be regular/normal/usual/typical.\n\n\n\nQ7\nWhen should you use LASSO instead of Ridge?\n\nWhen you want to do subset selection in a way that minimizes out-of-sample prediction error.\nWhen you want most of the coefficients to be 0.\nWhen you want to minimize out-of-sample prediction error at all costs.\nWhen you only want coefficients with significant p-values left over in your model.\n\n\n\nPersonal Opinion Time\nWith the existence of LASSO, there’s no reason to do automated feature selection.\nBest subset selection can be written as: \\[\n\\text{Minimize } MSE(\\underline\\beta)\\text{ subject to }\\sum_{j=1}^pI(\\beta\\ne 0) \\le s\n\\] This can minimize out-of-sample error, but results in something that could be mistaken for inference.\nWith LASSO, you know the estimates are biased and you know why. Best subset tricks you into thinking your \\(\\underline\\beta\\) estimates are accurate - they are not.\n\n\nImplementation in R: glmnet\n\nThe glm in glmnet is because it fits all GLMs.\n\nIncluding Logistic Regression.\nThe family = binomial argument works as in glm()\n\nHowever, family = \"binomial\" is an optimized version.\n\n\nThe net in glmnet refers to elasticnet.\n\nNext slide or two.\n\n\n\n\nElastic Net: Like a lasso, but more “flexible”\n\\[\n\\text{Minimize } MSE(\\underline\\beta) + \\lambda\\left[\\alpha||\\underline\\beta||_1 + (1-\\alpha)||\\underline\\beta||_2\\right]\n\\]\nElastic Net is “doubly regularized”.\nElastic net needs more time to fit and needs more data.\n\n\nElasticnet and LASSO/Ridge\n\\[\n\\text{Minimize } MSE(\\underline\\beta) + \\lambda\\left[\\alpha||\\underline\\beta||_1 + (1-\\alpha)||\\underline\\beta||_2\\right]\n\\]\n\n\\(\\alpha = 0 \\implies\\) Ridge\n\\(\\alpha = 1 \\implies\\) LASSO\n\n\nHere’s an example of LASSO in R. We’ll load in the Wage data from ISLR2 package1.\nThis data set has a column for wage and a column for logwage. We’re going to use wage as our response, and removing wage makes it easier to tell R to use all columns other than logwage. I also remove region since there are some regions with too few observations and I am not going to set up cross-validation appropriately for this scenario.\n\nlibrary(glmnet) # cv.glmnet() and glmnet()\nlibrary(ISLR2) # Wage data set\n\nWage &lt;- ISLR2::Wage\n# From names(Wage), I want to remove \"region\" and \"wage\"\nWage &lt;- Wage[, -c(6, 11)]\n\nglmnet doesn’t use the formula notation (y ~ x); we have to manually set up the design matrix (including dummy variables) and the response vector.\n\nX &lt;- model.matrix(logwage ~ ., data = Wage)[,-1]\ny &lt;- as.numeric(Wage$logwage)\n\nThe first step to fitting a LASSO model is choosing \\(\\lambda\\) via cv. The cv.glmnet() function does this for us. The results are not a final model; the resultant object gives us an idea of which value of \\(\\lambda\\) is appropriate.\n\ncv_check &lt;- cv.glmnet(x = X, y = y, alpha = 1)\nplot(cv_check)\n\n\n\n\nThe first dotted line indicates the value of \\(\\lambda\\) that minimizes the “loss function.” However, across different samples we would get different values of \\(\\lambda\\). BEcause we know there’s randomness, we know that a slightly larger (more restrictive) value of \\(\\lambda\\) would also be consistent with our data. Since cross-validation emulates the idea of having many samples, we can get an estimate of the standard error of \\(\\lambda\\). We can then choose the alue of \\(\\lambda\\) that is within 1 standard error of the minimum. This gives a much simpler model while still having a plausible \\(\\lambda\\).2\nNow that we have a way of telling R what value we want for lambda, we cna fit the model.\n\nmy_lasso &lt;- glmnet(X, y, lambda = cv_check$lambda.1se)\nmy_lasso\n\n\nCall:  glmnet(x = X, y = y, lambda = cv_check$lambda.1se) \n\n  Df  %Dev Lambda\n1 10 34.91 0.0153\n\n\nThe output isn’t very informative, but the model can make predictions via the predict() function and these will be comparable or better than the predictions from an unconstrained linear model.\nLet’s compare the coefficient values to see the shrinkage in action! Of course, glmnet standardizes by default, so we need to ensure that the linear model is based on standardized predictors.\nIn the output, I include a column for the difference in the coefficients. Specifically, it’s lm minus lasso, so we may expect “shrinkage” to mean that the lasso estimates are smaller.\n\nstandardized_X &lt;- apply(X, 2, scale)\nstandardized_lm &lt;- lm(y ~ standardized_X)\ncoef_mat &lt;- cbind(coef(my_lasso),\n    coef(standardized_lm))\n\nres &lt;- cbind(\n        coef_mat, \n        apply(coef_mat, 1, function(x) abs(x[2]) - abs(x[1]))\n    ) |&gt; \n    round(3)\ncolnames(res) &lt;- c(\"lasso\", \"lm\", \"|lm|-|lasso|\")\nres\n\n17 x 3 sparse Matrix of class \"dgCMatrix\"\n                             lasso     lm |lm|-|lasso|\n(Intercept)                 -5.272  4.654       -0.618\nyear                         0.005  0.026        0.021\nage                          0.002  0.029        0.027\nmaritl2. Married             0.125  0.076       -0.049\nmaritl3. Widowed             .      0.004        0.004\nmaritl4. Divorced            .      0.012        0.012\nmaritl5. Separated           .      0.017        0.017\nrace2. Black                 .     -0.012        0.012\nrace3. Asian                 .     -0.005        0.005\nrace4. Other                 .     -0.007        0.007\neducation2. HS Grad         -0.015  0.038        0.023\neducation3. Some College     0.033  0.075        0.042\neducation4. College Grad     0.143  0.119       -0.024\neducation5. Advanced Degree  0.290  0.151       -0.139\njobclass2. Information       0.015  0.013       -0.003\nhealth2. &gt;=Very Good         0.037  0.027       -0.010\nhealth_ins2. No             -0.188 -0.089       -0.099\n\n\n\nThe estimates aren’t all smaller! Lasso chose to set some to 0, which freed up some coefficient “budget” to spend elsewhere."
  },
  {
    "objectID": "L21-Regularization.html#footnotes",
    "href": "L21-Regularization.html#footnotes",
    "title": "20  L21: Regularization Methods",
    "section": "",
    "text": "ISLR stands for Introduction to Statistical Learning with R, a fantastic (and free) book if you want to learn more advanced topics in predictive modelling!↩︎\nThis is similar to the Box-Cox transformation, where we find a bunch of plausible transformations, and go with a simple one like \\log() or sqrt().↩︎"
  },
  {
    "objectID": "Lb02-OLS_Estimates.html#analysis-of-variance",
    "href": "Lb02-OLS_Estimates.html#analysis-of-variance",
    "title": "21  Lab 2: OLS Estimates",
    "section": "21.1 Analysis of Variance",
    "text": "21.1 Analysis of Variance\nThe code below demonstrates how the ANOVA tables are calculated.\n\ny &lt;- mydata$y\nyhat &lt;- b0 + b1*mydata$x\nybar &lt;- mean(y)\n\nc(sum((yhat - ybar)^2), sum((y - yhat)^2), sum((y - ybar)^2))\n\n[1] 4809.33809   30.93852 4840.27661\n\nANOVA &lt;- data.frame(\n    Source = c(\"Regression\", \"Error\", \"Total (cor.)\"),\n    df = c(1, nrow(mydata) - 2, nrow(mydata)),\n    SS = c(sum((yhat - ybar)^2), sum((y - yhat)^2), sum((y - ybar)^2))\n)\nANOVA$MS &lt;- ANOVA$SS / ANOVA$df\nANOVA\n\n        Source df         SS          MS\n1   Regression  1 4809.33809 4809.338089\n2        Error 28   30.93852    1.104947\n3 Total (cor.) 30 4840.27661  161.342554\n\n\nThis is equivalent to what R’s built-in functions do!\n\nanova(lm(y ~ x, data = mydata))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 4809.3  4809.3  4352.5 &lt; 2.2e-16 ***\nResiduals 28   30.9     1.1                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Lb02-OLS_Estimates.html#dependence-and-centering",
    "href": "Lb02-OLS_Estimates.html#dependence-and-centering",
    "title": "21  Lab 2: OLS Estimates",
    "section": "21.2 Dependence and Centering",
    "text": "21.2 Dependence and Centering\nSomething not touched on in class is that \\(cov(\\hat\\beta_0, \\hat\\beta_1)\\) is not 0! This should be clear from the formula got \\(\\hat\\beta_0\\), which is \\(\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\\).\nThe code below repeats what we did before, but with higher variance to better demonstrate the problem.\nIt also records the estimates based on centering \\(\\underline x\\). Notice how the formula for \\(\\hat\\beta_0\\) is no longer dependent on \\(\\hat\\beta_1\\) if the mean of \\(\\underline x\\) is 0!\n\nb1s &lt;- double(R)\nb0s &lt;- double(R)\nb1cs &lt;- double(R)\nb0cs &lt;- double(R)\nn &lt;- 25\nx &lt;- runif(n, 0, 10)\nxc &lt;- x - mean(x) # centered\n\nfor (i in 1:R) {\n    y &lt;- 2 - 2 * x + rnorm(25, 0, 4)\n    b1 &lt;- Sdotdot(x, y) / Sdotdot(x, x)\n    b0 &lt;- mean(y) - b1 * mean(x)\n    b1s[i] &lt;- b1\n    b0s[i] &lt;- b0\n\n    # Centered\n    y &lt;- 2 - 2 * xc + rnorm(25, 0, 4)\n    b1c &lt;- Sdotdot(xc, y) / Sdotdot(xc, xc)\n    b1cs[i] &lt;- b1c\n    b0c &lt;- mean(y) - b1 * mean(xc)\n    b0cs[i] &lt;- b0c\n}\n\npar(mfrow = c(1,2))\nplot(b1s, b0s)\nplot(b1cs, b0cs)"
  },
  {
    "objectID": "Lb03-MSE.html#best-estimator-of-sigma2",
    "href": "Lb03-MSE.html#best-estimator-of-sigma2",
    "title": "22  Lab 3: Bias/variance of \\(\\sigma^2\\)",
    "section": "22.1 Best estimator of \\(\\sigma^2\\)",
    "text": "22.1 Best estimator of \\(\\sigma^2\\)\nWe saw in the notes that \\(E(s^2) = \\sigma^2\\). Let’s explore why this might not be the best estimator.\nWe define the MSE of an estimator \\(\\theta\\) as \\(E((\\theta - \\hat\\theta^2)^2)\\). For the variance, this is \\(E((\\sigma^2 - s^2)^2)\\).\nLet’s simulate a bunch of linear models, calculate the standard deviations, and then calculate this quantity.\nWe’re going to focus on multiplicative bias, and you’ll see why at the end. This means that we’ll focus on estimators of the form \\(as^2\\).\n\nset.seed(2112)\n\nn &lt;- 30\nx &lt;- runif(n, 0, 10)\nbeta0 &lt;- -3\nbeta1 &lt;- -4\nsigma &lt;- 3\n\nreps &lt;- 1000\nesst &lt;- double(reps)\n\nfor (i in 1:reps) {\n    y &lt;- beta0 + beta1*x + rnorm(n, 0, sigma)\n    beta1 &lt;- sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)\n    beta0 &lt;- mean(y) - beta1 * mean(x)\n    yhat &lt;- beta0 + beta1 * x\n    e &lt;- y - yhat\n    esst[i] &lt;- sum(e^2)\n}\n\na &lt;- (n-4):(n+4)\nmse &lt;- double(length(a))\nbias2 &lt;- double(length(a))\nfor (i in seq_along(a)) {\n    mse[i] &lt;- mean((sigma^2 - esst / a[i])^2)\n    bias2[i] &lt;- (sigma^2 - mean(esst / a[i]))^2\n}\n\npar(mfrow = c(1,2))\nplot(a - n, mse, main = \"MSE = Bias^2 + Variance\")\nplot(a - n, bias2, main = \"Bias^2\")\nabline(h = 0)\n\n\n\n\nFrom these plots, we see that the lowest MSE, i.e. the lowest value of \\(E((\\sigma^2 - as^2)^2)\\), is at \\(a = 1/n\\). Note that this corresponds to the MLE of \\(\\sigma^2\\). However, this is a biased estimate, and the unbiased estimate occurs at \\(a=1/(n-2)\\).\nWhat’s happening here? Shouldn’t unbiased be best? Well, yes, if our criteria is minimizing bias! If we want to minimize \\(E((\\sigma^2 - \\hat\\sigma^2)^2)\\), we have to account for the variance of the estimator across all possible samples as well!\nHWK: Modify the code to show that the bias of the constant model (\\(\\beta_1 = 0\\)) is minimized at \\(n-1\\), with the MSE being minimized at \\(n+1\\). It’s bizarre, but that’s how it works!"
  },
  {
    "objectID": "Lb03-MSE.html#residuals",
    "href": "Lb03-MSE.html#residuals",
    "title": "22  Lab 3: Bias/variance of \\(\\sigma^2\\)",
    "section": "22.2 Residuals",
    "text": "22.2 Residuals\nThe plot.lm() function makes most of the plots you’ll need.\n\nmylm &lt;- lm(mpg ~ disp, data = mtcars)\nplot(mylm, which=1:6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe broom package will be very useful in the future. In particular, the augment() function results in a tidy data frame with columns that are very relevant to our analyses.\n\nlibrary(broom)\n\nhead(augment(mylm))\n\n# A tibble: 6 × 9\n  .rownames           mpg  disp .fitted .resid   .hat .sigma .cooksd .std.resid\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1 Mazda RX4          21     160    23.0  -2.01 0.0418   3.29 0.00865     -0.630\n2 Mazda RX4 Wag      21     160    23.0  -2.01 0.0418   3.29 0.00865     -0.630\n3 Datsun 710         22.8   108    25.1  -2.35 0.0629   3.28 0.0187      -0.746\n4 Hornet 4 Drive     21.4   258    19.0   2.43 0.0328   3.27 0.00983      0.761\n5 Hornet Sportabout  18.7   360    14.8   3.94 0.0663   3.22 0.0558       1.25 \n6 Valiant            18.1   225    20.3  -2.23 0.0313   3.28 0.00782     -0.696\n\n\n\nglance(mylm)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.718         0.709  3.25      76.5 9.38e-10     1  -82.1  170.  175.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nanova(mylm)\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndisp       1 808.89  808.89  76.513 9.38e-10 ***\nResiduals 30 317.16   10.57                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nx &lt;- rnorm(1000); qqnorm(x); qqline(x)"
  },
  {
    "objectID": "Lb04-R_Matrix_Form.html#verifying-matrix-results",
    "href": "Lb04-R_Matrix_Form.html#verifying-matrix-results",
    "title": "23  Lab 4: Verifying Matrix Identities",
    "section": "23.1 Verifying Matrix Results",
    "text": "23.1 Verifying Matrix Results\nWe’ll use the mtcars data for this. Here’s what it looks like:\n\nx &lt;- mtcars$disp\ny &lt;- mtcars$mpg\n\nplot(y ~ x)\nabline(lm(y ~ x))\n\n\n\n\nIt looks like the slope is negative, and the intercept will be somewhere between 25 and 35.\nLet’s use the formulae from the previous course: \\(\\hat\\beta_1 = S_{XY}/S_{XX}\\) and \\(\\hat\\beta_0 = \\bar y - \\hat\\beta_1\\bar x\\).\n\nb1 &lt;- sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)\nb0 &lt;- mean(y) - mean(x) * b1\n\nmatrix(c(b0, b1))\n\n            [,1]\n[1,] 29.59985476\n[2,] -0.04121512\n\n\nTo make the matrix multiplication to work, we need \\(X\\) to be a column of 1s and a column representing our covariate.\n\nX &lt;- cbind(1, x)\nhead(X)\n\n         x\n[1,] 1 160\n[2,] 1 160\n[3,] 1 108\n[4,] 1 258\n[5,] 1 360\n[6,] 1 225\n\n\nThe estimates should be \\((X^TX)^{-1}X^T\\underline y\\). In R, we find the transpose with the t() function and we find inverse with the solve() function.\n\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nbeta_hat\n\n         [,1]\n  29.59985476\nx -0.04121512\n\n\nIt works!\nNow let’s check the ANOVA table!\n\nn &lt;- length(x)\ndata.frame(source = c(\"Regression\", \"Error\", \"Total\"),\n    df = c(2, n-2, n),\n    SS = c(t(beta_hat) %*% t(X) %*% y, \n        t(y) %*% y - t(beta_hat) %*% t(X) %*% y,\n        t(y) %*% y)\n)\n\n      source df         SS\n1 Regression  2 13725.1513\n2      Error 30   317.1587\n3      Total 32 14042.3100\n\n\n\nanova(lm(y ~ x))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nx          1 808.89  808.89  76.513 9.38e-10 ***\nResiduals 30 317.16   10.57                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncolSums(anova(lm(y ~ x)))\n\n       Df    Sum Sq   Mean Sq   F value    Pr(&gt;F) \n  31.0000 1126.0472  819.4605        NA        NA \n\n\nThey’re slightly different? Why?\nBecause the equation in the textbook is for the uncorrected sum of squares, which basically means we’re looking at estimating both \\(\\beta_0\\) and \\(\\beta_1\\) at the same time (hence the df of \\(n-2\\)). The usual ANOVA table is the corrected sum of squares, which the textbook labels \\(SS(\\hat\\beta_1|\\hat\\beta_1)\\) to make it clear that it’s estimating \\(\\beta_1\\) only; \\(\\beta_0\\) has already been estimated.\n\nn &lt;- length(x)\ndata.frame(source = c(\"Regression\", \"Error\", \"Total\"),\n    df = c(1, n-1, n),\n    SS = c(t(beta_hat) %*% t(X) %*% y - n * mean(y)^2, \n        t(y) %*% y - t(beta_hat) %*% t(X) %*% y,\n        t(y) %*% y - n * mean(y)^2)\n)\n\n      source df        SS\n1 Regression  1  808.8885\n2      Error 31  317.1587\n3      Total 32 1126.0472\n\n\nThe matrix form for \\(R^2\\) is a little different from what you might expect. It uses this idea of “corrected” sum-of-squares as well. For homework, verify that the corrected sum-of-squares works out to the same formula.\nHere’s how to extract the \\(R^2\\) value from R (note that the programming language R has nothing to do with the \\(R^2\\); R is named after S, which was the programming language that came before it (both chronologically and alphabetically); you’ll still find references to S and S-Plus).\n\nsummary(lm(y ~ x))$r.squared\n\n[1] 0.7183433\n\n\nIn the textbook, the formula is given as: \\[\nR^2 = \\frac{\\hat{\\underline\\beta}^TX^T\\underline y - n\\bar y^2}{\\underline y^t\\underline y - n\\bar y^2}\n\\]\n\nnumerator &lt;- t(beta_hat) %*% t(X) %*% y - n * mean(y)^2\ndenominator &lt;- t(y) %*% y - n * mean(y)^2\nnumerator / denominator\n\n          [,1]\n[1,] 0.7183433"
  },
  {
    "objectID": "Lb04-R_Matrix_Form.html#section",
    "href": "Lb04-R_Matrix_Form.html#section",
    "title": "23  Lab 4: Verifying Matrix Identities",
    "section": "23.2 ",
    "text": "23.2"
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html#basic-anova",
    "href": "Lb05-ANOVA-R2-F_test-CI.html#basic-anova",
    "title": "24  Lab 5: ANOVA",
    "section": "24.1 Basic ANOVA",
    "text": "24.1 Basic ANOVA\n\nset.seed(2221)\nplot(mpg ~ disp, data = mtcars)\nabline(lm(mpg ~ disp, data = mtcars))\n\n\n\n\n\nanova(lm(mpg ~ qsec, data = mtcars))\n\nAnalysis of Variance Table\n\nResponse: mpg\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nqsec       1 197.39 197.392  6.3767 0.01708 *\nResiduals 30 928.66  30.955                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsummary(lm(mpg ~ qsec, data = mtcars))$coef\n\n             Estimate Std. Error    t value   Pr(&gt;|t|)\n(Intercept) -5.114038 10.0295433 -0.5098974 0.61385436\nqsec         1.412125  0.5592101  2.5252133 0.01708199\n\n\nNotice the p-values! Also notice that the \\(F\\)-value is the square of the \\(t\\)-value! It’s like magic! Math is cool."
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html#r2-always-increases-with-new-predictors",
    "href": "Lb05-ANOVA-R2-F_test-CI.html#r2-always-increases-with-new-predictors",
    "title": "24  Lab 5: ANOVA",
    "section": "24.2 \\(R^2\\) always increases with new predictors",
    "text": "24.2 \\(R^2\\) always increases with new predictors\n\nnx &lt;- 10 # Number of uncorrelated predictors\nuncorr &lt;- matrix(rnorm(50*(nx + 1)), \n    nrow = 50, ncol = nx + 1)\n## First column is y, rest are x\ncolnames(uncorr) &lt;- c(\"y\", paste0(\"x\", 1:nx))\nuncorr &lt;- as.data.frame(uncorr)\n\nrsquares &lt;- NA\nfor (i in 2:(nx + 1)) {\n    rsquares &lt;- c(rsquares,\n        summary(lm(y ~ ., data = uncorr[,1:i]))$r.squared)\n}\nplot(1:10, rsquares[-1], type = \"b\",\n    xlab = \"Number of Uncorrelated Predictors\",\n    ylab = \"R^2 Value\",\n    main = \"R^2 ALWAYS increases\")\n\n\n\n\nThe one exception is when one of the predictors is a linear combination of the previous predictors. In this case, \\(R^2\\) will not change!\n\nuncorr[, nx + 2] &lt;- uncorr[,2] + 3*uncorr[,3]\nrsquares &lt;- c(rsquares, summary(lm(y ~ ., data = uncorr))$r.squared)\nrsquares\n\n [1]          NA 0.003898013 0.056398466 0.056407225 0.069142075 0.073155890\n [7] 0.105824965 0.122449599 0.145307322 0.168746574 0.172758068 0.172758068\n\nplot(rsquares, type = \"b\")\n\n\n\n\n\nx &lt;- runif(20, 0, 10)\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nb0s &lt;- b1s &lt;- double(1000)\nplot(NA, pch = 0, \n    xlim = c(-2, 12), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nabline(h = b0 + b1*mean(x))\nabline(v = mean(x))\nfor (i in 1:1000) {\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    abline(lm(y ~ x), col = rgb(0,0,0,0.1))\n}\n\n\n\n\nLet’s do that again, but record the values and only show the 89% quantiles!\n\nx &lt;- runif(20, 0, 10)\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nall_lines &lt;- replicate(1000, {\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    predict(lm(y ~ x), newdata = list(x = seq(0, 10, 0.1)))\n})\n\neightnine &lt;- apply(all_lines, 1, quantile, probs = c(0.045, 0.945))\n\n\nplot(NA, pch = 0, \n    xlim = c(0, 10), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nlines(seq(0, 10, 0.1), eightnine[1, ])\nlines(seq(0, 10, 0.1), eightnine[2, ])\n\n\n\n\nNote that the theoretical calculation of these bounds is built into R:\n\nplot(NA, pch = 0, \n    xlim = c(0, 10), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nlines(seq(0, 10, 0.1), eightnine[1, ])\nlines(seq(0, 10, 0.1), eightnine[2, ])\n\n## Add the TRUE relationship\nxseq &lt;- seq(0, 10, 0.1)\nlines(xseq, b0 + b1*xseq, col = 3)\n\n## New sample from the data generating process\nx &lt;- runif(20, 0, 10)\ny &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n\n## Extract the CI\nmylm &lt;- lm(y ~ x)\nxbeta &lt;- predict(mylm, interval = \"confidence\",\n    newdata = list(x = xseq))\n#lines(xseq, xbeta[,\"fit\"], col = 4)\nlines(xseq, xbeta[,\"upr\"], col = 4)\nlines(xseq, xbeta[,\"lwr\"], col = 4)\n\n\n\n\nNote that the intervals won’t exactly align - the samples are going to be different each time! In 95% of the samples we collect from this data generating process, the CI we construct from the sample will contain the true (green) line. This is a basic definition for confidence intervals, but it’s neat to see it around a line.\nNotice how the CI is curved. This is completely, 100% expected. Recall that \\((\\bar x, \\bar y)\\) is always a point on the line. If \\(x\\) is the same for all samples, then the variance in the height at \\(\\bar y\\) is just the variance in \\(y\\). However, we can rotate the line around this point and still fit most of the data “pretty well”, which is where the curved nature of the line comes from!\n\nAside\nWhy did I use the same \\(x\\) values for all of the simulations? Because that’s part of the assumptions (this isn’t an important point to make). Again, notice how the point \\((\\bar x, \\bar y)\\) is always on the line, and how the variance at the point \\(\\bar x\\) is minimized. If \\(\\bar x\\) is randomly moved, then there’s extra variance in the line.\n\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nall_lines2 &lt;- replicate(1000, {\n    x &lt;- runif(20, 0, 10)\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    predict(lm(y ~ x), newdata = list(x = seq(0, 10, 0.1)))\n})\n\neightnine2 &lt;- apply(all_lines2, 1, quantile, probs = c(0.045, 0.945))\n\n\nplot(NA, pch = 0, \n    xlim = c(0, 10), ylim = c(-10, 65),\n    xlab = \"x\", ylab = \"y\",\n    main = \"Variance of the line\")\nlines(seq(0, 10, 0.1), eightnine[1, ])\nlines(seq(0, 10, 0.1), eightnine[2, ])\nlines(seq(0, 10, 0.1), eightnine2[1, ], col = 2)\nlines(seq(0, 10, 0.1), eightnine2[2, ], col = 2)\nlegend(\"topleft\", legend = c(\"non-random x\", \"random x\"), col = 1:2, lty = 1)\n\n\n\n\nThe textbook for this course also covers models that incorporate randomness in \\(X\\), but this is not covered in this course."
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html#covariance-of-the-parameters",
    "href": "Lb05-ANOVA-R2-F_test-CI.html#covariance-of-the-parameters",
    "title": "24  Lab 5: ANOVA",
    "section": "24.3 Covariance of the parameters",
    "text": "24.3 Covariance of the parameters\n\nb0 &lt;- 2; b1 &lt;- 5; sigma &lt;- 10\nall_params &lt;- replicate(1000, {\n    x &lt;- runif(20, 0, 10)\n    y &lt;- b0 + b1*x + rnorm(20, 0, sigma)\n    coef(lm(y ~ x))\n})\n\n\nplot(t(all_params))\n\n\n\n\nIntuition check: were you expecting a negative slope? Does this make sense? If you increase \\(\\beta_0\\), why would \\(\\beta_1\\) decrease?\nFor homework, try a negative intercept and see what happens! What about a negative slope?"
  },
  {
    "objectID": "Lb05-ANOVA-R2-F_test-CI.html#joint-normality-of-the-underlinehatbetas",
    "href": "Lb05-ANOVA-R2-F_test-CI.html#joint-normality-of-the-underlinehatbetas",
    "title": "24  Lab 5: ANOVA",
    "section": "24.4 Joint Normality of the \\(\\underline{\\hat\\beta}\\)s",
    "text": "24.4 Joint Normality of the \\(\\underline{\\hat\\beta}\\)s\nJoint normality leads to marginal normality! This means we can create a confidence interval based on the marginal. However, if the joint distribution has a strong correlation, the marginal confidence intervals might contain unlikely points!\n\npar(mfrow = c(1, 3))\n\n## Marginal distribution of beta_0\nplot(density(all_params[1,]),\n    main = \"Distribution of b_0\",\n    xlab = \"b_0\")\nabline(v = quantile(all_params[1,], c(0.055, 0.945)), col = 2)\nabline(v = 8, col = 3) # Hypothesized beta_0\n\n## Marginal distribution of beta_1\nplot(density(all_params[2,]),\n    main = \"Distribution of b_1\",\n    xlab = \"b_1\")\nabline(v = quantile(all_params[2,], c(0.055, 0.945)), col = 2)\nabline(v = 6, col = 3) # Hypothesized beta_1\n\n## Joint distribution\nplot(t(all_params), main = \"Joint Distribution\",\n    xlab = \"b_0\", ylab = \"b_1\")\nabline(v = quantile(all_params[1,], c(0.055, 0.945)), col = 2)\nabline(h = quantile(all_params[2,], c(0.055, 0.945)), col = 2)\npoints(x = 8, y = 6, col = 3, pch = 16, cex = 1.5)\n\n\n\n\nNotice how the rectangular confidence region in the joint distribution contains regions where there are no points! For example, a hypothesis test for whether \\(\\beta_0=8\\) and \\(\\beta_1 = 6\\) (the green lines/points) would not be rejected if we checked the two confidence intervals separately, but likely should be rejected given the joint distribution! This is exactly what happens when the F-test is significant but none of the t-tests for individual predictors is significant.\nIn general, the CIs for each individual \\(\\hat\\beta\\) are missing something - especially if there’s correlation in the predictors!\nIn these examples, we repeatedly sampled from the true relationship to obtain simulation-based confidence intervals. The normality assumption allows us to make inferences about the distribution of the parameters - including the joint distribution - from a single sample! Inference is powerful!"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#the-hat-matrix",
    "href": "Lb11-R_hat_resids_cook.html#the-hat-matrix",
    "title": "25  Lab 11: Hat Matrix and Residuals in R",
    "section": "25.1 The Hat Matrix",
    "text": "25.1 The Hat Matrix\n\\[\nH = X(X^TX)^{-1}X^T\n\\]\nIn R, we can calculate the diagonal of the hat matrix as follows:\n\nmylm &lt;- lm(mpg ~ disp + wt, data = mtcars)\nhatvalues(mylm) |&gt; unname()\n\n [1] 0.04339369 0.04550894 0.06309504 0.03877647 0.14078260 0.04406584\n [7] 0.11516157 0.09635365 0.09875274 0.11012510 0.11012510 0.08141444\n[13] 0.04168379 0.04521644 0.17206264 0.19889125 0.19275897 0.08015728\n[19] 0.12405357 0.09579747 0.05703451 0.06246825 0.05648077 0.06838477\n[25] 0.14119998 0.08720679 0.07149742 0.16032953 0.18794989 0.05044456\n[31] 0.04474121 0.07408572"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#extracting-the-diagonals-from-h",
    "href": "Lb11-R_hat_resids_cook.html#extracting-the-diagonals-from-h",
    "title": "25  Lab 11: Hat Matrix and Residuals in R",
    "section": "25.2 Extracting the Diagonals from H",
    "text": "25.2 Extracting the Diagonals from H\nThere isn’t a built-in function for the full hat matrix (the diagonals are usually all you’ll need). For demonstration, here are some demonstrations of the features of the hat matrix.\n\nX &lt;- model.matrix(mylm)\nH &lt;- X %*% solve(t(X) %*% X) %*% t(X)\nall.equal(diag(H), hatvalues(mylm))\n\n[1] TRUE"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#features-of-h",
    "href": "Lb11-R_hat_resids_cook.html#features-of-h",
    "title": "25  Lab 11: Hat Matrix and Residuals in R",
    "section": "25.3 Features of H",
    "text": "25.3 Features of H\nIn the code below, I use the unname() function because mtcars has rownames which make the output harder to see (this used to be the norm in R, but it’s fallen out of fashion).\n\ncolSums(H) |&gt; unname()\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\nrowSums(H) |&gt; unname()\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\nrange(H) # -1 &lt;= h_{ij} &lt;= 1\n\n[1] -0.1076609  0.1988913\n\nrange(diag(H)) # 0 &lt;= h_{ii} &lt;= 1\n\n[1] 0.03877647 0.19889125\n\nH %*% rep(1, ncol(H)) # H1 = 1\n\n                    [,1]\nMazda RX4              1\nMazda RX4 Wag          1\nDatsun 710             1\nHornet 4 Drive         1\nHornet Sportabout      1\nValiant                1\nDuster 360             1\nMerc 240D              1\nMerc 230               1\nMerc 280               1\nMerc 280C              1\nMerc 450SE             1\nMerc 450SL             1\nMerc 450SLC            1\nCadillac Fleetwood     1\nLincoln Continental    1\nChrysler Imperial      1\nFiat 128               1\nHonda Civic            1\nToyota Corolla         1\nToyota Corona          1\nDodge Challenger       1\nAMC Javelin            1\nCamaro Z28             1\nPontiac Firebird       1\nFiat X1-9              1\nPorsche 914-2          1\nLotus Europa           1\nFord Pantera L         1\nFerrari Dino           1\nMaserati Bora          1\nVolvo 142E             1"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#extracting-the-residuals",
    "href": "Lb11-R_hat_resids_cook.html#extracting-the-residuals",
    "title": "25  Lab 11: Hat Matrix and Residuals in R",
    "section": "25.4 Extracting the residuals",
    "text": "25.4 Extracting the residuals\nSee ?influence.measures.\n\n?rstandard\ncooks.distance(mylm)\n\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n       1.022220e-02        4.351414e-03        1.721743e-02        5.241995e-03 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n       2.027544e-02        3.089406e-03        3.094888e-02        3.443123e-02 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n       3.775416e-03        8.693734e-03        3.864780e-02        4.425052e-06 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n       1.330376e-04        9.458368e-03        1.920638e-02        3.794816e-02 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n       3.441118e-01        1.429900e-01        3.046404e-02        1.850525e-01 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n       2.372110e-02        1.146745e-02        2.036673e-02        2.070769e-02 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n       1.331763e-01        2.049673e-04        3.812246e-04        4.292901e-02 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n       5.996344e-02        2.547326e-02        1.362489e-02        1.494183e-02"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#the-broom-package",
    "href": "Lb11-R_hat_resids_cook.html#the-broom-package",
    "title": "25  Lab 11: Hat Matrix and Residuals in R",
    "section": "25.5 The broom package",
    "text": "25.5 The broom package\nThe broom package has a wonderful function called augment(). This function sets up our data so that it’s super easy to see what we need.\n\nlibrary(broom)\naugment(mylm)\n\n# A tibble: 32 × 10\n   .rownames     mpg  disp    wt .fitted .resid   .hat .sigma .cooksd .std.resid\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 Mazda RX4    21    160   2.62    23.3 -2.35  0.0434   2.93 0.0102      -0.822\n 2 Mazda RX4 …  21    160   2.88    22.5 -1.49  0.0455   2.95 0.00435     -0.523\n 3 Datsun 710   22.8  108   2.32    25.3 -2.47  0.0631   2.93 0.0172      -0.876\n 4 Hornet 4 D…  21.4  258   3.22    19.6  1.79  0.0388   2.95 0.00524      0.624\n 5 Hornet Spo…  18.7  360   3.44    17.1  1.65  0.141    2.95 0.0203       0.609\n 6 Valiant      18.1  225   3.46    19.4 -1.28  0.0441   2.96 0.00309     -0.448\n 7 Duster 360   14.3  360   3.57    16.6 -2.32  0.115    2.93 0.0309      -0.845\n 8 Merc 240D    24.4  147.  3.19    21.7  2.73  0.0964   2.92 0.0344       0.984\n 9 Merc 230     22.8  141.  3.15    21.9  0.890 0.0988   2.96 0.00378      0.322\n10 Merc 280     19.2  168.  3.44    20.5 -1.26  0.110    2.96 0.00869     -0.459\n# ℹ 22 more rows\n\n\nNotice that it includes:\n\n.fitted = \\(X\\underline{\\hat\\beta}\\)\n.resid = \\(\\hat{\\underline\\epsilon}\\)\n.hat = \\(diag(H)\\)\n.sigma = \\(s_{(i)}\\)\n.cooksd = D\n.std.resid = standardized or studentized residuals?"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#plotting-the-residuals",
    "href": "Lb11-R_hat_resids_cook.html#plotting-the-residuals",
    "title": "25  Lab 11: Hat Matrix and Residuals in R",
    "section": "25.6 Plotting the residuals",
    "text": "25.6 Plotting the residuals\nIf you’ve ever accidentally typed plot(mylm), you’ve seen some plots of the residuals.\n\npar(mfrow = c(2, 2))\nplot(mylm) # ?plot.lm\n\n\n\n\nYou can access individual plots with the which argument.\n\npar(mfrow = c(2, 3))\nplot(mylm, which = 1:6)\n\n\n\n\n99% of the time, the default plots are the ones you’ll want to look at. For teaching purposes, we’ll look at a few extra."
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#demonstrations",
    "href": "Lb11-R_hat_resids_cook.html#demonstrations",
    "title": "25  Lab 11: Hat Matrix and Residuals in R",
    "section": "25.7 Demonstrations",
    "text": "25.7 Demonstrations\nWe’ll use the Ozone data from the mlbench package.\n\nV4 is response (measurement of Ozone)\nV5 is atmospheric pressure\nV6 is wind speed\nV7 is humidity\nWe’ll ignore the rest.\n\n\nlibrary(mlbench)\ndata(Ozone)\nstr(Ozone) # V4 is response (measurement of Ozone)\n\n'data.frame':   366 obs. of  13 variables:\n $ V1 : Factor w/ 12 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ V2 : Factor w/ 31 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ V3 : Factor w/ 7 levels \"1\",\"2\",\"3\",\"4\",..: 4 5 6 7 1 2 3 4 5 6 ...\n $ V4 : num  3 3 3 5 5 6 4 4 6 7 ...\n $ V5 : num  5480 5660 5710 5700 5760 5720 5790 5790 5700 5700 ...\n $ V6 : num  8 6 4 3 3 4 6 3 3 3 ...\n $ V7 : num  20 NA 28 37 51 69 19 25 73 59 ...\n $ V8 : num  NA 38 40 45 54 35 45 55 41 44 ...\n $ V9 : num  NA NA NA NA 45.3 ...\n $ V10: num  5000 NA 2693 590 1450 ...\n $ V11: num  -15 -14 -25 -24 25 15 -33 -28 23 -2 ...\n $ V12: num  30.6 NA 47.7 55 57 ...\n $ V13: num  200 300 250 100 60 60 100 250 120 120 ...\n\nOzone &lt;- Ozone[complete.cases(Ozone), ]\n\nA small amount of exploration first:\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(patchwork)\ng5 &lt;- ggplot(Ozone) +\n    aes(x = V5, y = V4) +\n    geom_point()\ng6 &lt;- ggplot(Ozone) +\n    aes(x = V6, y = V4) +\n    geom_jitter() # deal with overlapping points\ng7 &lt;- ggplot(Ozone) +\n    aes(x = V7, y = V4) +\n    geom_point()\ng5 + g6 + g7\n\n\n\n\nNow let’s check some residuals!\n\nChange .resid to .std.resid.\n\nTry rstudent(olm) as well.\n\nChange .hat to .cooksd.\n\n\nlibrary(dplyr)\nolm &lt;- lm(V4 ~ V5 + V6 + V7, data = Ozone)\naugment(olm) %&gt;%\n    ggplot() +\n        aes(x = .fitted, y = .std.resid, col = .cooksd) +\n        scale_colour_viridis_c(option = 2, end = 0.7) +\n        theme(legend.position = \"bottom\") +\n        geom_point(size = 2) +\n        geom_hline(yintercept = 0, colour = \"grey\")\n\n\n\n\nWhich ones have a large hat value?\nThe plots below are the same as the ones above, but coloured according to the hat values.\n\ng5 &lt;- ggplot(Ozone) +\n    aes(x = V5, y = V4, col = hatvalues(olm)) +\n    geom_point(size = 2)\ng6 &lt;- ggplot(Ozone) +\n    aes(x = V6, y = V4, col = hatvalues(olm)) +\n    geom_jitter(size = 2) # deal with overlapping points\ng7 &lt;- ggplot(Ozone) +\n    aes(x = V7, y = V4, col = hatvalues(olm)) +\n    geom_point(size = 2)\n(g5 + g6 + g7) +\n    plot_layout(guides = \"collect\") &\n    scale_colour_viridis_c(option = 2, end = 0.8) &\n    theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#adding-an-outlier",
    "href": "Lb11-R_hat_resids_cook.html#adding-an-outlier",
    "title": "25  Lab 11: Hat Matrix and Residuals in R",
    "section": "25.8 Adding an Outlier",
    "text": "25.8 Adding an Outlier\nLet’s add an outlier to see what happens with these data.\n\nnewzone &lt;- Ozone[, c(4:7)]\nnewzone &lt;- rbind(newzone,\n    data.frame(V4 = 30, V5 = 5300, V6 = 5, V7 = 40))\nnewlm &lt;- augment(lm(V4 ~ ., data = newzone))\n\ng5 &lt;- ggplot(newlm) +\n    aes(x = V5, y = V4, col = .hat) +\n    geom_point(size = 2)\ng6 &lt;- ggplot(newlm) +\n    aes(x = V6, y = V4, col = .hat) +\n    geom_jitter(size = 2) # deal with overlapping points\ng7 &lt;- ggplot(newlm) +\n    aes(x = V7, y = V4, col = .hat) +\n    geom_point(size = 2)\n(g5 + g6 + g7) +\n    plot_layout(guides = \"collect\") &\n    scale_colour_viridis_c(option = 2, end = 0.8) &\n    theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#using-rs-built-in-diagnostics",
    "href": "Lb11-R_hat_resids_cook.html#using-rs-built-in-diagnostics",
    "title": "25  Lab 11: Hat Matrix and Residuals in R",
    "section": "25.9 Using R’s Built-In Diagnostics",
    "text": "25.9 Using R’s Built-In Diagnostics\n\npar(mfrow = c(2, 2), mar = rep(2, 4))\nplot(lm(V4 ~ ., data = newzone))\n\n\n\nnewzone[row.names(newzone) %in% c(1, 58, 243), ]\n\n    V4   V5 V6 V7\n58  23 5740  3 47\n243 38 5950  5 62\n1   30 5300  5 40\n\n\n\nHuge residual!\n\nThis plot also just has a bad pattern\n\nDeviates from normality!\n\nOtherwise this looks pretty good.\n\nLarge standardized residual\n\nClear pattern without the outlier\n\nCook’s distance is massive compared to the others\n\nPotentially some large \\(D_i\\)’s\n\nIn the corner\n\nOtherwise this looks okay-ish\n\nLast plot also shows it as something different (harder to interpret)"
  },
  {
    "objectID": "Lb11-R_hat_resids_cook.html#what-to-do-with-a-large-residual",
    "href": "Lb11-R_hat_resids_cook.html#what-to-do-with-a-large-residual",
    "title": "25  Lab 11: Hat Matrix and Residuals in R",
    "section": "25.10 What to do with a large residual?",
    "text": "25.10 What to do with a large residual?\n\nMisrecorded: remove or fix, if possible\n\nFires with negative lengths (MDY versus DMY)\nCO2 measured as -99 (code for NA in a system with no NA option)\nHeights measured in the wrong units\n\nReal, but large residual: Consider whether it’s actually part of the population of interest\n\nStudying heights and got a basketball player in your sample? That’s a real data point and your model should allow for it!\nStudying fish and a shark was included? That’s real, but maybe you should narrow your scope!\n\nMany large outliers: you may need to try more predictors or a non-linear model.\n\nDo NOT remove a point simply because it’s an outlier!"
  },
  {
    "objectID": "Lb12-Corr_in_Betas.html",
    "href": "Lb12-Corr_in_Betas.html",
    "title": "26  Lab 12: Extra Topics",
    "section": "",
    "text": "n_sim &lt;- 1000\nn &lt;- 30\nbetas &lt;- matrix(ncol = 3, nrow = n_sim)\nbetacs &lt;- betas\n\nfor (i in 1:n_sim) {\n    x1 &lt;- runif(n, 0, 10)\n    x2 &lt;- runif(n, 0, 10) + 2*x1\n    y &lt;- 3 - 8*x1 + 4*x2 + rnorm(n, 0, 4)\n    betas[i, ] &lt;- coef(lm(y ~ x1 + x2))\n\n    x1c &lt;- scale(x1)\n    x2c &lt;- scale(x2)\n    betacs[i, ] &lt;- coef(lm(y ~ x1c + x2c))\n}\n\npar(mfrow = c(2, 3))\nplot(betas[, c(1,3)])\nplot(betas[, c(1,3)])\nplot(betas[, c(2,3)])\nplot(betacs[, c(1,2)])\nplot(betacs[, c(1,3)])\nplot(betacs[, c(2,3)])"
  },
  {
    "objectID": "Lb13-Wrong_Model.html#missing-predictors",
    "href": "Lb13-Wrong_Model.html#missing-predictors",
    "title": "27  Lab 13: Missing/Extra Predictors",
    "section": "27.1 Missing Predictors",
    "text": "27.1 Missing Predictors\n\nTrue model: \\(Y = X\\underline\\beta + X_2\\underline\\beta_2 + \\underline\\epsilon\\)\nEstimated model: \\(Y = X\\underline\\beta + \\underline\\epsilon\\)\n\nWe’re going to do this a little differently than other days. Let’s look at the penguins data again:\n\nset.seed(2121)\nlibrary(palmerpenguins)\n## Remove NAs and take continuous variables\npenguins &lt;- subset(penguins, species == \"Chinstrap\")\npeng &lt;- penguins[complete.cases(penguins), c(3, 4, 5, 6)]\n## Standardize the x values\n#peng[, 1:3] &lt;- apply(peng[, 1:3], 2, scale)\nhead(peng)\n\n# A tibble: 6 × 4\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1           46.5          17.9               192        3500\n2           50            19.5               196        3900\n3           51.3          19.2               193        3650\n4           45.4          18.7               188        3525\n5           52.7          19.8               197        3725\n6           45.2          17.8               198        3950\n\n\nLet’s “““make”“” a true model:\n\npenglm &lt;- lm(body_mass_g ~ ., data = peng)\nbeta &lt;- coef(penglm)\nsigma &lt;- summary(penglm)$sigma\n\nbeta\n\n      (Intercept)    bill_length_mm     bill_depth_mm flipper_length_mm \n      -3157.53005          16.03916          91.51275          22.57975 \n\nsigma\n\n[1] 276.9998\n\n\nWe’ll use these values as if they are population values and forget that they were calculated from a sample.\n\nThe x-values will stay the same, we’ll simulate new \\(y\\) values according to this model.\nThe advantage of this approach is that the predictors retain any correlation that they had."
  },
  {
    "objectID": "Lb13-Wrong_Model.html#simulating-from-the-right-model",
    "href": "Lb13-Wrong_Model.html#simulating-from-the-right-model",
    "title": "27  Lab 13: Missing/Extra Predictors",
    "section": "27.2 Simulating from the “Right” model",
    "text": "27.2 Simulating from the “Right” model\nLet’s forget the actual values of body_mass_g, and pretend that this is the true relationship: \\[\nbodymass = 4207 + 18*billlength + 35*billdepth + 711.5*flipperlength + \\epsilon\n\\] where \\(\\epsilon_i \\sim N(0, 393)\\).\nWe can simulate from this as follows:\n\nX &lt;- cbind(1, as.matrix(peng[, 1:3]))\nn &lt;- nrow(X)\nbody_mass_g &lt;- X %*% beta + rnorm(n, 0, sigma)\n\nunname(beta)\n\n[1] -3157.53005    16.03916    91.51275    22.57975\n\nunname(coef(lm(body_mass_g ~ -1 + X)))\n\n[1] -3311.31867    16.27829   120.41292    20.77560\n\nunname(coef(lm(body_mass_g ~ X[, -1])))\n\n[1] -3311.31867    16.27829   120.41292    20.77560\n\n\nNow let’s do this 1000s of times!\n\nres &lt;- matrix(ncol = 4, nrow = 0)\nfor (i in 1:10000) {\n    body_mass_g &lt;- X %*% beta + rnorm(n, 0, sigma)\n    right_lm &lt;- lm(body_mass_g ~ -1 + X)\n    res &lt;- rbind(res, unname(coef(right_lm)))\n}\n\ndim(res)\n\n[1] 10000     4\n\n\n\npar(mfrow = c(2, 2))\nfor(i in 1:4) {\n    hist(beta[i] - res[, i], \n        main =paste0(\"Bias = \",round(beta[i], 2), \" - \", \n            round(mean(res[, i]), 2), \" = \", \n            round(beta[i] - mean(res[, i]), 2)))\n    abline(v = 0, col = 2, lwd = 2)\n}\n\n\n\n\nThis looks good- we simulated according to the values in beta, and we were able to recover them. We’ve also shown that the linear model is unbiased!"
  },
  {
    "objectID": "Lb13-Wrong_Model.html#estimating-the-wrong-model---too-few-predictors",
    "href": "Lb13-Wrong_Model.html#estimating-the-wrong-model---too-few-predictors",
    "title": "27  Lab 13: Missing/Extra Predictors",
    "section": "27.3 Estimating the Wrong Model - Too few predictors",
    "text": "27.3 Estimating the Wrong Model - Too few predictors\nIn the following code, I remove the “flipper_length_mm” (the third predictor) by only taking the first three columns of X, which includes the column of 1s.\nI then fit the model without flipper length, which we’ve seen before is an important predictor!\n\nres_reduced &lt;- matrix(ncol = 3, nrow = 0)\nX_reduced &lt;- X[, 1:3] # Still includes column of 1s\nbeta_reduced &lt;- beta[1:3]\nfor (i in 1:10000) {\n    # Simulate from the correct model\n    body_mass_g &lt;- X %*% beta + rnorm(n, 0, sigma)\n    # Only estimate beta 0-3 (not beta4)\n    wrong_lm &lt;- lm(body_mass_g ~ -1 + X_reduced)\n    res_reduced &lt;- rbind(res_reduced, unname(coef(wrong_lm)))\n}\n\ndim(res_reduced)\n\n[1] 10000     3\n\n\n\npar(mfrow = c(2, 2))\nfor(i in 1:3) {\n    bias &lt;- beta[i] - res_reduced[, i]\n    hist(bias, \n        main =paste0(\"Bias = \",round(beta[i], 2), \" - \", \n            round(mean(res_reduced[, i]), 2), \" = \", \n            round(mean(bias), 2)),\n        xlim = range(0, bias))\n    abline(v = 0, col = 2, lwd = 2)\n}\n\n\n\n\nEverything is biased! Since flipper_length_mm was an important predictor, the estimates from the other predictors are biased!\nHere’s how I like to think of this: the machine is trying to learn a pattern using the predictors we give it. These other predictors are trying to pick up on as much pattern as possible. Without the true pattern, they have to adjust.\nA big part of this comes from the fact that there’s correlation in the predictors. Since they’re correlated, if one is missing then the others can find the pattern through their correlation. Instead of flipper_length_mm causing a change in body mass, flipper_length_mm is correlated with bill_length_mm and bill_depth_mm, which then affect body_mass_g in place of flipper_length_m’s affect. In other words, they’re trying to make up for missing patterns through the correlation, like a game of telephone where information has been lost along the way."
  },
  {
    "objectID": "Lb13-Wrong_Model.html#too-many-predictors",
    "href": "Lb13-Wrong_Model.html#too-many-predictors",
    "title": "27  Lab 13: Missing/Extra Predictors",
    "section": "27.4 Too Many Predictors",
    "text": "27.4 Too Many Predictors\nWhat happens if we include predictors that aren’t correlated with the response?\nBefore we run this code, what do you expect?\nRecall our results when \\(Y = X\\underline\\beta + X_2\\underline\\beta_2 + \\underline\\epsilon\\):\n\\[\n\\begin{align*}\nE(\\hat{\\underline\\beta}) &= E((X^TX)^{-1}X^TY)\\\\\n& = (X^TX)^{-1}X^T(X\\underline\\beta + X_2\\underline\\beta_2) \\\\\n& = (X^TX)^{-1}X^TX\\underline\\beta + (X^TX)^{-1}X^TX_2\\underline\\beta_2 \\\\\n&= \\underline\\beta+ (X^TX)^{-1}X^TX_2\\underline\\beta_2\\\\\n&= \\underline\\beta + A\\underline\\beta_2\n\\end{align*}\n\\]\nThis isn’t directly applicable, but might help you think about what happens when \\(\\underline\\beta\\) is too big.\nSince we already have the objects created, let’s pretend that X_reduced is correct.\n\nres &lt;- matrix(ncol = 4, nrow = 0)\nX_reduced &lt;- X[, 1:3] # Still includes column of 1s\nbeta_reduced &lt;- beta[1:3]\nfor (i in 1:10000) {\n    # Simulate from the correct (smaller) model\n    body_mass_g &lt;- X_reduced %*% beta_reduced + rnorm(n, 0, sigma)\n    # Estimate the wrong model\n    wrong_lm &lt;- lm(body_mass_g ~ -1 + X)\n    res &lt;- rbind(res, unname(coef(wrong_lm)))\n\n}\n\npar(mfrow = c(2, 2))\nfor(i in 1:3) {\n    bias &lt;- beta_reduced[i] - res[, i]\n    hist(bias, \n        main =paste0(\"Bias = \",round(beta[i], 2), \" - \", \n            round(mean(res[, i]), 2), \" = \", \n            round(mean(bias), 2)),\n        xlim = range(0, bias))\n    abline(v = 0, col = 2, lwd = 2)\n}\n\n\n\n\nIt’s unbiased! In this case, the estimate of \\(\\beta\\) for flipper_length_mm is 0, and it’s successfully estimating this:\n\nhist(res[, 4])"
  },
  {
    "objectID": "Lb13-Wrong_Model.html#too-many-and-too-few",
    "href": "Lb13-Wrong_Model.html#too-many-and-too-few",
    "title": "27  Lab 13: Missing/Extra Predictors",
    "section": "27.5 Too many and too few",
    "text": "27.5 Too many and too few\nSo let’s get to the final case. As we know, bill length and bill depth are correlated:\n\ncor(X[, -1]) # correlation matrix, without column of 1s\n\n                  bill_length_mm bill_depth_mm flipper_length_mm\nbill_length_mm         1.0000000     0.6535362         0.4716073\nbill_depth_mm          0.6535362     1.0000000         0.5801429\nflipper_length_mm      0.4716073     0.5801429         1.0000000\n\n\nLet’s simulate with the coefficient for bill depth as 0, but include it in the model.\n\nprint(beta)\n\n      (Intercept)    bill_length_mm     bill_depth_mm flipper_length_mm \n      -3157.53005          16.03916          91.51275          22.57975 \n\nprint(head(X))\n\n       bill_length_mm bill_depth_mm flipper_length_mm\n[1,] 1           46.5          17.9               192\n[2,] 1           50.0          19.5               196\n[3,] 1           51.3          19.2               193\n[4,] 1           45.4          18.7               188\n[5,] 1           52.7          19.8               197\n[6,] 1           45.2          17.8               198\n\n\nTo be clear:\n\nData Generating Process: body mass = \\(\\beta_0\\) + \\(\\beta_1\\) bill length + \\(\\beta_3\\) flipper length\nEstimating: body mass = \\(\\beta_0\\) + \\(\\beta_2\\) bill depth + \\(\\beta_3\\) flipper length\n\n\nres &lt;- matrix(ncol = 3, nrow = 0)\nbeta_fewmany &lt;- beta\nbeta_fewmany[3] &lt;- 0 # True coefficient for depth is 0, length != 0\nX_fewmany &lt;- X[, c(1, 3, 4)] # estimating depth, not length\n\nfor (i in 1:10000) {\n    # Simulate from the correct (smaller) model\n    body_mass_g &lt;- X %*% beta_fewmany + rnorm(n, 0, sigma)\n    # Estimate the wrong model\n    wrong_lm &lt;- lm(body_mass_g ~ -1 + X_fewmany)\n    res &lt;- rbind(res, unname(coef(wrong_lm)))\n\n}\n\npar(mfrow = c(2, 2))\nhist(res[, 1],\n    main = paste0(\"bias=\", round(beta[1], 2),\n        \"-\", round(mean(res[, 1]), 2),\n        \"=\", round(beta[1] - mean(res[, 1]), 2)))\nabline(v = beta[1], col = 2, lwd = 2)\n\nhist(res[, 2])\nabline(v = 0, col = 2, lwd = 2)\n\nhist(res[, 3],\n    main = paste0(\"bias=\", round(beta[4], 2),\n        \"-\", round(mean(res[, 3]), 2),\n        \"=\", round(beta[4] - mean(res[, 3]), 2)))\nabline(v = beta[4], col = 2, lwd = 2)\n\n\n\n\n\nIt looks like flipper length is unbiased\n\nTechnically, it isn’t, but in this case it’s a small bias.\nIf we were primarily interested in flipper length, misspecifying bill length/depth isn’t so bad.\n\nThe estimate of bill_depth isn’t 0, but also doesn’t correspond to anything in the DGP!\n\nIt’s called a “proxy measure”, and the coefficient must be interpreted carefully."
  },
  {
    "objectID": "Lb13-Wrong_Model.html#summary",
    "href": "Lb13-Wrong_Model.html#summary",
    "title": "27  Lab 13: Missing/Extra Predictors",
    "section": "27.6 Summary",
    "text": "27.6 Summary\nChoosing the right subset of predictors can be HARD!\n\nMissing predictors means your estimates are biased\nToo many predictors isn’t as bad of an issue\n\nOverfitting!\n\nThe wrong subset means no relation to DGP\n\nCan still give (nearly) unbiased estimates for predictors of interest."
  },
  {
    "objectID": "Lb15-Transformations.html#finding-a-model-for-mpg-disp",
    "href": "Lb15-Transformations.html#finding-a-model-for-mpg-disp",
    "title": "28  Lab 15: Transformations",
    "section": "28.1 Finding a model for mpg ~ disp",
    "text": "28.1 Finding a model for mpg ~ disp\n\nPolynomial Models\nFirst, let’s consider the polynomial models from previous lecture:\n\nplot(mpg ~ disp, data = mtcars)\n\n\n\n\nA reasonable model for this looks to be \\(mpg_i = \\beta_0 + \\beta_1disp_i + \\beta_{11}disp_i^2\\):\n\nplot(mpg ~ disp, data = mtcars)\ndisp_seq &lt;- seq(min(mtcars$disp), max(mtcars$disp), 0.1)\npoly_seq &lt;- lm(mpg ~ poly(disp, 2), data = mtcars) |&gt;\n    predict(newdata = list(disp = disp_seq))\nlines(disp_seq, poly_seq)\n\n\n\n\nIt fits! However, the point of the polynomial lecture was that polynomials are tempting, but must be justified by theory. I’m not sure it’s reasonable to assume that the fuel efficiency is proportional to the square of the displacement.\nMaybe a transformation will help?\n\n\nTransformations\nLet’s try the two main transformations we talked about in class.\n\npar(mfrow = c(2,2))\ndisp_seq &lt;- seq(min(mtcars$disp), max(mtcars$disp), 0.1)\nlm_seq &lt;- lm(mpg ~ disp, data = mtcars) |&gt;\n    predict(newdata = list(disp = disp_seq))\n\nplot(mpg ~ disp, data = mtcars,\n    main = \"Polynomial Model\")\npoly_seq &lt;- lm(mpg ~ poly(disp, 2), data = mtcars) |&gt;\n    predict(newdata = list(disp = disp_seq))\nlines(disp_seq, lm_seq, col = \"#4a4a4a\", lty = 2)\nlines(disp_seq, poly_seq, col = 2, lwd = 2)\n\nplot(sqrt(mpg) ~ disp, data = mtcars,\n    main = \"Square Root Transfomation\")\nsqrt_seq &lt;- lm(sqrt(mpg) ~ disp, data = mtcars) |&gt;\n    predict(newdata = list(disp = disp_seq))\nlines(disp_seq, sqrt(lm_seq), col = \"#4a4a4a\", lty = 2)\nlines(disp_seq, sqrt_seq, col = 2, lwd = 2)\n\nplot(log(mpg) ~ disp, data = mtcars,\n    main = \"Log Transfomation\")\nlog_seq &lt;- lm(log(mpg) ~ disp, data = mtcars) |&gt;\n    predict(newdata = list(disp = disp_seq))\nlines(disp_seq, log(lm_seq), col = \"#4a4a4a\", lty = 2)\nlines(disp_seq, log_seq, col = 2, lwd = 2)\n\nplot(exp(mpg) ~ disp, data = mtcars,\n    main = \"Exponential Transfomation\")\nexp_seq &lt;- lm(exp(mpg) ~ disp, data = mtcars) |&gt;\n    predict(newdata = list(disp = disp_seq))\nlines(disp_seq, exp(lm_seq), col = \"#4a4a4a\", lty = 2)\nlines(disp_seq, exp_seq, col = 2, lwd = 2)\n\n\n\n\nI also added the linear model, transformed to the scale of the data. Notice how the linear model is curved on these non-linear scales!\nIn the end, the log-transform is actually pretty good! Let’s evaluate that one!\nI’m going to add log_mpg as a column in mtcars because we’re only going to be working on that scale. With transformations, all of our assumptions about the residuals are on the transformed scale!!! This is important!!!\n\nmtcars$log_mpg &lt;- log(mtcars$mpg)\n\nlog_lm &lt;- lm(log_mpg ~ disp, data = mtcars)\nraw_lm &lt;- lm(mpg ~ disp, data = mtcars)\n\npar(mfrow = c(3,2), mar = c(3,3,2,2))\nplot(raw_lm, which = 1)\nplot(log_lm, which = 1)\nplot(raw_lm, which = 2)\nplot(log_lm, which = 2)\nplot(raw_lm, which = 3)\nplot(log_lm, which = 3)\n\n\n\n\n\nResids versus fitted looks better for log_lm!\nNormal Q-Q looks about the same\nScale-location looks better for log_lm!\n\nIt’s worth noting that we always use “fitted” rather than, say, disp. When using a polynomial model, the fitted values go from the lowest to highest. For a positive coefficient for disp^2, this means that it starts from the lowest point in the parabola and goes upward in both directions! Keep that in mind when interpreting residual plots of polynomial functions!"
  },
  {
    "objectID": "Lb15-Transformations.html#the-box-cox-transformation",
    "href": "Lb15-Transformations.html#the-box-cox-transformation",
    "title": "28  Lab 15: Transformations",
    "section": "28.2 The Box-Cox Transformation",
    "text": "28.2 The Box-Cox Transformation\n\nlibrary(MASS)\nboxcox(lm(mtcars$mpg ~ mtcars$disp), data = mtcars)\n\n\n\n\nSince 0 is the the top 5% of log-Likelihood values, the log transform is reasonable according to Box-Cox!\n\nbc &lt;- boxcox(lm(mtcars$mpg ~ mtcars$disp), data = mtcars, plotit = FALSE)\nprint(bc)\n\n$x\n [1] -2.0 -1.9 -1.8 -1.7 -1.6 -1.5 -1.4 -1.3 -1.2 -1.1 -1.0 -0.9 -0.8 -0.7 -0.6\n[16] -0.5 -0.4 -0.3 -0.2 -0.1  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9\n[31]  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2.0\n\n$y\n [1] -3.57059866 -2.41311198 -1.29583952 -0.22262913  0.80247262  1.77526399\n [7]  2.69144571  3.54669319  4.33674248  5.05748822  5.70509010  6.27608329\n[13]  6.76748690  7.17690379  7.50260505  7.74359272  7.89963620  7.97127900\n[19]  7.95981574  7.86724092  7.69617369  7.44976444  7.13158953  6.74554135\n[25]  6.29571958  5.78632931  5.22158959  4.60565519  3.94255266  3.23613075\n[31]  2.49002447  1.70763143  0.89209868  0.04631829 -0.82707014 -1.72567168\n[37] -2.64732452 -3.59008589 -4.55221639 -5.53216368 -6.52854602\n\nprint(paste0(\"Optimal value of lamba is: \", bc$x[which.max(bc$y)]))\n\n[1] \"Optimal value of lamba is: -0.3\"\n\n\n\nmtcars$opt_mpg &lt;- (mtcars$mpg^(-0.3) - 1)/0.3\n\noptimal_lm &lt;- lm(opt_mpg ~ disp, data = mtcars)\nraw_lm &lt;- lm(mpg ~ disp, data = mtcars)\n\npar(mfrow = c(3,2), mar = c(3,3,2,2))\nplot(log_lm, which = 1)\nplot(optimal_lm, which = 1)\nplot(log_lm, which = 2)\nplot(optimal_lm, which = 2)\nplot(log_lm, which = 3)\nplot(optimal_lm, which = 3)\n\n\n\n\nIt does look a little bit better, but the log is simpler to interpret and should probably be used."
  },
  {
    "objectID": "Lb16-Regression_with_Dummies.html#regression-as-a-t-test",
    "href": "Lb16-Regression_with_Dummies.html#regression-as-a-t-test",
    "title": "29  Lab 16 - Regression with Categorical Predictors",
    "section": "29.1 Regression as a t-test",
    "text": "29.1 Regression as a t-test\nWhen we do regression against a dummy variable, we are actually just doing a t-test.\n\nA regression models the mean of \\(y\\) for each value of \\(x\\).\nA one-unit increase in \\(X\\) is the difference between 0 and 1.\n\nThe slope is the difference in means.\n\nWe assume constant variance\n\nSame variance at \\(x=0\\) and \\(x=1\\).\n\nA t-test has a df of \\(n-1\\), so does a regression with just one predictor.\n\n\n# Note that I need the var.equal=TRUE to match the assumptions of regression.\nt.test(mpg ~ am, data = mtcars, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  mpg by am\nt = -4.1061, df = 30, p-value = 0.000285\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -10.84837  -3.64151\nsample estimates:\nmean in group 0 mean in group 1 \n       17.14737        24.39231 \n\n\n\nlm(mpg ~ am, data = mtcars) |&gt; summary()\n\n\nCall:\nlm(formula = mpg ~ am, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3923 -3.0923 -0.2974  3.2439  9.5077 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   17.147      1.125  15.247 1.13e-15 ***\nam             7.245      1.764   4.106 0.000285 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.902 on 30 degrees of freedom\nMultiple R-squared:  0.3598,    Adjusted R-squared:  0.3385 \nF-statistic: 16.86 on 1 and 30 DF,  p-value: 0.000285\n\n\nThe two outputs have the exact same test statistic and p-value!"
  },
  {
    "objectID": "Lb16-Regression_with_Dummies.html#categorical-predictors-as-anova",
    "href": "Lb16-Regression_with_Dummies.html#categorical-predictors-as-anova",
    "title": "29  Lab 16 - Regression with Categorical Predictors",
    "section": "29.2 Categorical Predictors as ANOVA",
    "text": "29.2 Categorical Predictors as ANOVA\nIn order for R to recognize a variable as a dummy variable, we need to tell it that the values should be interpreted as a factor. This is a very particular data type:\n\nEvery observation is one level of the categorical variable.\nEvery observation can only be one level of that categorical variable\n\nE.g., a car can’t have 4 cylinders and 6 cylinders\n\nThere are a finite number of possible categories.\n\nUnless we specify, R assumes that the unique values that it sees constitutes all the possible categories. In other words, it won’t let us ask about cars with 5 cylinders because we’re making a factor variable with 4, 6, and 8 as the observed values.\n\n\n\nanova(aov(mpg ~ factor(cyl), data = mtcars))\n\nAnalysis of Variance Table\n\nResponse: mpg\n            Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nfactor(cyl)  2 824.78  412.39  39.697 4.979e-09 ***\nResiduals   29 301.26   10.39                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsummary(lm(mpg ~ factor(cyl), data = mtcars))\n\n\nCall:\nlm(formula = mpg ~ factor(cyl), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2636 -1.8357  0.0286  1.3893  7.2364 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   26.6636     0.9718  27.437  &lt; 2e-16 ***\nfactor(cyl)6  -6.9208     1.5583  -4.441 0.000119 ***\nfactor(cyl)8 -11.5636     1.2986  -8.905 8.57e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.223 on 29 degrees of freedom\nMultiple R-squared:  0.7325,    Adjusted R-squared:  0.714 \nF-statistic:  39.7 on 2 and 29 DF,  p-value: 4.979e-09\n\n\nThe overall test of significance has an F-value of 39.7 and a p-value of 4.979x10\\(^{-9}\\)\nBut what is up with the summary.lm() output? Why do we have a row labelled factor(cyl)6??"
  },
  {
    "objectID": "Lb16-Regression_with_Dummies.html#coding-dummy-variables-what-factor-is-actually-doing",
    "href": "Lb16-Regression_with_Dummies.html#coding-dummy-variables-what-factor-is-actually-doing",
    "title": "29  Lab 16 - Regression with Categorical Predictors",
    "section": "29.3 Coding Dummy Variables (what factor is actually doing)",
    "text": "29.3 Coding Dummy Variables (what factor is actually doing)\nWhen we have multiple variables, we set one aside as the “reference” category. For a factor variable with \\(k\\) categories, we set up \\(k-1\\) new variables to denote category membership. For example:\n\n4 is the reference category for cyl. If all the new dummy variables that we create are all 0, then the car must be 4 cylinders.\nWe set up a dummy variable for 6 cylinders, which is a column with a 1 if the car has 6 cylinders and a 0 otherwise. This is what’s labelled as factor(cyl)6 in the output.\n\nThis can be denoted \\(I(cyl == 6)\\).\n\nSimilarly, we have factor(cyl)8 = \\(I(cyl == 8)\\).\n\nConsider the model \\(y = \\beta_0 + \\beta_1I(cyl == 6) + \\beta_2I(cyl == 8)\\).\nWith this setup, the intercept represents the estimate for 4 cylinder cars, \\(\\beta_1\\) represents the difference in 4 and 6 cylinder cars, while \\(\\beta_2\\) represents the difference in 4 and 8 cylinder cars (you can find the difference between 6 and 8 by doing the right math).\nFrom model.matrix(), we can see this in action:\n\ncbind(model.matrix(mpg ~ factor(cyl), data = mtcars), cyl = cbind(mtcars$cyl)) |&gt; head(12)\n\n                  (Intercept) factor(cyl)6 factor(cyl)8  \nMazda RX4                   1            1            0 6\nMazda RX4 Wag               1            1            0 6\nDatsun 710                  1            0            0 4\nHornet 4 Drive              1            1            0 6\nHornet Sportabout           1            0            1 8\nValiant                     1            1            0 6\nDuster 360                  1            0            1 8\nMerc 240D                   1            0            0 4\nMerc 230                    1            0            0 4\nMerc 280                    1            1            0 6\nMerc 280C                   1            1            0 6\nMerc 450SE                  1            0            1 8\n\n\nThe model can be written as:\n\\[\ny_i = \\begin{cases}\\beta_0 & \\text{if }\\; cyl == 4\\\\ \\beta_0 + \\beta_1 & \\text{if }\\; cyl == 6\\\\\\beta_0  + \\beta_2 & \\text{if }\\; cyl == 8\\end{cases}\n\\]\nAnd indeed we can show that this is equivalent to fitting three separate models:\n\ncyl4 &lt;- coef(lm(mpg ~ 1, data = subset(mtcars, cyl == 4)))\ncyl6 &lt;- coef(lm(mpg ~ 1, data = subset(mtcars, cyl == 6)))\ncyl8 &lt;- coef(lm(mpg ~ 1, data = subset(mtcars, cyl == 8)))\nallcyl &lt;- coef(lm(mpg ~ factor(cyl), data = mtcars))\nprint(c(cyl4 = unname(cyl4), beta0=unname(allcyl[1])))\n\n    cyl4    beta0 \n26.66364 26.66364 \n\nprint(c(cyl6 = unname(cyl6), beta0_plus_beta1 = unname(allcyl[1] + allcyl[2])))\n\n            cyl6 beta0_plus_beta1 \n        19.74286         19.74286 \n\nprint(c(cyl8 = unname(cyl8), beta0_plus_beta2 = unname(allcyl[1] + allcyl[3])))\n\n            cyl8 beta0_plus_beta2 \n            15.1             15.1 \n\n\nThe advantage of having all three in one is that we can test for significance easily!\nOne way (a bad way) to visualize this is to treat \\(I(cyl ==6)\\) and \\(I(cyl==8)\\) as separate variables:\n\npar(mfrow = c(1,2))\nplot(mpg ~ I(as.numeric(factor(cyl)) - 1), data = subset(mtcars, cyl %in% c(4,6)),\n    xlab = \"Cyl (0 = 4, 1 = 6)\", main = \"I(cyl == 6)\")\nabline(lm(mpg ~ factor(cyl), data = subset(mtcars, cyl %in% c(4,6))))\nplot(mpg ~ I(as.numeric(factor(cyl)) - 1), data = subset(mtcars, cyl %in% c(4,8)),\n    xlab = \"Cyl (0 = 4, 1 = 8)\", main = \"I(cyl == 8)\")\nabline(lm(mpg ~ factor(cyl), data = subset(mtcars, cyl %in% c(4,8))))\n\n\n\n\nFrom this, we can see that the slope of the line is indeed looking at the difference in means.\n\nCategorical and Continuous Variables\nIf we have cyl and disp in the model, we get the following:\n\\[\ny = \\beta_0 + \\beta_1I(cyl == 6) + \\beta_2I(cyl == 8) + \\beta_3 disp\n\\] which is equivalent to: \\[\ny_i = \\begin{cases}\\beta_0  + \\beta_3 disp& \\text{if }\\; cyl == 4\\\\ \\beta_0 + \\beta_1  + \\beta_3 disp& \\text{if }\\; cyl == 6\\\\\\beta_0  + \\beta_2  + \\beta_3 disp& \\text{if }\\; cyl == 8\\end{cases}\n\\] This is three different models of mpg versus disp, but with a different intercept depending on the value of cyl.\n\ncyldisp &lt;- coef(lm(mpg ~ factor(cyl) + disp, data = mtcars))\ncyldisp\n\n (Intercept) factor(cyl)6 factor(cyl)8         disp \n 29.53476781  -4.78584624  -4.79208587  -0.02730864 \n\nplot(mpg ~ disp, col = factor(cyl), data = mtcars)\nabline(a = cyldisp[1], b = cyldisp[4], col = 1, lty = 1)\nabline(a = cyldisp[1] + cyldisp[2], b = cyldisp[4], col = 2, lty = 1)\nabline(a = cyldisp[1] + cyldisp[3], b = cyldisp[4], col = 3, lty = 2)\n\n\n\n\nThe plot looks like it only has 2 lines, but that’s because \\(\\beta_1=\\beta_2\\), so one line is plotted on top of the other! I changed the linetypes so you can see this.\nIt looks like the red and green lines are fitting the red and green data, but the black line doesn’t look quite right.\n\n\nThree Different Models\nIf we have an interaction between cyl and disp, then we essentially get 3 models. \\[\ny = \\beta_0 + \\beta_1I(6) + \\beta_2I(8) + \\beta_3 disp + \\beta_4I(6)disp + \\beta_5I(8)disp\n\\] where \\(I(6)\\) is just shorthand for \\(I(cyl == 6)\\).\nThis is the same as: \\[\ny_i = \\begin{cases}\\beta_0  + \\beta_3 disp& \\text{if }\\; cyl == 4\\\\ (\\beta_0 + \\beta_1)  + (\\beta_3 + \\beta_4) disp& \\text{if }\\; cyl == 6\\\\(\\beta_0  + \\beta_2)  + (\\beta_3 + \\beta_5) disp& \\text{if }\\; cyl == 8\\end{cases}\n\\]\nIn R, we cans use the fanciness of the formula notation. R interprets * as interaction as well as lower order terms.\n\ncoef(lm(mpg ~ disp * factor(cyl), data = mtcars))\n\n      (Intercept)              disp      factor(cyl)6      factor(cyl)8 \n       40.8719553        -0.1351418       -21.7899679       -18.8391564 \ndisp:factor(cyl)6 disp:factor(cyl)8 \n        0.1387469         0.1155077 \n\ncoef(lm(mpg ~ disp, data = subset(mtcars, cyl == 4))) # Others will be similar\n\n(Intercept)        disp \n 40.8719553  -0.1351418 \n\n\nggplot2 makes it super easy to plot this.\n\nlibrary(ggplot2); theme_set(theme_bw())\nggplot(mtcars) +\n    aes(x = disp, y = mpg, colour = factor(cyl)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, formula = \"y ~ x\")\n\n\n\n\nRecall that the model without interaction terms (different intercepts, same slopes) had practically the same intercept for 6 and 8. The slopes look different here, but there isn’t a lot of data in the 6 category and I suspect that we could force it to have the same intercept and slope as the 8 category."
  },
  {
    "objectID": "Lb16-Regression_with_Dummies.html#comparison-to-other-models",
    "href": "Lb16-Regression_with_Dummies.html#comparison-to-other-models",
    "title": "29  Lab 16 - Regression with Categorical Predictors",
    "section": "29.4 Comparison to Other Models",
    "text": "29.4 Comparison to Other Models\nIn previous lectures, we’ve looked at this same relationship using polynomial model for displacement and a transformation for mpg. Let’s see how these stack up!\nHow do we compare such models? There isn’t a statistical test for which one fits best, but, as always, we want to know about the residuals!\n\npoly_lm &lt;- lm(mpg ~ poly(disp, 2), data = mtcars)\nlog_lm &lt;- lm(log(mpg) ~ disp, data = mtcars)\ninteract_lm &lt;- lm(mpg ~ factor(cyl)*disp, data = mtcars)\n\n\npar(mfrow = c(1,3))\n\nresid_plot &lt;- 1 # Re-run with different values to check other plots\n# Use similar colours to the ggplot.\nmycolours &lt;- c(\"red\", \"green\", \"blue\")[as.numeric(factor(mtcars$cyl))]\n\nplot(poly_lm, which = resid_plot, main = \"Polynomial\", col = mycolours)\nplot(log_lm, which = resid_plot, main = \"Log Transform\", col = mycolours)\nplot(interact_lm, which = resid_plot, main = \"Interaction\", col = mycolours)\n\n\n\n\n\nResiduals versus fitted looks quite a bit better for the interaction model.\nNormal Q-Q also looks better for interaction model.\nScale-Location is slightly better for interaction model, although still not perfect.\nResiduals versus leverage indicates the Hornet 4 Drive car has somewhat high leverage. I’m guessing this is the largest green dot in the plot up above - the green line would be very different without it.\n\nThis highlights the importance of carefully interpreting Cook’s Distance. In this case, the interaction model is a combination of three different models.\n\n\nThe plots are quite similar, but for the most part the interaction model seems to work best.\nANOVA can be used to compare the residual variance across non-nested models, but is not appropriate for one of the three models we just saw. See if you can guess which !\n\nanova(poly_lm, log_lm, interact_lm)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ poly(disp, 2)\nModel 2: mpg ~ factor(cyl) * disp\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n1     29 233.39                                \n2     26 146.23  3    87.159 5.1655 0.006199 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe models have a significantly different fit, so the one with the lowest residual variance probably fits better.\n\nc(summary(poly_lm)$sigma, summary(interact_lm)$sigma)\n\n[1] 2.836907 2.371581\n\n\nThere are two important points here:\n\nThe interaction model fits the context of the problem. It absolutely makes sense that an engine with 4 cylinders is different from an engine with 6 cylinders, and part of that difference is the relationship between mpg and displacement. There are no two engines which are equivalent except someone added two cylinders to it; the cylinder values represent fundamentally different groups.\n\nIn other words, the theory supports an interaction model. There’s no theory that I know of that states there should be a quadratic relationship between mpg and displacement. If we want to say something about cars in general (inference), it’s best to go with the context of the problem.\n\nThe interaction model should fit better since it contains more information - it has the displacement and the number of cylinders, the other models only had displacement.\n\nAlso note that the interaction model takes up more degrees of freedom. This can be a negative, especially with small samples.\n\n\n\nThis is an ANCOVA Model\n\nanova(interact_lm)\n\nAnalysis of Variance Table\n\nResponse: mpg\n                 Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfactor(cyl)       2 824.78  412.39 73.3221 2.05e-11 ***\ndisp              1  57.64   57.64 10.2487 0.003591 ** \nfactor(cyl):disp  2  97.39   48.69  8.6574 0.001313 ** \nResiduals        26 146.23    5.62                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value for the ANCOVA test is 0.001313, indicating that there is a significantly different covariance between mpg and displacement depending on the number of cylinders.\n\nThe lower-order terms factor(cyl) and disp must be present in order for this test to make sense.\n\nIt is technically possible to fit a model with just the interaction term, but it’s slightly better to have extra predictors with a 0 coefficient than be missing predictors with a non-0 coefficient.\n\nFor the curious, the syntax for this model would be lm(mpg ~ factor(cyl):disp, data = mtcars), where the : indicates multiplication. This isn’t used often, since it’s almost always incorrect to include interaction terms without the individual effects.\nIt’s not clearly better in all cases! There may be some contextual reason why it makes sense to only have an interaction.\n\n\nRecall that the anova() function reports the sequential sum-of-squares. In this situation, we do not care about the p-values for factor(cyl) and disp, so we do not care which order they enter the model in. We only care about the p-value for inclusion of the interaction term factor(cyl):disp.\n\nR’s formula notation is clunky, but leads to a lot of great situations like this. By using factor(cyl)*disp), it added the lower order terms first and then the interaction, thus making the sequential sum-of-squares useful!"
  },
  {
    "objectID": "Lb17-Multico.html#the-problem-with-multicollinearity",
    "href": "Lb17-Multico.html#the-problem-with-multicollinearity",
    "title": "30  Multicollinearity",
    "section": "30.1 The problem with multicollinearity",
    "text": "30.1 The problem with multicollinearity\nConsider the model \\[\ny_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\epsilon_i\n\\] with the added assumption that \\(x_{i1} = a + bx_{i2} + z_i\\), where \\(z_i\\) is some variation.\n(As a technical note, none of this is assumed to be random; just uncertain. We’ll use the language of probability in this section, but it’s just to quantify uncertainty rather than make modelling assumptions.)\n\nlibrary(plot3D)\nn &lt;- 100\nx1 &lt;- runif(n, 0, 10)\nx2 &lt;- 2 + x1 + runif(n, -1, 1)\ny &lt;- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n\nscatter3D(x1, x2, y, phi = 30, theta = 15, bty = \"g\",\n    xlab = \"x1\", ylab = \"x2\", zlab = \"y\")\n\n\n\n\nThe 3D plot looks like a tube! By “tube”, we can think of the 3D plot as being a 2D plot that’s in the wrong coordinate system. In other words, the data define a single axis, which is different from the x1, x2, and y axes. When fitting multiple linear regression, we’re fitting a hyperplane. If the relationship is multicollinear, then we might imagine rotating a hyperplane around the axis defined by the “tube” in the plot above. Since any rotation of the plane around fits the line of data pretty well, the coefficients that define that plane are not well-estimated.\nIt’s hard to imagine 3D sets of points, so let’s do a 2D analogy. In the 3D example, we actually had a 2D relationship, so let’s consider a 2D example that’s almost 1D.\nIn the plot below, I’ve made \\(x\\) almost one-dimensional. That is, I’ve given it a vary small range. I intentionally changed the x limits of the plot to emphasize this.\n\nx &lt;- c(rep(-0.025, 10), rep(0.025, 10))\ny &lt;- x + rnorm(20, 0, 3)\nplot(y ~ x, xlim = c(-2, 2))\n\n\n\n\nIf we were to fit a regression line to this, there are many many slopes that would make this work!\n\nplot(y ~ x, xlim = c(-1, 1))\nfor(i in 1:20) abline(a = mean(y), b = runif(1, -20, 20))\n\n\n\n\nAll of the lines I added will fit the data pretty well, even though they all have completely different slopes! Yes, there’s one with a “best” slope, but slightly different data would have given us a very different slope:\n\n# Set up empty plot\nplot(NA, xlab = \"x\", ylab = \"y\",\n    xlim = c(-2, 2), ylim = c(-6,6))\n# Generate data from same dgp as before\nfor(i in 1:100) {\n    y &lt;- x + rnorm(20, 0, 3)\n    abline(lm(y ~ x))\n}\n\n\n\n\nAll of these lines would have worked for our data!\nThis can easily be seen the variance of the parameters:\n\nsummary(lm(y ~ x))$coef\n\n              Estimate Std. Error    t value  Pr(&gt;|t|)\n(Intercept) -0.6205096  0.6779798 -0.9152331 0.3721680\nx            6.0310840 27.1191916  0.2223917 0.8265128"
  },
  {
    "objectID": "Lb17-Multico.html#exact-multicollinearity",
    "href": "Lb17-Multico.html#exact-multicollinearity",
    "title": "30  Multicollinearity",
    "section": "30.2 Exact Multicollinearity",
    "text": "30.2 Exact Multicollinearity\nIn the example above, the line was very sensitive to a slight change in the data because we essentially had one dimension. If we actually had one dimension, then any line that cgoes through \\(\\bar y\\) has the exact same error, regardless of the slope. We can see this in the design matrix: \\(X\\) has a column of 1s for the intercept (which is good), but also has a column for \\(x_1\\) that has zero variance. This means that it’s a linear combination of the first column, and thus is rank-deficient.\n\nX &lt;- cbind(rep(1, 20), rep(0, 20))\ntry(solve(t(X) %*% X))\n\nError in solve.default(t(X) %*% X) : \n  Lapack routine dgesv: system is exactly singular: U[2,2] = 0\n\n\nSince one column is a linear combination of the other, \\(X^TX\\) cannot be inverted. In this case, the variance fo \\(\\hat\\beta_1\\) is infinite!\nThis can also happen if one column is “close” to being a linear combination of the others, such as:\n\n# Change the first element in the second column to 10^-23\nX[1, 2] &lt;- 1e-23\ntry(solve(t(X) %*% X))\n\nError in solve.default(t(X) %*% X) : \n  system is computationally singular: reciprocal condition number = 4.75e-48\n\n\nThe rows are mathematically different, but the difference is so small that computers cannot tell.\n\nx &lt;- runif(20, 0, 1) / 1e7\nX &lt;- cbind(1, x)\ntry(solve(t(X) %*% X))\n\n                            x\n   1.697904e-01 -2.406244e+06\nx -2.406244e+06  4.833450e+13\n\n\nIn the next code chunk, try playing around with the power of 10 (i.e., try 10^50, 10^-50, etc., for both positive and negative powers). At some point, the matrix is not invertible and the coefficient table stops reporting the slope! At the other end, the line is a near perfect fit (why?).\n\nx &lt;- runif(20, 0, 1) / (10^350)\ny &lt;- x + rnorm(20, 0, 3)\nsummary(lm(y ~ x))$coef\n\n             Estimate Std. Error   t value  Pr(&gt;|t|)\n(Intercept) 0.1092976  0.6173655 0.1770387 0.8613516"
  },
  {
    "objectID": "Lb17-Multico.html#back-to-regression",
    "href": "Lb17-Multico.html#back-to-regression",
    "title": "30  Multicollinearity",
    "section": "30.3 Back to Regression",
    "text": "30.3 Back to Regression\nWe can kinda detect multicollinearity mainly using the standard error of the estimates:\n\nlibrary(palmerpenguins)\npeng_lm &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n    data = penguins)\nsummary(peng_lm)\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm + bill_length_mm + \n    bill_depth_mm, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1054.94  -290.33   -21.91   239.04  1276.64 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -6424.765    561.469 -11.443   &lt;2e-16 ***\nflipper_length_mm    50.269      2.477  20.293   &lt;2e-16 ***\nbill_length_mm        4.162      5.329   0.781    0.435    \nbill_depth_mm        20.050     13.694   1.464    0.144    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 393.4 on 338 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7615,    Adjusted R-squared:  0.7594 \nF-statistic: 359.7 on 3 and 338 DF,  p-value: &lt; 2.2e-16\n\n\nIt looks like bill_depth_mm has a large standard error! Of course, this might be because:\n\nThe variance of bill_depth_mm is high to begin with.\nThe other predictors have explained most of the variance and any estimate for bill_depth_mm will do.\nMulticollinearity\n\nMultico. is just one of the possible reasons why the SE might be high, we need to look into it more to be sure.\n\nlibrary(car)\nvif(peng_lm)\n\nflipper_length_mm    bill_length_mm     bill_depth_mm \n         2.673338          1.865090          1.611292 \n\n\nIt actually has quite a small VIF!"
  },
  {
    "objectID": "Lb17-Multico.html#centering-and-scaling",
    "href": "Lb17-Multico.html#centering-and-scaling",
    "title": "30  Multicollinearity",
    "section": "30.4 Centering and Scaling",
    "text": "30.4 Centering and Scaling\n\nX &lt;- model.matrix(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n    data = penguins)\n\nZ &lt;- cbind(1, apply(X[, -1], 2, scale))\nprint(\"correlation of X\")\n\n[1] \"correlation of X\"\n\nround(cor(X), 4)\n\n                  (Intercept) flipper_length_mm bill_length_mm bill_depth_mm\n(Intercept)                 1                NA             NA            NA\nflipper_length_mm          NA            1.0000         0.6562       -0.5839\nbill_length_mm             NA            0.6562         1.0000       -0.2351\nbill_depth_mm              NA           -0.5839        -0.2351        1.0000\n\nprint(\"Also correlation of X\")\n\n[1] \"Also correlation of X\"\n\nround(t(Z) %*% Z/ (nrow(Z) - 1), 4)\n\n                         flipper_length_mm bill_length_mm bill_depth_mm\n                  1.0029            0.0000         0.0000        0.0000\nflipper_length_mm 0.0000            1.0000         0.6562       -0.5839\nbill_length_mm    0.0000            0.6562         1.0000       -0.2351\nbill_depth_mm     0.0000           -0.5839        -0.2351        1.0000\n\n\nBy simulation:\n\nn &lt;- 100\nx1 &lt;- runif(n, 0, 10)\nx2 &lt;- 2 + x1 + runif(n, -3, 3)\n\nreps_1 &lt;- replicate(1000, {\n    y &lt;- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n    coef(lm(y ~ x1 + x2))\n}) |&gt; t()\ncor(reps_1)\n\n            (Intercept)         x1         x2\n(Intercept)   1.0000000  0.1764180 -0.6112093\nx1            0.1764180  1.0000000 -0.8609998\nx2           -0.6112093 -0.8609998  1.0000000\n\n\n\nx1 &lt;- scale(x1)\nx2 &lt;- scale(x2)\n\nreps_2 &lt;- replicate(1000, {\n    y &lt;- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n    coef(lm(y ~ x1 + x2))\n}) |&gt; t()\ncor(reps_2)\n\n            (Intercept)          x1          x2\n(Intercept)  1.00000000 -0.01364783  0.03646404\nx1          -0.01364783  1.00000000 -0.85526575\nx2           0.03646404 -0.85526575  1.00000000\n\n\nThe correlation is… slightly lower? It’s much lower for the intercept, but it doesn’t make much of a difference for the correlation between the slopes. (It will generally be lower, and should be with a large enough number of simulations.)\nJust for fun, here’s the VIF for these data. I’ve added a parameter x1_around_x2 to allow you to play around with the correlation of x1 and x2.\n\nx2_around_x1 &lt;- 3\nx1 &lt;- runif(n, 0, 10)\nx2 &lt;- 2 + x1 + runif(n, -x2_around_x1, x2_around_x1)\ny &lt;- 0 + 4*x1 + 3*x2 + rnorm(n, 0, 5)\n\nvif(lm(y ~ x1 + x2))\n\n      x1       x2 \n4.494537 4.494537"
  },
  {
    "objectID": "Lb18-Backwards_Selection_is_Bad.html",
    "href": "Lb18-Backwards_Selection_is_Bad.html",
    "title": "31  Backwards p-values",
    "section": "",
    "text": "32 Algorithmic model selection\n\nDescription\nBackwards and forward model selection are sequential model selection techniques where predictors are sequentially removed or added until all predictors are significant and no other predictors would be significant if they were added.\n\n\nDistribution of a p-value\nUnder the null hypothesis, a p-value is uniform. That is, all p-values are equally likely. This is not a coincidence, this is how p-values are defined. The p-value is the area of the sampling distribution further away from the hypothesized mean than the observed data. The area “further away” means we are looking at the CDF of a distribution, and the CDF of any continuous distribution follows a normal distribution.\nTo demonstrate, let’s simulate!\n\nlibrary(magrittr) # pipes and `extract` function\nlibrary(dplyr)\nlibrary(ggplot2)\nx &lt;- runif(30, 0, 10)\ny &lt;- rnorm(30, 0, 1) # uncorrelated with x - null is true.\n\nsummary(lm(y ~ x))$coef[2, 4] # the p-value for x\n\n[1] 0.93823\n\nN &lt;- 10000 # Increase for better results\np_vals &lt;- double(N)\nfor (i in 1:N){\n    y &lt;- rnorm(30, 0, 1)\n    p_vals[i] &lt;- summary(lm(y ~ x))$coef[2, 4]\n}\n\n\nhist(p_vals, freq = FALSE)\nabline(h = 1) # theoretical pdf\n\n\n\n\n\n\n\n33 Backwards Selection\nWe’ve established that P(p.value &lt; 0.05) = 0.05, i.e. that p-values follow a uniform distribution. What about the probability of at least one significant value out of 2?\n\\[\\begin{align*}\nF(z) &= P(\\text{at least one of }X_1\\text{ and }X_2\\text{ is less than }z)\\\\\n&= 1 - P(\\text{both }X_1\\text{ and }X_2\\text{ are greater than }z)\\\\\n&= 1 - P(X_1 &gt; z, X_2 &gt; z)\\\\\n&= 1 - P(X_1 &gt; z)(X_2 &gt; z)\\\\\n&= 1 - (1-z)^2\n\\end{align*}\\]\nSo the pdf is the derivative of this, which is \\(f(z) = 2 - 2z\\).\n\nN &lt;- 10000\nmin_p_vals &lt;- double(N)\nfor (i in 1:N){\n    yxx &lt;- data.frame(x1 = runif(30), x2 = runif(30), y = rnorm(30))\n    p_vals &lt;- summary(lm(y ~ ., data = yxx))$coef[-1, 4]\n    max_p &lt;- which.max(p_vals) # to be removed\n    min_p_vals[i] &lt;- summary(lm(y ~ ., data = yxx[, -max_p]))$coef[2, 4]\n}\n\n\nhist(min_p_vals, freq = FALSE)\nabline(a = 2, b = -2) # Theoretical pdf\n\n\n\n\nSo when we remove the largest p-value, we end up with a much larger chance that the remaining p-value is below 0.05 - even though the null hypothesis is true. This bears repeating - when we make a change in the model based on the p-value, the rest of the p-values must be interpreted carefully. They are no longer the probability of data at least as extreme under the null hypothesis - they have been artificially shrunk.\n\nWhat about AIC?\nIn R, step() performs stepwise model selection (by default, backwards) until the AIC no longer decreases. I’m going to simulate under the null hypothesis: none of the predictors are actually related to the response.\n\nN &lt;- 1000 # This will take a while\nmin_p &lt;- double(N)\nmax_p &lt;- double(N)\np1 &lt;- double(N)\nnum_signif &lt;- double(N)\n\nt0 &lt;- Sys.time()\nfor (i in 1:N){\n    # First columns is y, the rest are x1...x20\n    bigdf &lt;- matrix(data = rnorm(30 * 11), nrow = 30, ncol = 11)\n    colnames(bigdf) &lt;- c(\"y\", paste(\"x\", 1:10))\n    bigdf &lt;- as.data.frame(bigdf)\n\n    # Regress y against everything\n    newmod &lt;- step(lm(y ~ ., data = bigdf), trace = 0)\n\n    # Extract the p-values\n    mycoefs &lt;- summary(newmod)$coef[-1, 4]\n    num_signif[i] &lt;- length(mycoefs)\n    min_p[i] &lt;- min(mycoefs)\n    max_p[i] &lt;- max(mycoefs)\n    p1[i] &lt;- mycoefs[1] # should be representative of p-values\n}\nSys.time() - t0\n\nTime difference of 19.89682 secs\n\n\n\npar(mfrow = c(2, 2))\nhist(min_p, breaks = seq(0, 0.25, 0.01))\nabline(v = 0.05, col = \"red\")\nhist(max_p, breaks = seq(0, 0.55, 0.01))\nabline(v = 0.05, col = \"red\")\nbarplot(table(num_signif), main = \"Number of remaining predictors\")\nhist(p1, breaks = seq(0, 0.45, 0.01))\nabline(v = 0.05, col = \"red\")\n\n\n\n\nIn the plots above, keep in mind that none of the predictors were related to the response. We see that the p-values are far from being uniform, and sometimes we took all of the predictors.\nThe bottom right plot shows the distribution of the p-value for the first predictor. There’s nothing special about this predictor, so this plot is representative of the distribution of p-values resulting from stepwise AIC model selection with 20 predictors, none of which are actually related to the response.\n\n\n\n34 So what’s the right way?\nBest: Domain knowledge. Start with the model that you think will be the final model, then experiment with adding/removing predictors. For instance, if your model has weather predictors, start with the ones that are most meaningful (e.g. daily high temperature), then look at what changes when you switch it out for something else (e.g., daily average or low temperature). Check the residual diagnostic plots each time - ignore the p-values until the very end!"
  }
]