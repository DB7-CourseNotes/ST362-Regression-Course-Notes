---
title: "Fitting Straight Lines"
institute: "Jam: **Great Expectations** by The Gaslight Anthem"
---

```{r}
#| label: setup
#| include: false
set.seed(2112)
```

::: {.content-visible unless-profile="book"}
## Preamble

### Announcements

- Check discussions!
- Assignment 1 groups are assigned

:::

## Why Fit Models?

### The Quote.

"All models are wrong, some are useful." - George Box

::: {.content-visible when-profile="book"}
This quote is something that you should tattoo on the inside of your eyelids so that you see it every time you blink.

... okay that's a little extreme, but this quote perfectly encapsulates the required mindset for modelling. No model is ever going to perfectly describe complex phenomena. However, a good model will at least say something useful about the world!

Of course, it's not a guarantee that a model is going to be useful. There are an infinite amount of useless models out there, and only a finite number of useful ones.
:::

### All Models Are Wrong, But Some Are Useful

Model: A mathematical equation that explains something about the world.

- Gravity: 9.8 $m/s^2$ right?
    - This is a *model* - it's a relationship that explains something in the world.
    - Varies across the surface of the Earth.
    - Varies according to air resistance.\lspace
- Every additional cigarette decreases your lifespan by 11 minutes.
    - Very, very much wrong.
    - Very, very useful for health communication.

### All Linear Models are Wrong

:::: {.columns}
::: {.column width="50%"}
```{r}
#| fig-height: 5
#| label: penguins-straight
library(ggplot2)
theme_set(theme_bw())
library(palmerpenguins)
penguins <- penguins[complete.cases(penguins),]

ggplot(penguins) +
    aes(x = flipper_length_mm, y = body_mass_g) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, formula = y~x) +
    labs(title = "A line fits reasonably well",
        x = "Flipper Length (mm)",
        y = "Body Mass (g)")
```


:::
::: {.column width="50%"}

```{r}
#| fig-height: 5
#| label: mtcars-curved
ggplot(mtcars) +
    aes(x = disp, y = mpg) + 
    geom_point(size = 2) + 
    geom_smooth(method = "lm", se = FALSE, formula = y~x) + 
    geom_smooth(method = "lm", se = FALSE, formula = "y~poly(x, 2)", colour = 2) +
    labs(title = "A straight line misses the pattern",
        subtitle = "A polynomial might fit better",
        x = "Engine Displacement (1000 cu.in)", 
        y = "Fuel Efficiency (mpg)")
```
:::
::::

::: {.content-visible when-profile="book"}
The plot on the left shows penguins' body mass against their flipper length. A straight line seems to fit pretty well. There's still a fair bit of variance above and below the line so we can't perfectly say exactly how much a penguin weighs if we only know their height, but the overall pattern is pretty straight.

On the right, we see the mtcars data again (definitely not the last time!), showing the fuel efficiency versus the engine displacement. The straight blue line is clearly missing important features of the data. We might be able to say that the fuel efficiency decreases with larger engines, but the line we've fit isn't going to be a useful description of the true pattern. The red curve seems to fit better, but we'll learn about a more useful model later in this course.
:::

### Some Linear Models are Useful

```{r}
#| label: penguins-annotated
xpred <- 200
ypred <- predict(
    lm(body_mass_g ~ flipper_length_mm, data = penguins),
        newdata = data.frame(flipper_length_mm = xpred))

ggplot(penguins) +
    aes(x = flipper_length_mm, y = body_mass_g) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, formula = y~x) +
    labs(title = "The height of the line is an okay prediction for Body Mass",
        x = "Flipper Length (mm)",
        y = "Body Mass (g)") +
    annotate(geom = "point", shape = "*", size = 30, colour = 2,
        x = xpred, 
        y = ypred) + 
    annotate(geom = "segment", colour = 2, size = 2, 
        linetype = "dashed",
        x = xpred, xend = xpred, 
        yend = -Inf, y = ypred) + 
    annotate(geom = "segment", colour = 2, size = 2, 
        linetype = "dashed",
        x = -Inf, xend = xpred, 
        yend = ypred, y = ypred)
```

### A Model is an Equation

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i;\; E(y_i|x_i) = \beta_0 + \beta_1 x_i;\; V(y_i|x_i)=\sigma^2
$$

- For a one unit increase in $x_i$, $y_i$ increases by $\beta_1$.
    - If our model fits well and this is statistically significant, we can apply this to the population.\lspace
- The estimated value of $y_i$ when $x_i=0$ is $\beta_0$.
    - Not always interesting, but usually$^*$ necessary.\lspace

Our prediction for $y_i$ at any given value of $x_i$ is calculated as:
$$
\hat y_i = \hat\beta_0 + \hat \beta_1x_i
$$
where the "hats" mean we've found estimates for the $\beta$ values.

::: {.content-visible when-profile="book"}
Side note: there are many models that are systems of equations - not just a single equation!
:::

### Notes on Notation

I will make mistakes, but in general:

- $Y$ is definitely a random variable (usually a vector) representing the **response**.
    - $Y_i$ is also a random variable (not a vector)
    - I also use $y_i$ as a random variable sometimes - context should make this clear!\lspace
- $X$ is a matrix of covariates
    - *Not* a random variable!!! Capital letters could be either random or matrix (or both). I will specify when necessary.
    - I will avoid the notation $X_i$.
    - Entries of $X$ are labelled $x_{ij}$.

### The Mean of $Y$ at any value of $X$

$$
E(Y|X) = X\underline\beta
$$

\includegraphics[width=0.9\textwidth]{figs/mean.png}

::: {.content-visible when-profile="book"}
The plot above shows the height of a child against the average height of their parents.

The yellow dots are caclulated as follows: a "bin" of x-values is defined - in this case, a bin is all values between, say, 60 to 63, and the next bin might be 61 to 64, and the next is 62 to 65, etc. The average height of the y-values was found within each bin. The yellow dot is the average y height, plotted at the x-axis value in the middle of the bin. 

The takeaway message of this plot is just how well the red line follows the yellow dots. A linear model can be seen as the average y-value for each x-value. This is a very useful way to think of linear models - they're just fancy averages!
:::



### A Linear Model is Linear in the *Parameters*

The following are linear:

- $y_i = \beta_0 + \beta_1x_i + \epsilon_i$
- $y_i = \beta_0 + \beta_1x_i + \beta_2x_i^2 + \epsilon_i$
- $y_i = \beta_0 + \beta_1\sin\left(x_i^{x_i}\right) + \beta_2x_i^{\sin(x_i)} + \epsilon_i$

\pspace

The following are *not* linear:

- $y_i = \beta_0 + \beta_1^2x_i +\epsilon_i$
- $y_i = \beta_0 + \beta_0\beta_1x_i +\epsilon_i$
- $y_i = \beta_0 + \sin(\beta_1)x_i +\epsilon_i$

::: {.content-visible when-profile="book"}
In the "not linear" list, note that it's the $\beta$s that are non-linear.
:::

## Estimation

### Goal: Find $\beta$ values which minimize the error

Model: $y_i = \beta_0 + \beta_1x_i + \epsilon_i$

Error: $\hat\epsilon_i = y_i - \hat y_i$

\pspace

To fit the model, why not just find the one that minimizes the sum of the errors?

::: {.content-visible when-profile="book"}
Answer: the errors can be negative, and the negatives can just cancel out the positives. If we just add the errors, then we can always get the sum to be 0 even if the model isn't actually fitting well.

Here's an example:

```{r}
#| label: sum-raw-errors

set.seed(2112)
x <- runif(50, 0, 10)
y <- 4 + 5 * x + rnorm(50, 0, 4)

par(mfrow = c(1, 2))
plot(x, y)

sum_of_errors <- c()
beta0s <- seq(24, 68, 4)
for (beta0 in beta0s) {
    abline(a = beta0, b = -4, lty = 2,col = 2)
    text(x = 5, y = beta0 - 4 * 5,
        labels = bquote(beta[0] ~ "=" ~ .(beta0)))
    errors <- y - beta0 + 4 * x
    sum_of_errors <- c(sum_of_errors, sum(errors))
}

plot(beta0s, sum_of_errors)
abline(h = 0)
```

In the plot above, the value of $\hat\beta_0$ is varied from 24 to 68 by 4 (24, 28, 32, ...), with the value of $\hat\beta_1$ fixed at -4. These lines are shown on the left.

The right plots the sum of the errors in these lines. The sum starts above 0, then at some point goes below 0. This means that we can have a slope of $\hat\beta_1 = -4$, then still find an intercept that makes the sum of the errors equal to 0!
:::

### Least Squares

Goal: Minimize $\sum_{i=1}^n\hat\epsilon_i^2=\sum_{i=1}^n(y_i - \hat y_i)^2$, the **sum of squared errors**.

\pspace

This ensures errors don't cancel out. It also penalizes large errors more.

\pspace

Could we have used $|\epsilon_i|$, or some other function that removes negatives? $|ly|$!

::: {.content-visible when-profile="book"}
The expression $|ly|$ represents the absolute value of "ly". It's a short form for "absolutely" that I'm trying to get the youths to use. It's not going well.

We'll see in other lectures and homework problems that we can define different errors, which prioritizes different aspects of the data. Squaring the errors means that large errors get even larger, so outliers are penalized more. Absolute errors penalize all values according to the size. You could also use exponential errors ($\exp(\hat\epsilon_i)$) if you really wanted to. 

We use squared errors for one main reason: because the math is easy. (This is not true, but it's a big selling point of squared errors.)
:::

### Least Squares Estimates - Start

To find the minimum of a function, we take the derivative and set it to 0!

\begin{align*}
R(\underline \beta) \stackrel{def}{=}\sum_{i=1}^n\hat\epsilon_i^2 &= \sum_{i=1}^n(y_i - \hat y_i)^2 = \sum_{i=1}^n(y_i - \beta_0 - \beta_1x_1)\\
\implies \frac{dR(\underline\beta)}{d\beta_0} &= -2\sum_{i=1}^n(y_i - \beta_0 - \beta_1x_i)\\
\text{and }\frac{dR(\underline\beta)}{d\beta_1} &= -2\sum_{i=1}^n(y_i - \beta_0- \beta_1^2)x_i = -2\sum y_ix_i + 2\beta_0\sum x_i + 2\beta_1\sum x_i^2
\end{align*}

\pspace

Homework: complete the derivation.

### Least Squares Estimates

I will assume that you can do the Least Squares estimate in your sleep (be ready for tests). 

The final results are:
\begin{align*}
\hat\beta_0 &= \bar y - \hat\beta_1\bar x\\
\hat\beta_1 &= \frac{\sum_{i-1}^n(x_i - \bar x)(y_i - \bar y)}{\sum_{i-1}^n(x_i - \bar x)^2}\stackrel{def}{=}\frac{S_{XY}}{S_{XX}}
\end{align*}

\pspace

The textbook was written in 1998 and gives formulas to make the calculation easier to do on pocket calculators. You will not need a pocket calculator for this course.

::: {.content-visible when-profile="book"}
If you've never done this before, try it! Make sure you get the same final answer, and then do it again without looking at your notes. This sort of derivation is a major part of any regression course.

An important point about these estimates is that we *have not yet assumed normality*. These are the values that minimize the squared errors. That's it; we did nothing else, assumed nothing else, and apply this to nothing else.
:::

### Let's Add Assumptions

Assumptions allow great things, but only when they're correct!

\pspace

1. $E(\epsilon_i) = 0$, $V(\epsilon_i) = \sigma^2$.\lspace
2. $cov(\epsilon_i, \epsilon_j) =0$ when $i\ne j$. 
    - Implies that $E(y_i) = \beta_0 + \beta_1x_i$ and $V(y_i) = \sigma^2$.\lspace
3. $\epsilon_i\stackrel{iid}{\sim} N(0,\sigma^2)$
    - This is a strong assumption, but often works!

\pspace

Interpretation: the model looks like a line with completely random errors. ("Completely random" doesn't mean "without structure"!)

### Mean of $\hat\beta_1$

It is easy to show that 
$$
\hat\beta_1 = \frac{\sum_{i-1}^n(x_i - \bar x)(y_i - \bar y)}{\sum_{i-1}^n(x_i - \bar x)^2} = \frac{\sum_{i-1}^n(x_i - \bar x)y_i}{\sum_{i-1}^n(x_i - \bar x)^2}
$$
since $\sum(x_i - \bar x) = 0$ (show this). 

\pspace

Homework: show that $E(\hat\beta_1) = \beta_1$ (*Hint: use the fact that $E(y_i) = \beta_0 + \beta_1x_i$). 

### Variance of $\hat\beta_1$

$$
\hat\beta_1 = \frac{\sum_{i-1}^n(x_i - \bar x)y_i}{\sum_{i-1}^n(x_i - \bar x)^2}
$$
can be re-written as 
$$
\hat\beta_1 = \sum_{i=1}^na_iy_i\text{, where }a_i = \frac{x_i - \bar x}{\sum_{i=1}^n(x_i - \bar x)^2}
$$

Thus the variance of $\hat\beta_1$ is
$$
V(\hat\beta_1) = \sum_{i=1}^na_i^2V(y_i) = ... = \frac{\sigma^2}{S_{XX}} \stackrel{plug-in}{=} = \frac{s^2}{S_{XX}}
$$

### Confidence Interval and Test Statistic for $\hat\beta_1$

The test statistic can be found as:
$$
t = \frac{\hat\beta_1 - \beta_1}{se(\hat\beta_1)} = \frac{(\hat\beta_1 - \beta_1)\sqrt{S_{XX}}}{s} \sim t_\nu
$$

Since this follows a $t$ distribution, we can get the CI:
$$
\hat\beta_1 \pm t_\nu(\alpha/2)\sqrt{\frac{s^2}{S_{XX}}}
$$

::: {.content-visible unless-profile="book"}

## Participation Questions

### Q1

All models are useful, but some are wrong.

\pspace

a. True\lspace
b. False

<!--- B --->

### Q2

For a one unit increase in $x$, $y$ increases by

\pspace

a. $\beta_0$\lspace
b. $\beta_1$\lspace
c. $\beta_1x$\lspace
d. $\beta_0 + \beta_1x$\lspace

<!--- B --->

### Q3

The standard error of $\beta_1$ refers to:

\pspace

a. The variance of the population slope\lspace
b. The amount by which the slope might be off\lspace
c. The variance of the estimated slope across different samples\lspace
d. A big chicken. Not, like, disturbingly big, but big enough that you'd be like, "Wow, that's a big chicken!"\lspace

<!--- C --->

### Q4

Which is *not* an assumption that we usually make in linear regression?

\pspace

a. $V(\epsilon_i) = \sigma^2$\lspace
b. $E(\epsilon_i) = 0$\lspace
c. $E(X) = 0$\lspace
d. $\epsilon_i\stackrel{iid}{\sim}N(0,\sigma^2)$

<!--- C --->

### Q5

Which of the following is *not* a linear model?

\pspace

a. $y_i = \beta_0 + \beta_1^2x_i + \epsilon_i$\lspace
b. $y_i = \beta_0 + \beta_1x_i + \beta_2x_i^2 + \epsilon_i$\lspace
c. $y_i = \beta_0 + \beta_0x_i +\epsilon_i$\lspace
d. $y_i = \beta_0 + \beta_1\sin\left(x_i^{x_i}\right) + \beta_2x_i^{\sin(x_i)} + \epsilon_i$\lspace

<!--- A --->

### Q6

The sum of squared errors is clearly the best way to estimate the model parameters in all situations.

\pspace

a. True\lspace
b. False

<!--- B --->

:::




## Analysis of Variance

### Statistics is the Study of Variance

- Given a data set with variable $\underline y$, $V(\underline y) = \sigma^2_y$.
    - This is just the variance of a single variable.\lspace
- Once we've incorporated the linear relationship with $\underline x$, $V(\hat\beta \underline x + \underline\epsilon)=0+V(\underline\epsilon) = \sigma^2$ 
    - Mathematically, $\sigma^2 \le \sigma_y^2$.
        - Homework: when are they equal?

\pspace

The variance in $Y$ is **explained** by $X$!


### A Useful Identity, and its Interpretations

\begin{align*}
\hat\epsilon_i &= y_i - \hat y_i \\&= y_i - \bar y - (\hat y_i - \bar y)\\
\implies \sum_{i=1}^n\hat\epsilon_i^2 &= \sum_{i=1}^n(y_i-\bar y)^2 - \sum_{i=1}^n(\hat y_i - \bar y)^2
\end{align*}
where we've simply added and subtracted $\bar y$. The final line skips a few steps (try them yourself, or see textbook page 28.)!

\pspace

The last line is often written as: $SS_E = SS_T - SS_{Reg}$. 

### Sums of Squares

$$
SS_E = SS_T - SS_{Reg}
$$

- $SS_E$: Sum of Squared Errors\lspace
- $SS_T$: Sum of Squares Total (i.e., without $\underline x$)\lspace
- $SS_{Reg}$: Sum of Squares due to the regression.
    - It's the variance of the line (calculated at observed $\underline x$ values) around the mean of $\underline y$???
        - This is incredibly useful, but weird.
    - I use $SS_{Reg}$ instead of $SS_R$; some textbooks use $SS_R$ as the Sum of Squared *Residuals* (same as $SS_E$), which is confusing.

### Aside: Degrees of Freedom

Def: The number of "pieces of information" from $y_1, y_2, ..., y_n$ to construct a new number.

- If I have $x = (1,3,2,1,3,???)$ and I know that $\bar x = 2$, I can recover the missing piece.
    - The mean "uses" (accounts for) one degree of freedom\lspace
- If I have $x = (1,2,3,1,???,???)$ and I know $\bar x = 2$ and $s_x^2=1$, I can recover the *two* missing pieces.
    - The variance accounts for two degrees of freedom.
        - One $df$ is required to compute it.

\pspace

Estimating one parameter takes away one degree of freedom for the rest!

- Can find $\bar x$ when $x = (1)$, but can't find $s_x^2$ because there aren't enough $df$!

### Sums of Squares

| Source of Variation | Degress of Freedom $df$ | Sum of Squares ($SS$) | Mean Square ($MS$) |
|---|---|---|---|
| Regression | 1 | $\sum_{i=1}^n(\hat y_i - \bar y)^2$ | $MS_{Reg}$ |
| Error | $n-2$ | $\sum_{i=1}^n(y_i - \hat y_i)^2$ | $MS_E=s^2$ |
| Total | $n-1$ | $\sum_{i=1}^n(y_i - \bar y)^2$ | $s_y^2$ |

\pspace

- Notice that $SS_T = SS_{Reg} + SS_E$, which is also true for the $df$ (but not $MS$). \lspace
- Why is $df_E = n-2$? What two parameters have we estimated?
    - $df_{Reg}$ is trickier to explain. It suffices to know that $df_{Reg} = df_T-df_E$.

### Using Sums/Means of Squares

- If $\hat y_i = \bar y$ for all $i$, then we have a horizontal line!
    - That is, there is *no relationship* between $\underline x$ and $\underline $y$.
    - In this case, $SS_{reg} = \sum_{i=1}^n(\hat y_i - \bar y)^2 = 0$.

\pspace

Okay, so, just test for $SS_{Reg} = 0$?

\pspace\pause

But how??? We need some measure of *how far from 0* is statistically significant!!!

Recall that $SS_E = \sum_{i=1}^n(y_i - \hat y_i)^2$.

- We can compare the variation around the line to the variation *of* the line.
    - This is $MS_{Reg}/MS_E$, and it follows an $F$ distribution!!

### The F-test for Significance of Regression

$$
F_{df_{Reg}, df_E} = \dfrac{MS_{Reg}}{MS_E} = \dfrac{MS_{Reg}}{s^2}
$$

- Recall that $SS_{Reg} = SS_E + SS_T > SS_E$, but the $df$ make a difference.\lspace
- For homework, show that $E(MS_{Reg}) = \sigma^2 + \beta_1^2\sum_{i=1}^n(x_i-\bar x)^2$\lspace
- This implies that $E(MS_{Reg}) > E(MS_E) = \sigma^2$.
    - **UNLESS** $\beta_1 = 0$


### Exercise A from Chapter 3 (Ch03 Exercises cover Ch1-3)

:::: {.columns}
::: {.column width="50%"}
\vspace{0.25cm}

A study was made on the effect of temperature on the yield of a chemical process. The data are shown to the right.

\pspace

1. Assuming $y_i = \beta_0 + \beta_1x_i + \epsilon_i$, what are the least squares estimates of $\beta_0$ and $\beta_1$? 
2. Construct the analysis of variance table and test the hypothesis $H_0: \beta_1=0$ at the  0.05 level.
3. What are the confidence limits (at $\alpha$ = 0.05) for $\beta_1$?

:::
::: {.column width="50%"}

```{r}
#| label: load-text-data
#| echo: true
#| eval: true
#| code-line-numbers: false

library(aprean3) # Data from textbook
head(dse03a) # Data Set Exercise Ch03A
nrow(dse03a)
```
:::
::::


::: {.content-visible when-profile="book"}

Solution: see [Lab 2](Lb02-OLS_Estimates.html).

## Exercises

Good textbook questions: A, K, O, P, T, U, X (using `data(anscombe)` in R), Z, AA, CC. Note that all data sets in the textbook are available in an R package found [here](https://search.r-project.org/CRAN/refmans/aprean3/html/00Index.html).

1. Find the OLS estimates of $\beta_0$ and $\beta_1$ for the model $y_i = \beta_0 + \beta_1x_i + \epsilon_i$.
2. Find the OLS estimates of $\beta_0$ and $\beta_1$ for the model $y_i = \beta_1x_i + \epsilon_i$.
    - Comment on these estimates compared to the previous question. What happened to $\bar x$ and $\bar y$?
    - Investigate what happens to $\hat\beta_1$ if the true intercept is not 0.

<details>
<summary>**Solution**</summary>

We're trying to minimize:
$$
\sum_{i=1}^n(\epsilon_i)^2 = \sum_{i=1}^n(y_i - \beta_1x_i)^2
$$
Setting the derivative to 0 (and putting on our hats, since we're now working with the value of $\beta_1$ that minimizes the equation, not the true value of $\beta_1$):
$$
0 = -2\sum(y_i -\hat\beta_1x_i)x_i = -2\sum(y_ix_i) + 2\hat\beta_1\sum x_i^2\implies\hat\beta_1 = \frac{\sum x_iy_i}{\sum x_i^2}
$$

Note that this is the exact same formula as the estimate for $\hat\beta_1$ in the model with an intercept, but with $\bar x = \bar y = 0$. In the intercept model, the line is guaranteed to pass through the point $(\bar x, \bar y)$ (prove this!). In the intercept model, the line is guaranteed to pass through the point $(0, 0)$. 

In the following simulation, the true slope is the exact same no matter what. However, the true intercept varies. When $\beta_0 = 0$, we get the true value for $\beta_1$. When the true intercept is negative, the estimate for the slope is too low. When $\beta_0$ is positive, the slope is too high. The plots on the right show why this is the case.

For further homework, change the range of `x` (to be far from 0 or to cover 0), the value of `beta1` (try small, large, and negative values), and the standard deviation of `e` (small and large) and see if you can guess the pattern before you run the code.

```{r}
#| label: bias-without-intercept
#| fig-height: 7
#| fig-width: 7
n <- 75
x <- runif(n, 0, 10)
beta1 <- 5

beta_hats <- c()
beta0s <- seq(-10, 10, by = 1)
for (beta0 in beta0s) {
    e <- rnorm(n, 0, 3)
    y <- beta0 + beta1*x + e
    beta1_est <- sum(x * y) / sum(x^2)
    beta_hats <- c(beta_hats, beta1_est)
}

layout(matrix(c(1,1,2,1,1,3,1,1,4), ncol = 3, byrow = TRUE))

plot(beta0s, beta_hats)
abline(h = 5, col = 2, lwd = 3)

beta0 <- -10
e <- rnorm(n, 0, 3)
y <- beta0 + beta1*x + e
plot(y ~ x, main = "True b0 = -10")
abline(a = 0, b = sum(x * y) / sum(x^2), col = 2, lwd = 3)

beta0 <- 0
e <- rnorm(n, 0, 3)
y <- beta0 + beta1*x + e
plot(y ~ x, main = "True b0 = 0")
abline(a = 0, b = sum(x * y) / sum(x^2), col = 2, lwd = 3)

beta0 <- 10
e <- rnorm(n, 0, 3)
y <- beta0 + beta1*x + e
plot(y ~ x, main = "True b0 = 10",
    ylim = c(0, max(y)))
abline(a = 0, b = sum(x * y) / sum(x^2), col = 2, lwd = 3)
```

</details>
*****

3. Given that $\hat\beta_1 = \frac{\sum_{i-1}^n(x_i - \bar x)(y_i - \bar y)}{\sum_{i-1}^n(x_i - \bar x)^2}$, find $E(\hat\beta)$.
4. Complete the derivation of $V(\hat\beta_1)$.
5. Suppose that we fit the model without an intercept, but there actually is an intercept term. That is, suppose $y_i = \beta_0 + \beta_1x_i +\epsilon_i$, but our estimator is what was found in Question 2, and find $E(\hat\beta_1)$. Hint: it's *not* $\beta_1$!
6. Show that $E(MS_{Reg}) = \sigma^2 + \beta_1^2\sum_{i=1}^n(x_i-\bar x)^2$
7. Use R to calculate the values in the ANOVA table, i.e. the "Sum of Squares" section, for a linear regression of `waiting` versus `eruptions` in the `faithful` data. Compare the the `anova()` output, shown below.

```{r}
#| label: faithful-anova
data(faithful)

y <- faithful$waiting
y_bar <- mean(y)
n <- length(y)

faithful_lm <- lm(waiting ~ eruptions, data = faithful)
y_hat <- predict(faithful_lm)

anova(faithful_lm)
```


8. Use a simulation to demonstrate that, assuming $\beta_1 = 0$, $MS_{Reg} / MS_E \sim F_{df_{Reg}, df_E}$.
    - Explain why this is only true when $\beta_1 = 0$

<details>
<summary>**Solution**</summary>



```{r}
#| label: dist_msreg
n <- 100
x <- runif(n, 0, 10)

f_vals <- c()
msreg_vals <-c()
for (i in 1:1000) {
    e <- rnorm(n, 0, 1)
    y <- -3 + 0.005*x + e
    mylm <- lm(y ~ x)
    
    y_hat <- predict(mylm)
    y_bar <- mean(y)
    ms_reg <- sum((y_hat - y_bar)^2) / 1
    msreg_vals[i] <- ms_reg
    ms_e <- sum((y - y_hat)^2) / (n - 2)
    f_vals <- c(f_vals, ms_reg / ms_e)
}

hist(f_vals, freq = FALSE, breaks = 40)
curve(df(x, 1, n - 2), col = 2, lwd = 3, add = TRUE)
```

Why is this  only true when $\beta_1=0$? For intuition, this only *should* be true when the null is true: recall that p-values are always calculated *assuming that the null is true*, and the F-stat is no different.

Mathematically, this has to do with the fact that $E(MS_{Reg}) = \sigma^2 + \beta_1S_{XX}$, not just $\sigma^2$. Since this is not equal to $\sigma^2$, this does not follow a Chi-Square distribution.

```{r}
#| label: dist-msreg-2
par(mfrow = c(1, 2))
hist(msreg_vals, breaks = 40, freq = FALSE)
curve(dchisq(x, 1), add = TRUE, col = 2, lwd = 3)

# qqplot, just to check
qqplot(x = qchisq(ppoints(length(msreg_vals)), 1), y = msreg_vals)
abline(a = 0, b = 1)
```

... actually it kinda does follow a Chi-Square distribution. Not sure what to make of that.
</details>
*****

:::
