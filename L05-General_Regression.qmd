---
title: "The General Regression Situation"
institute: "Jam: **I Predict a Riot** by The Kaiser Chefs"
---

```{r}
#| label: setup
#| include: false
set.seed(2112)
```


::: {.content-visible unless-profile="book"}
## Preamble

### Announcements

- Lectures are back to normal next week.\lspace

:::

## Chapter Summary

### The Normal Equations

\begin{align*}
\underline\epsilon^T\underline\epsilon &= (Y - X\underline\beta)^T(Y - X\underline\beta)\\
&= ... = Y^TY - 2\underline\beta^TX^TY + \underline\beta^TX^TX\underline\beta
\end{align*}
We then take the derivative with respect to $\underline\beta$. Note that $X^TX$ is symmetric and $Y^TX\underline\beta$ is a scalar..
\begin{align*}
\frac{\partial}{\partial\underline\beta}\underline\epsilon^T\underline\epsilon &= 0 - 2X^TY + 2X^TX\underline\beta
\end{align*}

- For the 1 predictor case, verify that the equations look the same!

Setting to 0, rearranging, and plugging in our data gets us the Normal equations:
\begin{align*}
X^TX\underline{\hat\beta} &= X^T\underline y
\end{align*}

### Facts

The solution to the normal equations satisfies:
$$X^TX\underline{\hat\beta} = X^T\underline y$$

1. No distributional assumptions.\lspace
2. If $X^TX$ is invertible, $\hat{\underline\beta} = (X^TX)^{-1}X^T\underline y$.
    - $\hat{\underline\beta}$ is a linear transformation of $\underline y$!
    - This is the same as the MLE.\lspace
3. $E(\hat{\underline\beta}) = \underline\beta$ and $V(\hat{\underline\beta}) = \sigma^2(X^TX)^{-1}$.
    - This is the smallest variance amongst all unbiased estimators of $\underline\beta$.

### Example Proof Problems

1. Prove that $\sum_{i=1}^n\hat\epsilon_i\hat y_i = 0$.
2. Prove that $\sum_{i=1}^n\hat\epsilon_i = 0$.
3. Prove that $X^TX$ is symmetric. Is $A^TA$ symmetric in general?

\small
```{r}
#| label: XTX = XTXT
#| echo: true
#| eval: true
## Demonstration that they're true (up to a rounding error)
mylm <- broom::augment(lm(mpg ~ disp, data = mtcars))
sum(mylm$.resid * mylm$.fitted)
mean(mylm$.resid)
X <- model.matrix(mpg ~ disp, data = mtcars)
all.equal(t(X) %*% X, t(t(X) %*% X))
```

### Analysis of Variance (Corrected)

| Source | $df$ | $SS$ |
|--------|------|------|
| Regression (corrected) | $p - 1$ | $\underline{\hat{\beta}}^TX^T\underline y - n\bar{y}^2$  |
| Error | $n - p$ | $\underline y^t\underline y- \hat{\underline\beta}^TX^T\underline y$ |
| Total (corrected) | $n - 1$ | $\underline y^t\underline y - n\bar{y}^2$ |

- Note that $p$ is the number of parameters, not the index of the largest param.
    - $\underline\beta = (\beta_0, \beta_1, ..., \beta_{p-1})$\lspace
- We'll always be using corrected sum-of-squares.
    - Especially next chapter!

### $F$-test for overall significance

If SSReg is significantly larger than SSE, then fitting the model was worth it!

- This is a test for $\beta_1 = \beta_2 = ... = \beta_{p-1} = 0$, versus any $\beta_j\ne 0$.

As before, we find a quantity with a known distribution, then use it for hypothesis tests.

$$
F = \frac{MS(Reg|\hat\beta_0)}{MSE} = \frac{SS(Reg|\hat\beta_0)/(p-1)}{SSE/(n-p)} \sim F_{p-1, n-p}
$$

Again, note that a regression with no predictors always has $\hat\beta_0 = \bar y$.

### Example: Significance of `disp`

:::: {.columns}
::: {.column width="60%"}

\small
```{r}
#| label: mpg-anova-only
#| echo: true
anova(lm(mpg ~ disp, data = mtcars)) |> 
    knitr::kable()
```

\normalsize

- $p = 2$, $n = 32$. 
    - $df_R' + df_E' = df_T'$, where $df'$ is the df for corrected SS.
- Verified these numbers in the last lecture


:::
::: {.column width="40%"}

```{r}
#| label: plot-mpg
#| fig-width: 4
#| fig-height: 5
plot(mpg ~ disp, data = mtcars)
abline(lm(mpg ~ disp, data = mtcars))
```
:::
::::

### Example: $F_{1,p-1} = t^2_{p-1}$

\small
```{r}
#| label: mpg-anova
#| echo: true
anova(lm(mpg ~ qsec, data = mtcars)) |> 
    knitr::kable()

summary(lm(mpg ~ qsec, data = mtcars))$coef |>
    knitr::kable()
```

\vspace{-3mm}
\normalsize
What do you notice about these two tables?

### Example: Significance of Regression ($F_{2,p-1} \ne t^2_{p-1}$)

\scriptsize
```{r}
#| label: mpg-anova-multi
#| echo: true
anova(lm(mpg ~ qsec + disp, data = mtcars)) |> 
    knitr::kable()

summary(lm(mpg ~ qsec + disp, data = mtcars))$coef |>
    knitr::kable()
```

We'll learn more about the ANOVA table next lecture.

### $R^2$ again

$$
R^2 = \frac{SS(Reg|\hat\beta_0)}{Y^TY - SS(\beta_0)} = \frac{\sum(\hat y_i - \bar y)^2}{\sum(y_i - \bar y)^2}
$$

Works for multiple dimensions!\pause... kinda.

### $R^2$ is bad?

:::: {.columns}
::: {.column width="60%"}
\small
```{r}
#| label: r-sq-increases-code
#| echo: true
#| eval: false
nx <- 10 # Number of uncorrelated predictores
uncorr <- matrix(rnorm(50*(nx + 1)), 
    nrow = 50, ncol = nx + 1)
## First column is y, rest are x
colnames(uncorr) <- c("y", paste0("x", 1:nx))
uncorr <- as.data.frame(uncorr)

rsquares <- NA
for (i in 2:(nx + 1)) {
    rsquares <- c(rsquares,
        summary(lm(y ~ ., 
            data = uncorr[,1:i]))$r.squared)
}
plot(1:10, rsquares[-1], type = "b",
    xlab = "Number of Uncorrelated Predictors",
    ylab = "R^2 Value",
    main = "R^2 ALWAYS increases")
```

:::
::: {.column width="40%"}
```{r}
#| label: r-sq-increases-plot
#| echo: false
#| eval: true
#| fig-height: 4.5
#| fig-width: 4
nx <- 10 # Number of uncorrelated predictores
uncorr <- matrix(rnorm(50*(nx + 1)), 
    nrow = 50, ncol = nx + 1)
## First column is y, rest are x
colnames(uncorr) <- c("y", paste0("x", 1:nx))
uncorr <- as.data.frame(uncorr)

rsquares <- NA
for (i in 2:(nx + 1)) {
    rsquares <- c(rsquares,
        summary(lm(y ~ ., 
            data = uncorr[,1:i]))$r.squared)
}
plot(1:10, rsquares[-1], type = "b",
    xlab = "Number of Uncorrelated Predictors",
    ylab = "R^2 Value",
    main = "R^2 ALWAYS increases")
```
:::
::::

### Adjusted (Multiple) $R^2$
$$
R^2_a = 1 - (1 - R^2)\left(\frac{n-1}{n-p}\right)
$$

- Penalizes added predictors - won't always increase!
    - Still might increase by chance alone!
        - F-test
    - $R^2_a = R^2$ when $p=1$ (intercept model)\lspace
- Still not perfect!
    - Works for comparing different models on same data
    - Works (poorly) for comparing different models on different data.\lspace
- In general you should use $R^2_a$, but always be careful.


## Prediction and Confidence Intervals (Again)


### $R^2$ and $F$

Recall that
$$
F = \frac{MS(Reg|\hat\beta_0)}{MSE} = \frac{SS(Reg|\hat\beta_0)/(p-1)}{SSE/(n-p)} \sim F_{p-1, n-p}
$$

From the definition of $R^2$,
\begin{align*}
R^2 &= \frac{SS(Reg|\hat\beta_0)}{SST}\\
&= \frac{SS(Reg|\hat\beta_0)}{SS(Reg|\hat\beta_0) + SSE}\\
&= \frac{(p-1)F}{(p-1)F + (n-p)}
\end{align*}
Conclusion: Hypothesis tests/CIs for $R^2$ aren't useful. Just use $F$!


### Correlation of $\hat\beta_0$, $\hat\beta_1$, $\hat\beta_2$, etc.

With a different sample, we would have gotten slightly different numbers!

\pspace

- If the slope changed, the intercept must change to fit the data
    - (and \emph{vice-versa})
    - The parameter estimates are *correlated*!\lspace
- Similar things happen with multiple predictors!\lspace
- This correlation can be a problem for **confidence regions**

### Uncorrelated $\hat\underline{\beta}$

$$
V(\hat{\underline\beta}) = \sigma^2(X^TX)^{-1}
$$

In simple linear regression,
$$
(X^TX)^{-1} = \frac{1}{nS_{XX}}\begin{bmatrix}\sum x_i^2 & -n\bar x\\-n \bar x & n\end{bmatrix}
$$

so the correlation is 0 when $\bar x = 0$!

### Prediction and Confidence Intervals for $Y$

$\hat Y = X\hat\beta$.

- A confidence interval around $\hat Y$ is based on the variance of $\hat\beta$.
- $\hat Y \pm t * se(X\hat\beta)$

\pspace

$Y_{n+1} = X\beta + \epsilon_{n+1}$

- A prediction interval around $Y_{n+1}$ is based on the variance of $\hat\beta$ *and* $\epsilon$!
- $\hat Y_{n+1} \pm t * se(X\hat\beta + \epsilon_{n+1})$

::: {.content-visible unless-profile="book"}

## Participation Questions

### Q1

Which of the following are the Normal equations?

a. $X^TX\underline\beta = X^T\underline y$\lspace
b. $X^TX\underline{\hat\beta} = X^T\underline y$\lspace
c. $\hat{\underline\beta} = (X^TX)^{-1}X^T\underline y$\lspace
d. $f(\epsilon_i) = \frac{1}{\sqrt{2\pi\sigma}}\exp\left(\frac{-1}{2}\epsilon_i^2\right)$

<!--- B --->

### Q2

When is $X^TX$ not invertible?

a. One of the predictors can be written as a linear combination of the others.
b. There are more predictors than observations.
c. One of the predictors has 0 variance.
d. All of the above

<!--- D --->

### Q3

What does a significant F-test for the overall regression mean?

a. The variance in the line is significantly larger than the variance in the data.
b. The estimate of $\beta_1$ is significantly different from $\beta_0$,
c. The variance of the line is significantly different from 0.
d. At least one of the predictors in the model will have  significant $t$-test.

<!--- A --->

### Q4

$R^2$ is best used for:

a. Determining whether a new predictor is worth including.
b. Comparing models with different numbers of predictors.
c. Comparing models based on different data sets.
d. None of the above.

<!--- D --->

### Q5

Which of the following describes a Prediction Interval?

a. The CI for the predicted value of the line
b. The CI for the predicted value of the line, including unobserved error at an $X$ value
c. The CI for the predicted value of the line, including unobserved error at an $X$ value that was not observed in the data
d. The CI for the predicted value of the line, including unobserved error at an $X$ value that was not observed in the data, using the true value of $\sigma^2$

<!--- B --->

### Q6

Which ANOVA table does the `anova()` function calculate?

\footnotesize

1.

| Source | $df$ | $SS$ |
|--------|------|------|
| Regression | 1 | $\hat{\underline\beta}^TX^T\underline y - n\bar{y}^2$  |
| Error | $n-2$ | $\underline y^t\underline y- \hat{\underline\beta}^TX^T\underline y$ |
| Total | $n - 1$ | $\underline y^t\underline y - n\bar{y}^2$ |


2.

| Source | $df$ | $SS$ |
|--------|------|------|
| Regression | 1 | $\hat{\underline\beta}^TX^T\underline y$  |
| Error | $n-2$ | $\underline y^t\underline y- \hat{\underline\beta}^TX^T\underline y$ | 
| Total | $n - 1$ | $\underline y^t\underline y$ |

<!--- A --->

:::

::: {.content-visible when-profile="book"}
## Exercises

1. Prove that $\sum_{i=1}^n\hat\epsilon_i\hat y_i = 0$.
2. Prove that $\sum_{i=1}^n\hat\epsilon_i = 0$.
3. Prove that $X^TX$ is symmetric. Is $A^TA$ symmetric in general?
4. Simulate data with $n = 50$, $p = 11$, and $\beta_1 = \beta_2 = ... = \beta_10 = 0$. Check the individual t-tests for whether the slopes are 0, and the overall F-test. Do this multiple times. Do you find a significant result from one of the t-tests but not the overall F-test? What about vice-versa?

<details>
<summary>**Solution**</summary>

```{r}
n <- 50
p <- 11
X <- matrix(
    data = c(rep(1, n),
        runif(n * (p - 1), -5, 5)),
    ncol = p)
beta <- c(p - 1, rep(0, p - 1)) # beta_0 = 10, will not affect results
y <- X %*% beta + rnorm(n, 0, 3)

mydf <- data.frame(y, X[, -1])
mylm <- lm(y ~ ., data = mydf)
summary(mylm)
```

In my run (this will be different each time you run it), one of the slopes was found to be significant! This is a Type 1 error - we found a significant result where we shouldn't have.

However, the F-test successfully found that the overall fit is not significantly better than a model without any predictors. F-tests exists to prevent Type 1 errors!

As a side note, check out the difference between the Multiple R-squared and Adjusted R-squared. Which one would you believe in this situation?

</details>
*****

5. Plot the confidence intervals *and* the prediction intervals for a simple linear regression of body mass versus flipper length for penguins. Do this for a lot of points, and interpret the difference between the two.

```{r}
library(palmerpenguins)
penguins <- penguins[complete.cases(penguins),]
peng_lm <- lm(body_mass_g ~ flipper_length_mm, data = penguins)
```

<details>
<summary>**Solution**</summary>

```{r}
library(palmerpenguins)
penguins <- penguins[complete.cases(penguins),]
peng_lm <- lm(body_mass_g ~ flipper_length_mm, data = penguins)

# confidence intervals along all flipper lengths
flippers <- seq(from = min(penguins$flipper_length_mm),
    to = max(penguins$flipper_length_mm), length.out = 100)

confi <- predict(peng_lm, newdata = data.frame(flipper_length_mm = flippers), interval = "confidence")
predi <- predict(peng_lm, newdata = data.frame(flipper_length_mm = flippers), interval = "prediction")

plot(body_mass_g ~ flipper_length_mm, data = penguins)
lines(x = flippers, y = confi[, "upr"], col = 3, lwd = 2, lty = 2)
lines(x = flippers, y = confi[, "lwr"], col = 3, lwd = 2, lty = 2)
lines(x = flippers, y = predi[, "upr"], col = 2, lwd = 2, lty = 2)
lines(x = flippers, y = predi[, "lwr"], col = 2, lwd = 2, lty = 2)
legend("topleft", legend = c("Confidence", "Prediction"), col = 3:2, lwd = 2, lty = 2)
```

The confidence "band" is *much* thinner than the prediction band! With this many points, the model is pretty sure about the slope and intercept (i.e. $\hat y_i$); a new sample would be expected to produce a similar line.

However, even though the model knows about the slope and intercept of the line, there's still lots of uncertainty around an unobserved point $\hat y_{n + 1}$. 

*****
</details>

6. Use a simulation to demonstrate a confidence interval for $\hat y_i$ (an observed data point). For a model $y_i = 1 + 4x_i + \epsilon_i$, $\epsilon_i \sim N(0, 16)$, $n = 30$, simulate 100 data sets, calculate the line, and add it to a plot. What do you notice about the shape? Why is this? 

<details>
<summary>**Solution**</summary>

```{r}
n <- 30
x <- runif(n, -5, 5)
plot(NA, xlim = c(-5, 5), ylim = c(-25, 25), xlab = "x", ylab = "y")
for (i in 1:1000) {
    y <- 1 + 4 * x + rnorm(n, 0, 8)
    abline(lm(y ~ x))
}
```

It looks kind of like a bowtie!

As mentioned in lecture, the model can be seen as fitting a point at $(\bar x, \bar y)$, then finding a slope that fits the data well. If we got the exact same value for $\bar y$ every time, every single line would converge at the point $(\bar x, \bar y)$!

We also saw that the variance of $\hat y_i$ is minimized at $\bar x$, and this plot demonstrates this.

*****
</details>

:::

