---
title: "Assessing Fit"
institute: "Jam: **Left Overs** by Joe Bonamassa"
---

```{r}
#| label: setup
#| include: false
set.seed(2112)
```


::: {.content-visible unless-profile="book"}
## Preamble

### Announcements

- Lab 2 is Wednesday!
:::

## Rule Number 1: Always Plot Everything


### Anscombe's Quartet

Consider the following four data sets

```{r}
#| echo: false
#| label: anscombe1

xs <- paste0("x", 1:4)
ys <- paste0("y", 1:4)

df1 <- data.frame(data_set = 1:4,
    xbar = sapply(xs, function(x) mean(anscombe[, x])),
    sigma_x = sapply(xs, function(x) sd(anscombe[, x])),
    ybar = sapply(ys, function(y) mean(anscombe[, y])),
    sigma_y = sapply(ys, function(y) sd(anscombe[, y])),
    corr = sapply(1:4, function(i) cor(anscombe[, xs[i]], anscombe[, ys[i]]))
) 
rownames(df1) <- NULL
df1 |>
    knitr::kable(row.names = FALSE)

```

Can you guess what the plots look like?

### Anscombe's Quartet

```{r}
#| label: anscombe2
#| echo: false
#| fig-height: 5
#| fig-width: 10

par(mfrow = c(2, 2), mar = c(4, 4, 2, 2))
for (i in 1:4) {
    plot(x = anscombe[, xs[i]], y = anscombe[, ys[i]],
        xlab = paste0("x", i), ylab = paste0("y", i),
        ylim = c(0,13), xlim = c(4,20))
    abline(lm(anscombe[, ys[i]] ~ anscombe[, xs[i]]))
}
```


## The residual: what's left over

### $R^2$: percent of variance explained by the regression model

:::: {.columns}
::: {.column width="30%"}
\vspace{2cm}

\begin{align*}
R^2 &= \frac{SSReg}{SST}\\& = \frac{S_{XY}^2}{S_{XX}S_{YY}}
\end{align*}

:::
::: {.column width="70%"}

```{r}
#| label: R2_demo
#| fig-height: 4.5
#| fig-width: 6
layout(mat = matrix(c(1,2,3), nrow = 1), widths = c(0.5,1,1))
set.seed(18)
x <- runif(25, 0, 10)
y <- rnorm(25, 2 + 5*x, 6)

plot(rep(1, 25), y, xlab = "y", ylab = "y has variance", xaxt = "n")
abline(h = mean(y))
axis(2, mean(y), bquote(bar(y)), las = 1)

plot(x, y, ylab = "There's variance around the line")
abline(lm(y~x))
abline(h = mean(y))
axis(2, mean(y), bquote(bar(y)), las = 1)

mids <- predict(lm(y~x))
for(i in seq_along(mids)){
    lines(x = rep(x[i], 2), y = c(y[i], mids[i]), col = 1)
}

mids <- predict(lm(y~x))
plot(mids ~ x, type = "n", ylab = "The line varies around the mean of y!")
for(i in seq_along(mids)){
    lines(x = rep(x[i], 2), y = c(mean(y), mids[i]))
}
axis(2, at = mean(y), labels = bquote(bar(y)), las = 1)
abline(h = mean(y))
```
:::
::::

### Residual Assumptions

- **Residual**: what's left over
    - $\hat\epsilon_i = y_i - \hat y_i$\lspace
- Assumptions:
    - $E(\epsilon_i) = 0$
    - $V(\epsilon_i) = \sigma^2$
    - $\epsilon_i \sim N(0,\sigma^2)$

\pspace

We must check our assumptions!

- There are statistical tests, but they'll never tell you as much as a plot!

::: {.content-visible when-profile="book"}
The statistical tests try to give a p-value for the hypothesis that the residuals are normal, but looking at the residual plot will always be superior!
:::

### Residuals versus *fitted* values: $\hat{\underline\epsilon}$ versus $\hat{\underline{y}}$

:::: {.columns}
::: {.column width="50%"}
\vspace{1cm}

Why $\hat{\underline{y}}$ instead of $\underline y$?

- See text. Try a regression of $\hat{\underline\epsilon}$ versus $\underline{y}$ yourself (mathematically and with code).

\pspace 

Why not $\underline x$?

- For simple linear regression, $\hat{\underline{y}}$ is like a unit change for $\underline x$, so it doesn't matter. 
    - For multiple linear regression, it's easier to have one variable for the $x$ axis.

:::
::: {.column width="50%"}

```{r}
#| label: resids_vs_fitted
#| fig-height: 5
#| fig-width: 5
library(ggplot2)
theme_set(theme_bw())
library(patchwork)
library(broom)
library(palmerpenguins)

penguins <- penguins[complete.cases(penguins),]

g1 <- ggplot(penguins) + 
    aes(x = flipper_length_mm, y = body_mass_g) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, formula = y~x) +
    labs(x = "Flipper Length (mm)",
        y = "Body Mass (g)",
        title = "y versus x")

plm <- lm(body_mass_g ~ flipper_length_mm, data = penguins)

p2 <- augment(plm)

g2 <- ggplot(p2) + 
    aes(x = .fitted, y = .resid) + 
    geom_point() +
    geom_hline(yintercept = 0, col = "grey") +
    labs(x = "Fitted", y = "Residuals", title = "Residuals versus Fitted")


g1 / g2
```
:::
::::

::: {.content-visible when-profile="book"}
I just want to add a little more context to the "unit change" idea. In a linear model, the estimated height of the line is $\hat y_i = \hat\beta_0 + \hat\beta_1x_i$. To go from Celcius to Fahrenheit, we use the equation $F_i = 32 + \frac{9}{5}C_i$, where $C_i$ is the Celcius value that we have and $F_i$ is the Fahrenheit value that we get. A line is literally a change of units!
:::

### Residual Plots and Assumption Checking

Mathematics is the process of making assumptions and seeing if we can break them.

- $E(\epsilon_i) = 0$ is a given since $\sum_{i=1}^n\hat\epsilon_i=0$.\lspace
- $V(\epsilon_i) = \sigma^2$ (regardless of $i$)
    - Check if the variance looks stable.\lspace
- $\epsilon_i \sim N(0,\sigma^2)$ is harder to see on a residuals versus fitted plot
    - Expect more points close to 0, fewer further away, no outliers

### Residual plots: unstable error, a.k.a. Heteroscedasticity

```{r}
#| label: heteroskedastic
x <- runif(200, 0, 10)
y0 <- 2 - 3*x
y <- y0 + rnorm(length(x), 0, 2 * x)

g1 <- ggplot() + 
    aes(x = x, y = y) + 
    geom_point() + 
    geom_smooth(se = FALSE, method = "lm", formula = y~x) +
    labs(x = NULL, y = NULL, title = "y versus x")

xydf <- augment(lm(y ~ x))

g2 <- ggplot(xydf) +
    aes(x = .fitted, y = .resid) + 
    geom_point() +
    geom_hline(yintercept = 0, col = "grey") +
    labs(x = "Fitted", y = "Residuals", title = "Residuals versus Fitted")

g1 + g2
```

::: {.content-visible when-profile="book"}
The idea of heteroscedastic data is that $V(\epsilon_i) = \sigma^2_i$, which depends on $i$. The most obvious case is shown above, where the variance increases along the values of $x$. This pattern is obviously present in the plot of $y$ versus $x$, but this can be very hard to see in higher dimensions!
:::

### Residual plots: non-linear trend?

```{r}
#| label: non-linear-trend
## fig-height: 4
## fig-width: 8
g1 <- ggplot(mtcars) + 
    aes(x = disp, y = mpg) + 
    geom_point() + 
    geom_smooth(se = FALSE, method = "lm", formula = y~x) +
    labs(x = "Engine Displacement", y = "Miles per Gallon", title = "y versus x")

mtdf <- augment(lm(mpg ~ disp, data = mtcars))

g2 <- ggplot(mtdf) +
    aes(x = .fitted, y = .resid) + 
    geom_point() +
    geom_smooth(se = FALSE) +
    geom_hline(yintercept = 0, col = "grey") +
    labs(x = "Fitted", y = "Residuals", title = "Residuals versus Fitted - non-linear trend?")

g1 + g2
```

::: {.content-visible when-profile="book"}
The plots above show a (mild) violation of the assumption that $y_i = \beta_0 + \beta_1x_i + \epsilon_i$, i.e., that the true trend is linear. This can be seen in the plot of $y$ against $x$, but it's much more obvious in the residual plot!

Note that, in this course, the phrase "The Residual Plot" *always* refers to a plot of residuals against fitted. There are other residual plots, but **the** residual plot is resids versus fitted. 
:::

### Testing Normality: Quantile-Quantile Plots

:::: {.columns}
::: {.column width="58%"}
\vspace{0.15cm}

Consider the data (2,3,3,4,5,5,6).\vspace{0.15cm}

- 50\% of the data is below the median. 
    - For a $N(0,1)$ distribution, 50% of the data is below 0.
    - Put the median on the y axis, 0 on the x axis.
- 25\% of the data is below Q1.
    - For a $N(0,1)$ distribution, 25% is below `qnorm(0.25)` = -0.67
    - Put a point at x = -0.67, y = Q1.
- 75\% of the data is below Q3.
    - For a $N(0,1)$ distribution, 75% is below `qnorm(0.75)` = 0.67
    - Put a point at x = 0.67, y = Q3.
- ... and so on for the rest of the quantiles
    - See [this link](https://colab.research.google.com/drive/1CwTYG9lx1WcSN6DDf--xSBYzgAo9PwTS) for an example.

:::
::: {.column width="40%"}

If perfectly normal, expect a straight line!

```{r}
#| label: qq-demo
#| echo: true
#| fig-height: 4
mydata <- c(2, 3, 3, 4, 5, 5, 6)
quants <- qnorm(c(
    0.125, 0.25, 0.375, 0.5, 
    0.625, 0.75, 0.875))
plot(mydata ~ quants)
```

:::
::::

::: {.content-visible when-profile="book"}
The example link is a fun thing to play with! It's written in Python, so don't worry about the code. Change the sample sizes to get an idea of just how hard it is to definitively show that data are normal.
:::

### Other Residual Plots: 

**Scale-Location**

- Scale: Standardized residual
- Location: Fitted value
- More on standardized residuals in Ch08

\pspace

**Cook's Distance**

- Basically, an outlier detection method. 
- More in Ch08

\pspace

**Leverage**

- More in Ch08

::: {.content-visible unless-profile="book"}


## Participation Quiz

### Q1

Which of the following is not an assumption of linear models?

\pspace

a. The line is straight.
b. The variance is constant across all values of $X$.
c. The height of the line is the mean value of $Y$ for a given $X$.
d. None of the above.

<!--- D --->

### Q2

Which of the following is a confidence interval for $s$?

\pspace

a. $\chi^2_n(0.055) \le \frac{(n-1)s^2}{\sigma^2} \le \chi^2_n(0.945)$
b. $s^2 \pm \text{Critical Value } * \text{ se}(s^2)$
c. $s \pm \text{Critical Value } * \text{ se}(s)$
d. None of the above

<!--- D --->

### Q3

Which of the following is *not* a random variable?

\pspace

a. $Y$
b. $\beta_1$
c. $\hat Y$
d. $\hat\beta_1$

<!--- B --->

### Q4

For simple linear regression, the percent of variance explained is the square of the correlation between $y$ and $x$, i.e., $r^2$. 

a. True
b. False
c. Sometimes true

<!--- A --->

### Q5

Which of the following is the definition of a residual?

a. $y_i - \hat y_i$
b. $(y_i - \hat y_i)^2$
c. $\hat y_i - y_i$
d. $(\hat y_i - y_i)^2$

\pause

A is the *estimated residual*. The **true residual** is $y_i - \beta_0 + \beta_1x_i$, where $\beta_0$ and $\beta_1$ are the true intercept and slope.

<!--- A --->

### Q6

Which of the following statements about $R^2$ in simple linear regression is *false*?

a. $R^2 = SSReg / SST$
b. $R^2 = r^2$, where $r$ is the correlation between $\underline x$ and $\underline y$
c. $R^2$ compares the variance of the line to the variance of $y$ alone
d. $R^2$ is not a random variable

<!--- D --->

:::

## Maximum Likelihood

### Main Idea

Find the values $\hat\beta_0$, $\hat\beta_1$, and $\hat\sigma^2$ that **maximize the likelihood of seeing our data**.

\pspace

Under the assumptions that $X$ is fixed, $Y = \beta_0 + \beta_1X + \epsilon_i$, and $\epsilon_i\stackrel{iid}{\sim}N(0,\sigma^2)$,
$$
Y \sim N(\beta_0 + \beta_1X, \sigma^2)
$$ 

### The Likelihood

The **probability** of observering a data point is:
$$
f_Y(y_i|x_i, \beta_0, \beta_1, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(\frac{-1}{2\sigma^2}(y_i - \beta_0 - \beta_1x_i)^2\right)
$$

\pspace

The **likelihood** of the parameters, given the data, is:
$$
L(\beta_0, \beta_1, \sigma^2|x_i, y_i) = \prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(\frac{-1}{2\sigma^2}(y_i - \beta_0 - \beta_1x_i)^2\right)
$$

- It's just a shift in perspective!

### Simple Coin Flip Example

Suppose we flipped 10 bottle caps and got 6 "crowns". Assume the probability of "crown" ($C$) is unknown, labelled $p$.

\pspace

- The **probability** of one cap flip is $P(C|p) = p$.\lspace
- The **probability** of this is $P(C = 6|p) = p^6(1-p)^4$. 
    - This is just $P(\underline y|p) = \prod_{i=1}^nP(Y = y_i)$.\lspace
- The **likelihood** is $L(p|\underline y) = \prod_{i=1}^nP(Y = y_i)$.

### Maximizing the Likelihood in LM

$$
L(\beta_0, \beta_, \sigma^2) = \prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(\frac{-1}{2\sigma^2}(y_i - \beta_0 - \beta_1x_i)^2\right)
$$

- To maximize w.r.t $\beta_0$, we set the derivative w.r.t $\beta_0$ to 0 and solve for $\beta_0$.
    - $\frac{\partial L(\beta_0, \beta_, \sigma^2)}{\partial \beta_0} = 0$.
- Repeat for $\frac{\partial L(\beta_0, \beta_, \sigma^2)}{\partial \beta_1} = 0$ and $\frac{\partial L(\beta_0, \beta_, \sigma^2)}{\partial \sigma^2} = 0$

\pspace

HWK: Show that the estimates for $\hat\beta_0$ and $\hat\beta_1$ are the same as the OLS estimates. The estimate for $\hat\sigma^2$ should come out to:
$$
\hat\sigma^2 = \frac{1}{n}\sum_{i=1}^n\left(y_i - \hat\beta_0 - \hat\beta_1x_i\right)^2
$$

### Interpreting the MSE

The basic question of statistics: "How big is this number?"

\pspace

- Compare to previous studies - is MSE larger than $\sigma^2$?
    - Implies that Bias$^2$ and Fitted Model Variance are larger than expected.
    - F-test\lspace
- Compare to "pure error" - direct estimate of $\sigma^2$.
    - i.e. the variance in repeated trials on the same covariate values
    - Textbooks devotes a lot to this, but it's often not plausible.
        - Won't be on tests!\lspace
- Compare to another model 
    - We'll focus on this (later)!


### Compare to Previous Studies

Hypothesis test for $\sigma^2 = \sigma_0^2$ versus $\sigma^2 > \sigma_0^2$, where $\sigma_0$ is the value from a previous study.

\pspace

- If significant, some of your error is coming from the study design!

### Compare to other models

It can be shown that $E(MS_{Reg}) = \sigma^2 + \beta_1S_{XX}$.

\pspace

Consider the Null hypothesis $\beta_1 =0$ (why is this a good null?).

- Under this null, $\frac{MS_{Reg}}{s^2}\sim F_{1, n-2}$.
    - Obvious CI from this. \lspace
- This is exactly equivalent to the t-test for $\beta_1$! (See text.)


### MSE of a Parameter: Bias of $s^2$

From a previous class, we know that
$$
\frac{(n-2)s^2}{\sigma^2}\sim\chi^2_{n-2}
$$

From wikipedia, we know that the mean of a $\chi^2_k$ distribution is $k$. Therefore,
$$
E\left(\frac{(n-2)s^2}{\sigma^2}\right) = n-2 \Leftrightarrow E(s^2) = \sigma^2
$$
and thus $s^2$ is unbiased.\pause

\pspace

This does *not necessarily* mean that $s^2$ is the best estimator for $\sigma^2$!

::: {.content-visible unless-profile="book"}
See exercises in course notes for a simulation!
:::


### MSE of a Parameter: Bias of $s$

Even though $s^2$ is an unbiased estimator, $s = \sqrt{s^2}$ is biased! Specifically, $E(s) < \sigma$

To see why, first note that 
$$
V(s) = E(s^2) - (E(s))^2 \Leftrightarrow E(s) = \sqrt{E(s^2) - V(s)}
$$
since $V(s) > 0$, $E(s^2) - V(s) < E(s^2)$, and therefore
$$
E(s) < \sqrt{E(s^2)} = \sqrt{\sigma^2} = \sigma
$$

### Summary

- Most of regression involves **analysing the residuals**.
    - Residuals **are** regression.
    - Residual plots will all be explained/explored thoroughly.\lspace
- $R^2 = \frac{SSReg}{SST}$ is the **percent of variance explained**.
    - This is a very important concept.\lspace
- MLE usually involves writing out a product of normals
    - Take the `ln()`, then derivative, then set to 0 and solve.\lspace
- The MSE is best interpreted relative to a hypothesis or other model.
    - The raw value is sometimes - but rarely - useful.\lspace

::: {.content-visible when-profile="book"}
## Exercises

Recommended textbook exercises: TODO

1. Show that $\frac{SSReg}{SST} = \frac{S_{XY}^2}{S_{XX}S_{YY}}$.
1. Explore why we use residuals versus fitted, rather than residuals versus observed. 
    - Fit a regression, then plot $\underline\epsilon$ versus $\underline y$. What do you notice?
    - Find $cor(\epsilon_i, y_i)$ mathematically. Compare this with what you see in the plot.

<details>
<summary>**Solution**</summary>



```{r}
#| label: resids-vs-observed
#| code-fold: false
n <- 100
x <- runif(n, 0, 10)
e <- rnorm(n, 0, 3)
y <- 5 + 2*x + e

mylm <- lm(y ~ x)
plot(mylm$residuals ~ y)
```

Note that $y_i = \hat y_i + \hat \epsilon_i$

$$
cov(\hat\epsilon_i, y_i) = cov(\epsilon_i, \hat y_i + \hat \epsilon_i) = cov(\epsilon_i, \hat y_i) + cov(\hat\epsilon_i, \hat \epsilon_i) = 0 + \sigma^2
$$

Empiricially:

```{r}
#| label: var-covar-resids
#| code-fold: false
cov(mylm$residuals, y)
var(mylm$residuals)
```

This is why we use residuals versus fitted, not residuals versus observed! 

</details>
*****

2. Show that the estimates for $\hat\beta_0$ and $\hat\beta_1$ are the same as the OLS estimates.

3. Find the MLE estimate for $\sigma^2$.
    - Comment on the difference between this formula and the usual variance formula.
    - Comment on the variance formula from least squares

<details>
<summary>**Solution**</summary>

Derivation left as an exercise. Hint: You are allowed to take a derivative with respect to $\sigma^2$, not just $\sigma$. 

- MLE versus usual formula: We're dividing by $n$, not $n - 1$ or even $n - 2$! The mle is actually **biased**. We'll explore this in the next question.
- There is no variance formula in least squares! MLE allows us to incorporate our assumption about the distribution of the residuals, giving us more to work with!
</details>
*****

4. The MLE of $\sigma^2$ divides by $n$. Demonstrate via simulation that this is actually a biased estimate.

<details>
<summary>**Solution**</summary>


```{r}
#| label: mle_bias_1
#| code-fold: false
n <- 100
true_sigma <- 3

n_minus <- seq(-3, 3, 1)
vals <- matrix(ncol = length(n_minus), nrow = 10000)
for (i in 1:10000) {
    y <- rnorm(n, 0, true_sigma)
    vals[i, ] <- sum((y - mean(y))^2) / (n - n_minus)
}

apply(vals, 2, mean) |> plot(x = n_minus, y = _)
abline(h = true_sigma^2, lwd = 3, col = 2)
```

From the plot, we get closest to the true value when we subtract 1 from $n$. However, the MLE subtracts nothing from $n$! Why not? Let's check the MSE.

```{r}
#| label: mle_bias_2
#| code-fold: false
apply(vals, 2, function(x) {
    sum((x - true_sigma^2)^2)
}) |> plot(x = n_minus, y = _)
```

The MSE is lowest when we divide by... $n+1$???? Yes, this is just how it is. See [here](https://alemorales.info/post/variance-estimators/) for a bit more discussion on this issue. Basically, the estimator with $n-1$ is unbiased, and $n$ is lower variance without too much bias, while $n+1$ is much more bias than we're comfortable with.

</details>
*****

5. Prove that $\sum_i=1^n\hat\epsilon_i = 0$. Really try it yourself, then check the hint below only if your're struggling.

<details>
<summary>**Hint**</summary>
$\hat\epsilon_i = y_i - \hat\beta_0 - \hat\beta_1x_i$, $\sum_{i=1}^ny_i = n\bar y$, and $\beta_0 = \bar y - \hat\beta_1x_i$.
*****
</details>

6. Consider the following plot, where the grey line is the mean model (where $\beta_1 = 0$), the green line represents the simple linear regression of mpg versus displacement, and the red line represents a model where the intercept is forced to be 0 (I use this a lot because it's easy to conceptualize and the math is easy, *not* because it's usually relevant to applications). Using R, calculate the anova table for both models and interpret the results.

```{r}
#| label: no-intercept
#| code-fold: false

ggplot(mtcars) + 
    aes(x = disp, y = mpg) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, formula = y ~ 1, colour = "darkgrey", linewidth = 2) +
    geom_smooth(method = "lm", se = FALSE, formula = y ~ x, colour = "green", linewidth = 2) +
    geom_smooth(method = "lm", se = FALSE, formula = y ~ 0 + x, colour = "red", linewidth = 2)

# Model with an intercept (the usual model)
lm_with <- lm(mpg ~ disp, data = mtcars)
# Using "0 +" forces the intercept to be 0
lm_without <- lm(mpg ~ 0 + disp, data = mtcars)
```

<details>
<summary>**Solution**</summary>

```{r}
#| label: no-intercept-solution
#| code-fold: false

anova(lm_with)
anova(lm_without)
```

Some interpretations:

- For `lm_with`:
    - $MS_{Reg} = 808.89$ and $MS_E = 10.57$, which are clearly different numbers (even with a sample size of 32, which you can see from $df_E = 30 = n - 2$).
        - The model is clearly different from just using the mean!
        - The p-value confirms this.
- For `lm_without`:
    - Again, the model is clearly "different" from just using the mean, but it's not necessarily **better** than using it. The graph shows that this model fits very poorly, the variance is just significantly *different*!
    - Note that the degrees of freedom is $n - 1$, since we're only estimating one parameter (the slope).
- Comparison:
    - $MS_{E}^{(with)} = 10.57$ and $MS_E^{(without)} = 207.8$. Clearly the model with an intercept is fitting much much better!
        - Notice: we did not look at $MS_{E}^{(with)} = 10.57$ and conclude that it was clearly a small value. The MSE is really hard to interpret on it's own, but it's quite useful for comparisons!

</details>
*****

:::
