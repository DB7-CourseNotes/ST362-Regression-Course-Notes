---
title: "Assessing Fit"
institute: "Jam: **Left Overs** by Joe Bonamassa"
---

```{r}
#| label: setup
#| include: false
set.seed(2112)
```


::: {.content-visible unless-profile="book"}
## Preamble

### Announcements

- Check discussions!\lspace
- Lab 2 is Wednesday!
:::

## The residual: what's left over

### $R^2$: percent of variance explained by the regression model

:::: {.columns}
::: {.column width="30%"}
\vspace{2cm}

$$
R^2 = \frac{SSReg}{SST} = \frac{S_{XY}^2}{S_{XX}S_{YY}}
$$

:::
::: {.column width="70%"}

```{r}
#| label: R2_demo
#| fig-height: 4.5
#| fig-width: 6
layout(mat = matrix(c(1,2,3), nrow = 1), widths = c(0.5,1,1))
set.seed(18)
x <- runif(25, 0, 10)
y <- rnorm(25, 2 + 5*x, 6)

plot(rep(1, 25), y, xlab = "y", ylab = "y has variance", xaxt = "n")
abline(h = mean(y))
axis(2, mean(y), bquote(bar(y)), las = 1)

plot(x, y, ylab = "There's variance around the line")
abline(lm(y~x))
abline(h = mean(y))
axis(2, mean(y), bquote(bar(y)), las = 1)

mids <- predict(lm(y~x))
for(i in seq_along(mids)){
    lines(x = rep(x[i], 2), y = c(y[i], mids[i]), col = 1)
}

mids <- predict(lm(y~x))
plot(mids ~ x, type = "n", ylab = "The line varies around the mean of y!")
for(i in seq_along(mids)){
    lines(x = rep(x[i], 2), y = c(mean(y), mids[i]))
}
axis(2, at = mean(y), labels = bquote(bar(y)), las = 1)
abline(h = mean(y))
```
:::
::::

### Residual Assumptions

- **Residual**: what's left over
    - $\hat\epsilon_i = y_i - \hat y_i$\lspace
- Assumptions:
    - $E(\epsilon_i) = 0$
    - $V(\epsilon_i) = \sigma^2$
    - $\epsilon_i \sim N(0,\sigma^2)$

\pspace

We must check our assumptions!

- There are statistical tests, but they'll never tell you as much as a plot!

::: {.content-visible when-profile="book"}
The statistical tests try to give a p-value for the hypothesis that the residuals are normal, but looking at the residual plot will always be superior!
:::

### Residuals versus *fitted* values: $\hat{\underline\epsilon}$ versus $\hat{\underline{y}}$

:::: {.columns}
::: {.column width="50%"}
\vspace{1cm}

Why $\hat{\underline{y}}$ instead of $\underline y$?

- See text. Try a regression of $\hat{\underline\epsilon}$ versus $\underline{y}$ yourself (mathematically and with code).

\pspace 

Why not $\underline x$?

- For simple linear regression, $\hat{\underline{y}}$ is like a unit change for $\underline x$, so it doesn't matter. 
    - For multiple linear regression, it's easier to have one variable for the $x$ axis.

:::
::: {.column width="50%"}

```{r}
#| label: resids_vs_fitted
#| fig-height: 5
#| fig-width: 5
library(ggplot2)
theme_set(theme_bw())
library(patchwork)
library(broom)
library(palmerpenguins)

penguins <- penguins[complete.cases(penguins),]

g1 <- ggplot(penguins) + 
    aes(x = flipper_length_mm, y = body_mass_g) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, formula = y~x) +
    labs(x = "Flipper Length (mm)",
        y = "Body Mass (g)",
        title = "y versus x")

plm <- lm(body_mass_g ~ flipper_length_mm, data = penguins)

p2 <- augment(plm)

g2 <- ggplot(p2) + 
    aes(x = .fitted, y = .resid) + 
    geom_point() +
    geom_hline(yintercept = 0, col = "grey") +
    labs(x = "Fitted", y = "Residuals", title = "Residuals versus Fitted")


g1 / g2
```
:::
::::

### Residual Plots and Assumption Checking

Mathematics is the process of making assumptions and seeing if we can break them.

- $E(\epsilon_i) = 0$ is a given since $\sum_{i=1}^n\hat\epsilon_i=0$.\lspace
- $V(\epsilon_i) = \sigma^2$ (regardless of $i$)
    - Check if the variance looks stable.\lspace
- $\epsilon_i \sim N(0,\sigma^2)$ is harder to see on a residuals versus fitted plot
    - Expect more points close to 0, fewer further away, no outliers

### Residual plots: unstable error

```{r}
#| label: heteroskedastic
x <- runif(200, 0, 10)
y0 <- 2 - 3*x
y <- y0 + rnorm(length(x), 0, 2 * x)

g1 <- ggplot() + 
    aes(x = x, y = y) + 
    geom_point() + 
    geom_smooth(se = FALSE, method = "lm", formula = y~x) +
    labs(x = NULL, y = NULL, title = "y versus x")

xydf <- augment(lm(y ~ x))

g2 <- ggplot(xydf) +
    aes(x = .fitted, y = .resid) + 
    geom_point() +
    geom_hline(yintercept = 0, col = "grey") +
    labs(x = "Fitted", y = "Residuals", title = "Residuals versus Fitted")

g1 + g2
```

### Residual plots: non-linear trend?

```{r}
#| label: non-linear-trend
## fig-height: 4
## fig-width: 8
g1 <- ggplot(mtcars) + 
    aes(x = disp, y = mpg) + 
    geom_point() + 
    geom_smooth(se = FALSE, method = "lm", formula = y~x) +
    labs(x = "Engine Displacement", y = "Miles per Gallon", title = "y versus x")

mtdf <- augment(lm(mpg ~ disp, data = mtcars))

g2 <- ggplot(mtdf) +
    aes(x = .fitted, y = .resid) + 
    geom_point() +
    geom_smooth(se = FALSE) +
    geom_hline(yintercept = 0, col = "grey") +
    labs(x = "Fitted", y = "Residuals", title = "Residuals versus Fitted - non-linear trend?")

g1 + g2
```


### Testing Normality: Quantile-Quantile Plots

:::: {.columns}
::: {.column width="58%"}
\vspace{0.15cm}

Consider the data (2,3,3,4,5,5,6).\vspace{0.15cm}

- 50\% of the data is below the median. 
    - For a $N(0,1)$ distribution, 50% of the data is below 0.
    - Put the median on the y axis, 0 on the x axis.
- 25\% of the data is below Q1.
    - For a $N(0,1)$ distribution, 25% is below `qnorm(0.25)` = -0.67
    - Put a point at x = -0.67, y = Q1.
- 75\% of the data is below Q3.
    - For a $N(0,1)$ distribution, 75% is below `qnorm(0.75)` = 0.67
    - Put a point at x = 0.67, y = Q3.
- ... and so on for the rest of the quantiles



:::
::: {.column width="40%"}

If perfectly normal, expect a straight line!

```{r}
#| label: qq-demo
#| echo: true
#| fig-height: 4
mydata <- c(2, 3, 3, 4, 5, 5, 6)
quants <- qnorm(c(
    0.125, 0.25, 0.375, 0.5, 
    0.625, 0.75, 0.875))
plot(mydata ~ quants)
```

:::
::::

### Other Residual Plots: 

**Scale-Location**

- Scale: Standardized residual
- Location: Fitted value
- More on standardized residuals in Ch08

\pspace

**Cook's Distance**

- Basically, an outlier detection method. 
- More in Ch08

\pspace

**Leverage**

- More in Ch08

::: {.content-visible unless-profile="book"}


## Participation Quiz

### Q01

Which of the following is not an assumption of linear models?

\pspace

a. The line is straight.
b. The variance is constant across all values of $X$.
c. The height of the line is the mean value of $Y$ for a given $X$.
d. None of the above.

<!--- D --->

### Q02

Which of the following is a confidence interval for $s$?

\pspace

a. $\chi^2_n(0.055) \le \frac{(n-1)s^2}{\sigma^2} \le \chi^2_n(0.945)$
b. $s^2 \pm \text{Critical Value } * \text{ se}(s^2)$
c. $s \pm \text{Critical Value } * \text{ se}(s)$
d. None of the above

<!--- D --->

### Q03

Which of the following is *not* a random variable?

\pspace

a. $Y$
b. $\hat\epsilon_i$
c. $\hat Y$
d. $\underline\epsilon$

<!--- D --->

### Q04

For simple linear regression, the percent of variance explained is the square of the correlation between $y$ and $x$, i.e., $r^2$. 

a. True
b. False
c. Sometimes true

<!--- A --->

### Q05

Which of the following is the definition of a residual?

a. $y_i - \hat y_i$
b. $(y_i - \hat y_i)^2$
c. $\hat y_i - y_i$
d. $(\hat y_i - y_i)^2$

\pause

A is the *estimated residual*. The **true residual** is $y_i - \beta_0 + \beta_1x_i$, where $\beta_0$ and $\beta_1$ are the true intercept and slope.

<!--- A --->

### Q06

Which of the following statements about $R^2$ in simple linear regression is *false*?

a. $R^2 = SSReg / SST$
b. $R^2 = r^2$, where $r$ is the correlation between $\underline x$ and $\underline y$
c. $R^2$ compares the variance of the line to the variance of $y$ alone
d. $R^2$ is not a random variable

<!--- D --->

:::

## Maximum Likelihood

### Main Idea

Find the values $\hat\beta_0$, $\hat\beta_1$, and $\hat\sigma^2$ that **maximize the likelihood of seeing our data**.

\pspace

Under the assumptions that $X$ is fixed, $Y = \beta_0 + \beta_1X + \epsilon_i$, and $\epsilon_i\stackrel{iid}{\sim}N(0,\sigma^2)$,
$$
Y \sim N(\beta_0 + \beta_1X, \sigma^2)
$$ 

### The Likelihood

The **probability** of observering a data point is:
$$
f_Y(y_i|x_i, \beta_0, \beta_1, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(\frac{-1}{2\sigma^2}(y_i - \beta_0 - \beta_1x_i)^2\right)
$$

\pspace

The **likelihood** of the parameters, given the data, is:
$$
L(\beta_0, \beta_1, \sigma^2|x_i, y_i) = \prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(\frac{-1}{2\sigma^2}(y_i - \beta_0 - \beta_1x_i)^2\right)
$$

- It's just a shift in perspective!

### Simple Coin Flip Example

Suppose we flipped 10 bottle caps and got 6 "crowns". Assume the probability of "crown" ($C$) is unknown, labelled $p$.

\pspace

- The **probability** of one cap flip is $P(C|p) = p$.\lspace
- The **probability** of this is $P(C = 6|p) = p^6(1-p)^4$. 
    - This is just $P(\underline y|p) = \prod_{i=1}^nP(Y = y_i)$.\lspace
- The **likelihood** is $L(p|\underline y) = \prod_{i=1}^nP(Y = y_i)$.

### Maximizing the Likelihood in LM

$$
L(\beta_0, \beta_, \sigma^2) = \prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(\frac{-1}{2\sigma^2}(y_i - \beta_0 - \beta_1x_i)^2\right)
$$

- To maximize w.r.t $\beta_0$, we set the derivative w.r.t $\beta_0$ to 0 and solve for $\beta_0$.
    - $\frac{\partial L(\beta_0, \beta_, \sigma^2)}{\partial \beta_0} = 0$.
- Repeat for $\frac{\partial L(\beta_0, \beta_, \sigma^2)}{\partial \beta_1} = 0$ and $\frac{\partial L(\beta_0, \beta_, \sigma^2)}{\partial \sigma^2} = 0$

\pspace

HWK: Show that the estimates for $\hat\beta_0$ and $\hat\beta_1$ are the same as the OLS estimates. The estimate for $\hat\sigma^2$ should come out to:
$$
\hat\sigma^2 = \frac{1}{n}\sum_{i=1}^n\left(y_i - \hat\beta_0 - \hat\beta_1x_i\right)^2
$$

### Bias-Variance Decomposition

On the next slide, we'll show that $E\left((Y_i - \hat Y_i)^2\right)$ can be decomposed into $V(\epsilon_i)$, squared bias, and the variance of the fitted model.

\pspace

- $V(\epsilon_i)$ is the variance of the *true* errors.\lspace
- Bias is the difference between the true model and the estimated one.
    - **Systematic** difference, not just random errors
        - e.g. fitting a linear model to a non-linear trend\lspace
- Variance of the fitted model *across all possible samples*

\pspace

Note that this is a slight deviation from how the text presents it.

### Derivation

Suppose the true value is $Y_i = g(x_i) + \epsilon_i$ (not necessarily linear), where $E(\epsilon_i) = 0$, $E(Y_i) = g(x_i)$, and $V(\epsilon_i) = \sigma^2$.
\begin{align*}
E\left((Y_i - \hat Y_i)^2\right) &= E(Y_i - 2Y_i\hat Y_i + \hat Y_i^2)\\
&= E(Y_i^2) - 2E(Y_i\hat Y_i) + E(\hat Y_i^2)\\
&= E((g(x_i) + \epsilon)^2) - 2E(g(x_i) + \epsilon_i)E(\hat Y_i) + E(\hat Y_i^2)\\
&= g^2(x_i) + 2g(x_i)E(\epsilon_i) + E(\epsilon_i^2) - 2g(x_i)E(\hat Y_i) + E(\hat Y_i^2)\\
&= g^2(x_i) + \sigma^2 - 2g(x_i)E(\hat Y_i) + E(\hat Y_i^2) + E(\hat Y_i)^2 - E(\hat Y_i)^2\\
&= \sigma^2 + (g(x_i) - E(\hat Y_i))^2 + V(\hat Y_i)\\
&= \text{Error Variance} + \text{Bias}^2 + \text{Fitted Model Variance}
\end{align*}

### Not Interpreting the MSE

$$
E((Y_i - \hat{Y_i})^2) = \sigma^2 + (g(x_i) - E(\hat Y_i))^2 + V(\hat Y_i)
$$

\pspace

- We don't know any of the numbers on the right!!!

\pspace

The decomposition is theoretical, we can't tease apart $s^2$ into these terms.

### Interpreting the MSE

The basic question of statistics: "How big is this number?"

\pspace

- Compare to previous studies - is MSE larger than $\sigma^2$?
    - Implies that Bias$^2$ and Fitted Model Variance are larger than expected.
    - F-test\lspace
- Compare to "pure error" - direct estimate of $\sigma^2$.
    - i.e. the variance in repeated trials on the same covariate values
    - Textbooks devotes a lot to this, but it's often not plausible.
        - Won't be on tests!\lspace
- Compare to another model 
    - We'll focus on this (later)!


### Compare to Previous Studies

Hypothesis test for $\sigma^2 = \sigma_0^2$ versus $\sigma^2 > \sigma_0^2$, where $\sigma_0$ is the value from a previous study.

\pspace

- If significant, some of your error is coming from the study design!

### Compare to other models

It can be shown that $E(MSReg) = \sigma^2 + \beta_1S_{XX}$.

\pspace

Consider the Null hypothesis $\beta_1 =0$ (why is this a good null?).

- Under this null, $\frac{MSReg}{s^2}\sim F_{1, n-2}$.
    - Obvious CI from this. \lspace
- This is exactly equivalent to the t-test for $\beta_1$! (See text.)


### MSE of a Parameter: Bias of $s^2$

From a previous class, we know that
$$
\frac{(n-2)s^2}{\sigma^2}\sim\chi^2_{n-2}
$$

From wikipedia, we know that the mean of a $\chi^2_k$ distribution is $k$. Therefore,
$$
E\left(\frac{(n-2)s^2}{\sigma^2}\right) = n-2 \Leftrightarrow E(s^2) = \sigma^2
$$
and thus $s^2$ is unbiased.\pause

\pspace

This does *not necessarily* mean that $s^2$ is the best estimator for $\sigma^2$! 
::: {.content-visible unless-profile="book"}
See course notes for a simulation!
:::


### MSE of a Parameter: Bias of $s$

Even though $s^2$ is an unbiased estimator, $s = \sqrt{s^2}$ is biased! Specifically, $E(s) < \sigma$

To see why, first note that 
$$
V(s) = E(s^2) - (E(s))^2 \Leftrightarrow E(s) = \sqrt{E(s^2) - V(s)}
$$
since $V(s) > 0$, $E(s^2) - V(s) < E(s^2)$, and therefore
$$
E(s) < \sqrt{E(s^2)} = \sqrt{\sigma^2} = \sigma
$$

### Summary

- Most of regression involves **analysing the residuals**.
    - Residuals **are** regression.
    - Residual plots will all be explained/explored thoroughly.\lspace
- $R^2 = \frac{SSReg}{SST}$ is the **percent of variance explained**.
    - This is a very important concept.\lspace
- MLE usually involves writing out a product of normals
    - Take the `ln()`, then derivative, then set to 0 and solve.\lspace
- The MSE is best interpreted relative to a hypothesis or other model.
    - The raw value is sometimes - but rarely - useful.\lspace

::: {.content-visible when-profile="book"}
## Exercises

Recommended textbook exercises: TODO

1. Explore why we use residuals versus fitted, rather than residuals versus observed. 
    - Fit a regression, then plot $\underline\epsilon$ versus $\underline y$. What do you notice?
    - Find $cor(\epsilon_i, y_i)$ mathematically. Compare this with what you see in the plot.

<details>
<summary>**Solution**</summary>



```{r}
#| label: resids-vs-observed
#| code-fold: false
n <- 100
x <- runif(n, 0, 10)
e <- rnorm(n, 0, 3)
y <- 5 + 2*x + e

mylm <- lm(y ~ x)
plot(mylm$residuals ~ y)
```

Note that $y_i = \hat y_i + \hat \epsilon_i$

$$
cov(\hat\epsilon_i, y_i) = cov(\epsilon_i, \hat y_i + \hat \epsilon_i) = cov(\epsilon_i, \hat y_i) + cov(\hat\epsilon_i, \hat \epsilon_i) = 0 + \sigma^2
$$

Empiricially:

```{r}
#| label: var-covar-resids
#| code-fold: false
cov(mylm$residuals, y)
var(mylm$residuals)
```

This is why we use residuals versus fitted, not residuals versus observed! 

</details>
*****

2. Find the MLE estimate for $\sigma^2$.
    - Comment on the difference between this formula and the usual variance formula.
    - Comment on the variance formula from least squares

<details>
<summary>**Solution**</summary>

Derivation left as an exercise. Hint: You are allowed to take a derivative with respect to $\sigma^2$, not just $\sigma$. 

- MLE versus usual formula: We're dividing by $n$, not $n - 1$ or even $n - 2$! The mle is actually **biased**. We'll explore this in the next question.
- There is no variance formula in least squares! MLE allows us to incorporate our assumption about the distribution of the residuals, giving us more to work with!
</details>
*****

3. The MLE of $\sigma^2$ divides by $n$. Demonstrate via simulation that this is actually a biased estimate.

<details>
<summary>**Solution**</summary>


```{r}
#| label: mle_bias_1
#| code-fold: false
n <- 100
true_sigma <- 3

n_minus <- seq(-3, 3, 1)
vals <- matrix(ncol = length(n_minus), nrow = 10000)
for (i in 1:10000) {
    y <- rnorm(n, 0, true_sigma)
    vals[i, ] <- sum((y - mean(y))^2) / (n - n_minus)
}

apply(vals, 2, mean) |> plot(x = n_minus, y = _)
abline(h = true_sigma^2, lwd = 3, col = 2)
```

From the plot, we get closest to the true value when we subtract 1 from $n$. However, the MLE subtracts nothing from $n$! Why not? Let's check the MSE.

```{r}
#| label: mle_bias_2
#| code-fold: false
apply(vals, 2, function(x) {
    sum((x - true_sigma^2)^2)
}) |> plot(x = n_minus, y = _)
```

The MSE is lowest when we divide by... $n+1$???? Yes, this is just how it is. See [here](https://alemorales.info/post/variance-estimators/) for a bit more discussion on this issue. Basically, the estimator with $n-1$ is unbiased, and $n$ is lower variance without too much bias, while $n+1$ is much more bias than we're comfortable with.

</details>
*****

4. Prove that $\sum_i=1^n\hat\epsilon_i = 0$. Really try it yourself, then check the hint below only if your're struggling.

<details>
<summary>**Hint**</summary>
$\hat\epsilon_i = y_i - \hat\beta_0 - \hat\beta_1x_i$, $\sum_{i=1}^ny_i = n\bar y$, and $\beta_0 = \bar y - \hat\beta_1x_i$.
*****
</details>

:::
